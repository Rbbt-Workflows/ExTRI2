{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get List of TF NCBI IDs \n",
    "Get the list of IDs that will be considered TF, both from GO terms and from TFCheckpoint.\n",
    "\n",
    "The GO terms and columns from TFCheckpoint used for each of the TF types is detailed in the cell below. The specific procedure used is explained in their respective sections:  [Get GO terms](#get-go-terms) and [GET TFCheckpoint terms](#get-tfcheckpoint-terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3>Table of contents</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "[Get List of TF NCBI IDs](#Get-List-of-TF-NCBI-IDs)\n",
       "- [Setup](#Setup)\n",
       "- [Get GO terms](#Get-GO-terms)\n",
       "- [Get TFCheckpoint terms](#Get-TFCheckpoint-terms)\n",
       "- [Get final TF set](#Get-final-TF-set)\n",
       "- [Filter out less likely coTFs](#Filter-out-less-likely-coTFs)\n",
       "  - [Get coTF human orthologs for filtering](#Get-coTF-human-orthologs-for-filtering)\n",
       "  - [Send all coTFs.](#Send-all-coTFs.)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "__import__('sys').path.append('../scripts/common/'); __import__('notebook_utils').table_of_contents('get_NCBI_TF_IDs.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import requests\n",
    "\n",
    "from Bio import Entrez\n",
    "# *Always* tell NCBI who you are\n",
    "Entrez.email = \"example24@gmail.com\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('../common')\n",
    "from notebook_utils import h3, h4, h5, md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GO TERM & TFCHECKPOINT VARIABLES\n",
    "\n",
    "# GO terms used:\n",
    "GO_dbTF = [\"GO:0003700\"]\n",
    "GO_GTF =  [\"GO:0140223\"]\n",
    "GO_coTF = [\"GO:0003712\", \"GO:0001098\", \"GO:0002039\" , \"GO:0008134\" , \"GO:0042393\", \"GO:0046332\", \"GO:0006325\", \"GO:0140993\"]\n",
    "\n",
    "# Columns from TFCheckpoint used:\n",
    "TFCheckpoint_cols = {\n",
    "    'dbTF': ['GO:0003700.Annotations', 'TFclass.present.merged', 'lambert_2018.present', 'Lovering_2021.present'],\n",
    "    'GTF':  ['GO:0140223.Annotations'],\n",
    "    'coTF': ['animal_tfdb_Homo_sapiens_cofactors.present', 'animal_tfdb_Mus_musculus_cofactors.present', 'animal_tfdb_Rattus_norvegicus_cofactors.present', \n",
    "             'tcof_cotf_human.present', 'tcof_cotf_mouse.present']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "in_data_path = '../../data/external/TF_id/'\n",
    "postprocessing_path = '../../data/postprocessing/'\n",
    "\n",
    "less_likely_coTFs_path = postprocessing_path + 'all_coTFs_likely_checked_updated_AL.txt'\n",
    "\n",
    "QuickGO_dbTF_path = in_data_path + \"QuickGO-annotations-dbTF.tsv\"\n",
    "QuickGO_GTF_path  = in_data_path + \"QuickGO-annotations-GTF.tsv\"\n",
    "QuickGO_coTF_path = in_data_path + \"QuickGO-annotations-coTF.tsv\"\n",
    "TFCheckpoint_path = in_data_path + \"TFCheckpoint.tsv\"\n",
    "\n",
    "# Define a function to construct the TF types path (ll_coTFs are introduced later)\n",
    "TF_types = [\"dbTF\", \"GTF\", \"coTF\"]\n",
    "def get_TF_ids_path(TF_type, out_data_path):\n",
    "    return f\"{out_data_path}{TF_type}_entrez_code.list\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD LIKELY & LESS LIKELY COTFs\n",
    "ll_coTFs_db = pd.read_csv(less_likely_coTFs_path, sep=\"\\t\", dtype='str')\n",
    "m = ll_coTFs_db['likely'] == 'likely'\n",
    "ll_coTF = set(ll_coTFs_db[~m]['NCBI ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get GO terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Obtained the GO terms from [QuickGO](https://www.ebi.ac.uk/QuickGO/annotations?taxonId=10116,9606,10090&taxonUsage=exact&goId=GO:0140993,GO:0003712,GO:0003700,GO:0140223,GO:0001098,GO:0002039,GO:0008134,GO:0042393,GO:0046332,GO:0006325&goUsageRelationships=is_a,part_of,occurs_in&goUsage=descendants&geneProductSubset=Swiss-Prot&geneProductType=protein), using the terms shown below. Used as filters:\n",
    "\n",
    "* **Taxon:** 10116, 9606, 10090, Exact match (do not include descendants)\n",
    "* **Gene products:** Reviewed (not Unreviewed)\n",
    "* **GO terms:** the ones shown below.\n",
    "* **Export as:** tsv\n",
    "\n",
    "Downloaded separately a QuickGO tsv file for each TF type and renamed it as shown above in the setup section.\n",
    "\n",
    "As some terms can be identified as pertaining to more than 1 type, we have followed this hierarchy to remove duplicates:\n",
    "1. dbTF\n",
    "2. GTF\n",
    "3. coTF\n",
    "\n",
    "That implies that if a protein is classified as both dbTF and GTF, the protein's classification will be dbTF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES\n",
    "# Species:\n",
    "organismToTaxID = {\n",
    "    \"hsapiens\": \"9606\",\n",
    "    \"mmusculus\": \"10090\",\n",
    "    \"rnorvegicus\": \"10116\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "def fetch_gene_ids_gprofiler(gene_symbols: list, organism: str) -> dict:\n",
    "    \"Get NCBI Gene IDs from GProfiler\"\n",
    "    symboltoID = {}\n",
    "\n",
    "    # Query the IDs from GProfiler\n",
    "    result = requests.post(\n",
    "        url='https://biit.cs.ut.ee/gprofiler/api/convert/convert/',\n",
    "        json={\n",
    "            'organism': organism,\n",
    "            'target':'ENTREZGENE_ACC',\n",
    "            'query': gene_symbols,\n",
    "        }\n",
    "        )\n",
    "    \n",
    "    # Create a list of extracted IDs per symbol\n",
    "    for r in result.json()['result']:\n",
    "        incoming = r['incoming']\n",
    "        converted = r['converted']\n",
    "\n",
    "        if incoming not in symboltoID:\n",
    "            symboltoID[incoming] = []\n",
    "        if converted != 'None':\n",
    "            symboltoID[incoming].append(converted)\n",
    "\n",
    "    return symboltoID\n",
    "\n",
    "def retrieve_annotations_entrez(id_list):\n",
    "    \"\"\"Annotates Entrez Gene IDs using Bio.Entrez, in particular epost (to\n",
    "    submit the data to NCBI) and esummary to retrieve the information.\n",
    "    Returns a list of dictionaries with the annotations.\"\"\"\n",
    "\n",
    "    request = Entrez.epost(\"gene\", id=\",\".join(id_list))\n",
    "    result = Entrez.read(request)\n",
    "    webEnv = result[\"WebEnv\"]\n",
    "    queryKey = result[\"QueryKey\"]\n",
    "    data = Entrez.esummary(db=\"gene\", webenv=webEnv, query_key=queryKey)\n",
    "    annotations = Entrez.read(data)\n",
    "    annotationsSummary = annotations['DocumentSummarySet']['DocumentSummary']\n",
    "\n",
    "    assert len(id_list) == len(annotationsSummary), f\"id_list and annotationsSummary are of different length: {len(id_list)} != {len(annotationsSummary)}\"\n",
    "\n",
    "    return annotationsSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25259 rows were retrieved.\n",
      "Removing duplicates, we retrieve 6076 symbols:\n",
      "\t2849 dbTFs\n",
      "\t120 GTFs\n",
      "\t3107 coTFs\n"
     ]
    }
   ],
   "source": [
    "# JOIN QUICKGO TSVs\n",
    "# Create joined DataFrame from the 3 TF types\n",
    "QuickGO_dbTF = pd.read_csv(QuickGO_dbTF_path, sep='\\t', header=0, keep_default_na=False, dtype='str')\n",
    "QuickGO_dbTF['TF type'] = 'dbTF' \n",
    "QuickGO_GTF  = pd.read_csv(QuickGO_GTF_path,  sep='\\t', header=0, keep_default_na=False, dtype='str')\n",
    "QuickGO_GTF['TF type'] = 'GTF' \n",
    "QuickGO_coTF = pd.read_csv(QuickGO_coTF_path, sep='\\t', header=0, keep_default_na=False, dtype='str')\n",
    "QuickGO_coTF['TF type'] = 'coTF' \n",
    "QuickGO = pd.concat([QuickGO_dbTF, QuickGO_GTF, QuickGO_coTF], axis=0)\n",
    "\n",
    "print(f\"{len(QuickGO['SYMBOL'])} rows were retrieved.\")\n",
    "\n",
    "# Only keep relevant columns\n",
    "QuickGO = QuickGO[['SYMBOL', 'TAXON ID', 'TF type']]\n",
    "\n",
    "# Drop repeated cells. Use the following priority if duplicates of different TF type\n",
    "priority = {'dbTF': 0, 'GTF': 1, 'coTF': 2}\n",
    "QuickGO['priority'] = QuickGO['TF type'].map(priority)\n",
    "QuickGO = QuickGO.sort_values(by=['SYMBOL', 'TAXON ID', 'priority'])\n",
    "QuickGO = QuickGO.drop_duplicates(subset=['SYMBOL', 'TAXON ID'], keep='first')\n",
    "QuickGO.drop(columns=['priority'], inplace=True) \n",
    "\n",
    "# Show results\n",
    "print(f\"Removing duplicates, we retrieve {len(QuickGO['SYMBOL'])} symbols:\")\n",
    "for TF_type in ('dbTF', 'GTF', 'coTF'):\n",
    "    print(f\"\\t{len(QuickGO[QuickGO['TF type'] == TF_type]['SYMBOL'])} {TF_type}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GProfiler retrieved 98.4% NCBI Gene IDs from the QuickGO symbols\n",
      "It couldn't retrieve 99 of them\n"
     ]
    }
   ],
   "source": [
    "# GET GENE IDs FROM GPROFILER\n",
    "for organism in ['hsapiens', 'mmusculus', 'rnorvegicus']:\n",
    "    # Get IDs from GProfiler\n",
    "    symbols = list(QuickGO[QuickGO['TAXON ID'] == organismToTaxID[organism]]['SYMBOL'].unique())\n",
    "    symboltoID = fetch_gene_ids_gprofiler(symbols, organism)\n",
    "\n",
    "    # Map them to QuickGO db\n",
    "    m = QuickGO['TAXON ID'] == organismToTaxID[organism]\n",
    "    QuickGO.loc[m, \"TF ID\"] = QuickGO[m]['SYMBOL'].apply(lambda symbol: symboltoID[symbol])\n",
    "\n",
    "m = ~(QuickGO['TF ID'].str.len() == 0)\n",
    "print(f'GProfiler retrieved {m.sum() / len(QuickGO):.1%} NCBI Gene IDs from the QuickGO symbols')\n",
    "print(f\"It couldn't retrieve {(~m).sum()} of them\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve the rest through a query to Entrez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m symbolsQuery \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([s\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[Preferred Symbol]\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m symbols])\n\u001b[1;32m     16\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m OR \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(symbolsQuery)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) AND txid\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTaxID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m[Organism]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 17\u001b[0m handle \u001b[38;5;241m=\u001b[39m \u001b[43mEntrez\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mesearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgene\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m record \u001b[38;5;241m=\u001b[39m Entrez\u001b[38;5;241m.\u001b[39mread(handle)\n\u001b[1;32m     19\u001b[0m ids\u001b[38;5;241m.\u001b[39mappend(record\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIdList\u001b[39m\u001b[38;5;124m\"\u001b[39m, []))\n",
      "File \u001b[0;32m~/git/ExTRI2_gits/ExTRI2_development/.general_env/lib/python3.10/site-packages/Bio/Entrez/__init__.py:239\u001b[0m, in \u001b[0;36mesearch\u001b[0;34m(db, term, **keywds)\u001b[0m\n\u001b[1;32m    237\u001b[0m variables\u001b[38;5;241m.\u001b[39mupdate(keywds)\n\u001b[1;32m    238\u001b[0m request \u001b[38;5;241m=\u001b[39m _build_request(cgi, variables)\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/ExTRI2_gits/ExTRI2_development/.general_env/lib/python3.10/site-packages/Bio/Entrez/__init__.py:634\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_tries):\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 634\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;66;03m# Reraise if the final try fails\u001b[39;00m\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_tries \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[1;32m    518\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[0;32m--> 519\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[1;32m    522\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    535\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m--> 536\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m    537\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[0;32m-> 1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:1348\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m         \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTransfer-encoding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1283\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, url, body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, headers\u001b[38;5;241m=\u001b[39m{}, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   1281\u001b[0m             encode_chunked\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1283\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1329\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(body, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;66;03m# RFC 2616 Section 3.7.1 says that text default has a\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;66;03m# default charset of iso-8859-1.\u001b[39;00m\n\u001b[1;32m   1328\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1329\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1278\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1278\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1038\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1036\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer)\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1038\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1041\u001b[0m \n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(message_body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;66;03m# Let file-like take precedence over byte-like.  This\u001b[39;00m\n\u001b[1;32m   1045\u001b[0m         \u001b[38;5;66;03m# is needed to allow the current position of mmap'ed\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m         \u001b[38;5;66;03m# files to be taken into account.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:976\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    975\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 976\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    978\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NotConnected()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1448\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1446\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnect to a host on a given (SSL) port.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[1;32m   1451\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:942\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001b[39;00m\n\u001b[1;32m    941\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp.client.connect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport)\n\u001b[0;32m--> 942\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;66;03m# Might fail in OSs that don't implement TCP_NODELAY\u001b[39;00m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:833\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[1;32m    832\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m--> 833\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[1;32m    835\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# QUERY THE REST FROM ENTREZ\n",
    "# Entrez often gets stuck, so it's best to give some time between queries\n",
    "import time\n",
    "ids = []\n",
    "for TaxID in ['9606', '10090', '10116']:\n",
    "\n",
    "    # Get the symbols with missing ID\n",
    "    m = (QuickGO['TF ID'].str.len() == 0) & (QuickGO['TAXON ID'] == TaxID)\n",
    "    missing_symbols = list(QuickGO[m]['SYMBOL'].unique())\n",
    "\n",
    "    nSymbols = 15 # Symbols per query\n",
    "    for i in range(0, len(missing_symbols), nSymbols):\n",
    "        symbols = missing_symbols[i:i+nSymbols]\n",
    "        # Query them from Entrez\n",
    "        symbolsQuery = sorted([s+'[Preferred Symbol]' for s in symbols])\n",
    "        query = f'({\" OR \".join(symbolsQuery)}) AND txid{TaxID}[Organism]'\n",
    "        handle = Entrez.esearch(db=\"gene\", term=query, retmode=\"xml\")\n",
    "        record = Entrez.read(handle)\n",
    "        ids.append(record.get(\"IdList\", []))\n",
    "        print(TaxID)\n",
    "        # Sleep between queries to not get blocked\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 out of the remaining 99 missing have been retrieved through Entrez.\n",
      "Combined with Entrez, we retrieved 99.3% NCBI Gene IDs from the QuickGO symbols\n",
      "There's 40 NCBI Gene IDs that couldn't be retrieved\n"
     ]
    }
   ],
   "source": [
    "# Map IDs back to its symbol & organism\n",
    "all_ids = [j for i in ids for j in i]\n",
    "remaining_annotations = retrieve_annotations_entrez(all_ids)\n",
    "\n",
    "# Make a map from symbol/TaxID to Gene ID\n",
    "symboltoID_entrez = {'9606': {}, '10116': {}, '10090': {}}\n",
    "for id, ann in zip(all_ids, remaining_annotations):\n",
    "    symbol = ann['Name']\n",
    "    TaxID = ann['Organism']['TaxID']\n",
    "\n",
    "    if symbol not in symboltoID_entrez[TaxID]:\n",
    "        symboltoID_entrez[TaxID][symbol] = [id]\n",
    "    else:\n",
    "        symboltoID_entrez[TaxID][symbol].append(id)\n",
    "\n",
    "# Check how many we retrieved from Entrez\n",
    "m = (QuickGO['TF ID'].str.len() == 0)\n",
    "print(f\"{len(all_ids)} out of the remaining {m.sum()} missing have been retrieved through Entrez.\")\n",
    "\n",
    "# Map the retrieved ones to the QuickGO db\n",
    "for TaxID in symboltoID_entrez.keys():\n",
    "    m = (QuickGO['TF ID'].str.len() == 0) & (QuickGO['TAXON ID'] == TaxID)\n",
    "    QuickGO.loc[m, \"TF ID\"] = QuickGO[m]['SYMBOL'].apply(lambda symbol: symboltoID_entrez[TaxID].get(symbol, []))\n",
    "\n",
    "m = ~(QuickGO['TF ID'].str.len() == 0)\n",
    "print(f'Combined with Entrez, we retrieved {m.sum() / len(QuickGO):.1%} NCBI Gene IDs from the QuickGO symbols')\n",
    "print(f\"There's {(~m).sum()} NCBI Gene IDs that couldn't be retrieved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Get TFCheckpoint terms\n",
    "Cells copied from `analysis.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4>EntrezIDs mapped to 2 species</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "There are 5 Entrez IDs that are mapped to both Rat and Mouse:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>EntrezIDs</th>\n",
       "      <th>Associated.Gene.Name</th>\n",
       "      <th>TaxaIDs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20851</td>\n",
       "      <td>STAT5A</td>\n",
       "      <td>10116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20851</td>\n",
       "      <td>STAT5B</td>\n",
       "      <td>10090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22764</td>\n",
       "      <td>ZFX</td>\n",
       "      <td>10090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22764</td>\n",
       "      <td>ZFY</td>\n",
       "      <td>10116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24918</td>\n",
       "      <td>STAT5A</td>\n",
       "      <td>10090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24918</td>\n",
       "      <td>STAT5B</td>\n",
       "      <td>10116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25126</td>\n",
       "      <td>STAT5A</td>\n",
       "      <td>10090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25126</td>\n",
       "      <td>STAT5B</td>\n",
       "      <td>10116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>367832</td>\n",
       "      <td>ZFX</td>\n",
       "      <td>10116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>367832</td>\n",
       "      <td>ZFY</td>\n",
       "      <td>10090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "They have been searched in the NCBI and corrected manually"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## LOAD & PREPROCESS TFCHECKPOINT TSV\n",
    "# Load TFCheckpoint dataset\n",
    "TF_df = pd.read_csv(TFCheckpoint_path, sep='\\t', header=0)\n",
    "str_cols = ['Associated.Gene.Name', 'Synonyms', 'Official name', 'Entrez.Taxa.ID', 'Entrez.Gene.ID', 'UniProt.SwissProt.Accession', 'Ensembl.Gene.ID']\n",
    "TF_df[str_cols] = TF_df[str_cols].astype(str)\n",
    "\n",
    "\n",
    "# Split Entrez, Taxa & UniProt into individual IDs\n",
    "TF_df['EntrezIDs'] = TF_df['Entrez.Gene.ID'].str.split('|')\n",
    "TF_df['TaxaIDs'] = TF_df['Entrez.Taxa.ID'].str.split('|')\n",
    "TF_df['UniProt'] = TF_df['UniProt.SwissProt.Accession'].str.split('|')\n",
    "TF_df['Ensembl'] = TF_df['Ensembl.Gene.ID'].str.split('|')\n",
    "\n",
    "# Explode the TF\n",
    "TF_exploded = TF_df.explode(['EntrezIDs', 'TaxaIDs', 'UniProt', 'Ensembl'])\n",
    "TF_exploded = TF_exploded[TF_exploded[\"EntrezIDs\"] != ''] # Drop empty rows (Appeared when | was present at the end, e.g. \"9454|3425|\")\n",
    "TF_exploded = TF_exploded[TF_exploded[\"UniProt\"] != '']\n",
    "TF_exploded = TF_exploded[TF_exploded[\"Ensembl\"] != '']\n",
    "\n",
    "# Check whether each EntrezID only matches to 1 TaxaID:\n",
    "gene_taxa_unique = TF_exploded.drop_duplicates(subset=[\"EntrezIDs\", \"TaxaIDs\"], keep='first')\n",
    "gene_taxa_mismatch = gene_taxa_unique[gene_taxa_unique.duplicated(subset=[\"EntrezIDs\"], keep=False)]\n",
    "h4(\"EntrezIDs mapped to 2 species\")\n",
    "md(f\"There are {len(gene_taxa_mismatch['EntrezIDs'].unique())} Entrez IDs that are mapped to both Rat and Mouse:\")\n",
    "display(HTML(gene_taxa_mismatch[[\"EntrezIDs\", \"Associated.Gene.Name\", \"TaxaIDs\"]].sort_values(by=['EntrezIDs']).to_html(index=False)))\n",
    "\n",
    "# DROP MISMATCHING ROWS\n",
    "rows_to_drop = [\n",
    "    ['STAT5A', '20851', '10116'],\n",
    "    ['STAT5A', '25126', '10090'],\n",
    "    ['ZFY', '367832', '10090'],\n",
    "    ['ZFY', '22764', '10116'],\n",
    "    ['STAT5B', '24918', '10116']\n",
    "]\n",
    "for row in rows_to_drop:\n",
    "    to_drop = (TF_exploded[\"Associated.Gene.Name\"] == row[0]) & (TF_exploded[\"EntrezIDs\"] == row[1])\n",
    "    assert to_drop.sum() == 1, f\"{to_drop.sum()} rows are being dropped instead of 1\"\n",
    "    TF_exploded = TF_exploded[~to_drop]\n",
    "to_change = (TF_exploded[\"Associated.Gene.Name\"] == \"STAT5A\") & (TF_exploded[\"EntrezIDs\"] == \"24918\")\n",
    "assert to_change.sum() == 1, f\"{to_change.sum()} rows are being dropped instead of 1\"\n",
    "TF_exploded.loc[to_change, \"TaxaIDs\"] = \"10116\"\n",
    "md(\"They have been searched in the NCBI and corrected manually\")\n",
    "\n",
    "# Assert there's no duplicates anymore\n",
    "gene_taxa_unique = TF_exploded.drop_duplicates(subset=[\"EntrezIDs\", \"TaxaIDs\"], keep='first')\n",
    "gene_taxa_mismatch = gene_taxa_unique[gene_taxa_unique.duplicated(subset=[\"EntrezIDs\"], keep=False)]\n",
    "assert len(gene_taxa_mismatch) == 0, f\"There's still {len(gene_taxa_mismatch)} duplicated rows\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In 16 TFs, one EntrezID is mapped to 2 different SwissProt Accession IDs. They have been joined by |. Example:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Associated.Gene.Name</th>\n",
       "      <th>Official name</th>\n",
       "      <th>EntrezIDs</th>\n",
       "      <th>TaxaIDs</th>\n",
       "      <th>UniProt</th>\n",
       "      <th>Ensembl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>ABL1</td>\n",
       "      <td>Tyrosine-protein kinase ABL1</td>\n",
       "      <td>100909750</td>\n",
       "      <td>10116</td>\n",
       "      <td>E9PT20|F1M0A6</td>\n",
       "      <td>ENSRNOG00000009371|ENSRNOG00000009371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CHCHD2</td>\n",
       "      <td>Coiled-coil-helix-coiled-coil-helix domain-containing protein 2</td>\n",
       "      <td>316643</td>\n",
       "      <td>10116</td>\n",
       "      <td>Q5BJB3|M0R785</td>\n",
       "      <td>ENSRNOG00000051180|ENSRNOG00000051180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GROUP DUPLICATED ROWS & GET FINAL TFCHECKPOINT DATASET\n",
    "# In some rows, EntrezID, TaxaID & Name are the same -> Only SwissProt changes. We will group those rows\n",
    "\n",
    "# Remove all useless columns\n",
    "columns_to_keep = TF_exploded.columns.tolist()\n",
    "columns_to_remove = ['Entrez.Taxa.ID', 'Entrez.Gene.ID', 'UniProt.SwissProt.Accession', 'Ensembl.Gene.ID', 'UniProt', 'Ensembl']\n",
    "for column in columns_to_remove:\n",
    "    columns_to_keep.remove(column)\n",
    "\n",
    "# Group duplicated rows, with a | in between for UniProt & Ensembl.\n",
    "TF_exploded = TF_exploded.groupby(columns_to_keep, dropna=False).agg({\n",
    "    \"UniProt\": lambda x: \"|\".join(x),\n",
    "    \"Ensembl\": lambda x: \"|\".join(x)\n",
    "}).reset_index()\n",
    "\n",
    "# Display one example\n",
    "mask = TF_exploded[\"UniProt\"].str.contains(\"\\|\")\n",
    "md(f\"In {mask.sum()} TFs, one EntrezID is mapped to 2 different SwissProt Accession IDs. They have been joined by |. Example:\")\n",
    "display(HTML(TF_exploded[mask][:2][[\"Associated.Gene.Name\", \"Official name\", \"EntrezIDs\", \"TaxaIDs\", \"UniProt\", \"Ensembl\"]].to_html(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# dbTF NCBI IDs in TFCheckpoint:  4460\n",
      "# GTF NCBI IDs in TFCheckpoint:   146\n",
      "# coTF NCBI IDs in TFCheckpoint:  3598\n"
     ]
    }
   ],
   "source": [
    "TFCheckpoint_sets = {}\n",
    "for TF_type in TF_types:\n",
    "    mask = TF_exploded[TFCheckpoint_cols[TF_type]].notna().any(axis=1)  # Checks across the specified columns\n",
    "    TFCheckpoint_sets[TF_type] = set(TF_exploded[mask]['EntrezIDs'])\n",
    "    print(f\"# {TF_type} NCBI IDs in TFCheckpoint: {len(TFCheckpoint_sets[TF_type]):>5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get final TF set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We consider 9297 NCBI Gene IDs to be TFs\n"
     ]
    }
   ],
   "source": [
    "TF_IDs_dict = {}\n",
    "for TF_type in TF_types:\n",
    "    TF_IDs = set()\n",
    "\n",
    "    # Get QuickGO TFs\n",
    "    QuickGO_subset = QuickGO[QuickGO['TF type'] == TF_type]\n",
    "    QuickGO_IDs = [j for i in list(QuickGO_subset['TF ID']) for j in i]\n",
    "    TF_IDs.update(set(QuickGO_IDs))\n",
    "        \n",
    "    # Get TFCheckpoint TFs\n",
    "    TF_IDs.update(TFCheckpoint_sets[TF_type])\n",
    "\n",
    "    # Save into dictionary\n",
    "    TF_IDs_dict[TF_type] = TF_IDs\n",
    "\n",
    "\n",
    "# GTFs must not contain dbTFs, and coTFs must not contain neither dbTFs nor GTFs.\n",
    "TF_IDs_dict['GTF'].difference_update(TF_IDs_dict['dbTF'])\n",
    "TF_IDs_dict['coTF'].difference_update(TF_IDs_dict['dbTF'])\n",
    "TF_IDs_dict['coTF'].difference_update(TF_IDs_dict['GTF'])\n",
    "\n",
    "# Add the less likely coTFs as a subset of the coTFs\n",
    "TF_IDs_dict['ll_coTF'] = ll_coTF.intersection(TF_IDs_dict['coTF'])\n",
    "\n",
    "# Save them as a list\n",
    "for TF_type in TF_types + ['ll_coTF']:\n",
    "    path = get_TF_ids_path(TF_type, postprocessing_path)\n",
    "    with open(path, 'w') as f:\n",
    "        for TF in TF_IDs_dict[TF_type]:\n",
    "            f.write(TF + \"\\n\")\n",
    "\n",
    "\n",
    "# Combine all TFs & save them as a list\n",
    "all_TF_ids = TF_IDs_dict['dbTF'].union(TF_IDs_dict['GTF']).union(TF_IDs_dict['coTF'])\n",
    "print(f\"We consider {len(all_TF_ids)} NCBI Gene IDs to be TFs\")\n",
    "\n",
    "with open(get_TF_ids_path('tf', postprocessing_path), 'w') as f:\n",
    "    for TF in all_TF_ids:\n",
    "        f.write(TF + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out less likely coTFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get coTF human orthologs for filtering\n",
    "coTFs are converted to their human orthologs to then apply a filtering and discard non-coTFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous list of coTFs is polluted with proteins that do not act as coTFs. They are filtered out by analysing their human orthologs. For that:\n",
    "1. coTFs are separarted in human & mouse/rat coTFs.\n",
    "2. Human orthologs for mouse/rat coTFs are oubtained. Those mouse/rat coTFs without human orthologs (24/2822 cases) are discarded.\n",
    "3. Mouse/rat IDs frequently have +1 human orthologs. To not pollute the final coTF list with unnecessary orthologs, we check, for each mouse/rat coTF, if any of the human orthologs is already in the human coTF list. The majority has at least one, so the other orthologs were not added.\n",
    "4. For those that did not have any ortholog in the human coTF list (99 cases), we add all their human orthologs (115 additional ones).\n",
    "5. We create a TSV file with the resulting 1876 human coTF IDs + their NCBI Symbol, which is used to find filtering strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "data_path = '../../data/'\n",
    "orthologs_path =       data_path + 'external/human_HGNC_orthologs/'\n",
    "all_human_coTFs_path = data_path + 'tmp/all_human_ortholog_coTFs.tsv'\n",
    "r_m_coTFs_wo_orthologs_path = data_path + 'tmp/r_m_coTFs_wo_orthologs.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "from Bio import Entrez\n",
    "import os\n",
    "Entrez.email = \"example24@gmail.com\"\n",
    "\n",
    "def retrieve_annotations(id_list):\n",
    "    \"\"\"Annotates Entrez Gene IDs using Bio.Entrez, in particular epost (to\n",
    "    submit the data to NCBI) and esummary to retrieve the information.\n",
    "    Returns a list of dictionaries with the annotations.\"\"\"\n",
    "\n",
    "    request = Entrez.epost(\"gene\", id=\",\".join(id_list))\n",
    "    result = Entrez.read(request)\n",
    "    webEnv = result[\"WebEnv\"]\n",
    "    queryKey = result[\"QueryKey\"]\n",
    "    data = Entrez.esummary(db=\"gene\", webenv=webEnv, query_key=queryKey)\n",
    "    annotations = Entrez.read(data)\n",
    "    annotationsSummary = annotations['DocumentSummarySet']['DocumentSummary']\n",
    "\n",
    "    assert len(id_list) == len(annotationsSummary), f\"id_list and annotationsSummary are of different length: {len(id_list)} != {len(annotationsSummary)}\"\n",
    "\n",
    "    return annotationsSummary\n",
    "\n",
    "def get_HGNC_orthologs(coTF_ids: list, orthologs_path: str):\n",
    "    orthologs = pd.DataFrame()\n",
    "\n",
    "    for rodent in ['mouse', 'rat']:\n",
    "        # Load dataframe\n",
    "        path = os.path.join(orthologs_path, f\"human_{rodent}_hcop_fifteen_column.txt\")\n",
    "        hgnc_df = pd.read_csv(path, sep='\\t', header=0, keep_default_na=False, dtype=str)\n",
    "\n",
    "        # Rename\n",
    "        hgnc_df = hgnc_df.rename(columns={f\"{rodent}_entrez_gene\": \"entrez_gene\", f\"{rodent}_symbol\": \"symbol\"})\n",
    "        \n",
    "        orthologs = pd.concat([orthologs, hgnc_df])\n",
    "\n",
    "    # Keep only IDs present in the coTF list\n",
    "    orthologs = orthologs[orthologs['entrez_gene'].isin(coTF_ids)]\n",
    "\n",
    "    # Remove all rows that don't have a human entrez ID or hgnc ID\n",
    "    m = (orthologs['human_entrez_gene'] != '-') | (orthologs['hgnc_id'] != '-')\n",
    "    orthologs = orthologs[m]\n",
    "\n",
    "    # Join with ';' when an EntrezID has more than 1 human ortholog\n",
    "    agg_funcs = {\n",
    "        \"symbol\": lambda x: ';'.join(x.unique()),\n",
    "        \"human_entrez_gene\": lambda x: ';'.join(x.unique()),\n",
    "        \"hgnc_id\": lambda x: ';'.join(x.unique()),\n",
    "        \"human_symbol\": lambda x: ';'.join(x.unique())\n",
    "    }\n",
    "    orthologs = orthologs.groupby(['entrez_gene']).agg(agg_funcs).reset_index()\n",
    "\n",
    "    # Show how many we get\n",
    "    print(f\"We get ortholog info for {len(orthologs)}/{len(coTF_ids)} Gene IDs\")\n",
    "\n",
    "\n",
    "    # Convert into dictionary\n",
    "    orthologs_map = orthologs.set_index('entrez_gene').to_dict(orient='index')\n",
    "\n",
    "    return orthologs_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We get ortholog info for 2798/2822 Gene IDs\n",
      "In 2699 / 2798 mouse/rat IDs, at least one of their orthologs is also in the human_coTFs list.\n",
      "For those that don't, we'll take all its human orthologs: 115 extra human IDs\n"
     ]
    }
   ],
   "source": [
    "# Load coTF IDs list\n",
    "path = get_TF_ids_path('coTF', out_data_path+\"postprocessing/\")\n",
    "with open(path, 'r') as f:\n",
    "    coTF_ids = [l.strip('\\n') for l in f]\n",
    "\n",
    "# Create map between the ID & its Name / TaxID\n",
    "annotationsSummary = retrieve_annotations(coTF_ids)\n",
    "EntrezIDtoSymbol = {ID : {'Name': annotation[\"Name\"], 'TaxID': annotation['Organism']['TaxID']} for ID, annotation in zip(coTF_ids, annotationsSummary)}\n",
    "\n",
    "# Separate coTFs between human & mouse/rat\n",
    "human_coTFs = {id for id, val in EntrezIDtoSymbol.items() if val['TaxID'] == '9606'}\n",
    "m_r_coTFs   = {id for id, val in EntrezIDtoSymbol.items() if val['TaxID'] != '9606'}\n",
    "\n",
    "# Get mouse/rat human orthologs as a dataframe\n",
    "orthologs_map = get_HGNC_orthologs(m_r_coTFs, orthologs_path)\n",
    "orth_df = pd.DataFrame.from_dict(orthologs_map, orient='index')\n",
    "orth_df.index.name = 'ID'\n",
    "r_m_coTFs_wo_orthologs = m_r_coTFs.difference(set(orth_df.index))\n",
    "\n",
    "# Mask for mouse/rat IDs with an ortholog inside the human_coTFs list\n",
    "def contains_coTF(ids: str, coTFs: set):\n",
    "    return any(id in coTFs for id in ids.split(';'))\n",
    "m = orth_df['human_entrez_gene'].apply(lambda ids: contains_coTF(ids, human_coTFs))\n",
    "print(f\"In {m.sum()} / {len(orth_df)} mouse/rat IDs, at least one of their orthologs is also in the human_coTFs list.\")\n",
    "\n",
    "# Get all human orthologs of mouse/rat IDs w/o orthologs already present in human_coTFs\n",
    "remaining_m_r_human_orthologs = {id for id_list in orth_df[~m]['human_entrez_gene'] for id in id_list.split(';')}\n",
    "print(f\"For those that don't, we'll take all its human orthologs: {len(remaining_m_r_human_orthologs)} extra human IDs\")\n",
    "\n",
    "# Join human coTFs + the other human orthologs.\n",
    "all_human_IDs = remaining_m_r_human_orthologs | human_coTFs\n",
    "\n",
    "# Get the symbol from Entrez & save the list as a tsv file.\n",
    "annotationsSummary = retrieve_annotations(all_human_IDs)\n",
    "EntrezIDtoSymbol = {ID : {'Name': annotation[\"Name\"], 'TaxID': annotation['Organism']['TaxID']} for ID, annotation in zip(coTF_ids, annotationsSummary)}\n",
    "all_human_coTFs_df = pd.DataFrame.from_dict(EntrezIDtoSymbol, orient='index')\n",
    "all_human_coTFs_df.index.name = 'ID'\n",
    "all_human_coTFs_df.to_csv(all_human_coTFs_path, sep='\\t')\n",
    "\n",
    "# Do the same for the mouse/rat coTFs without human orthologs\n",
    "annotationsSummary = retrieve_annotations(r_m_coTFs_wo_orthologs)\n",
    "EntrezIDtoSymbol = {ID : {'Name': annotation[\"Name\"], 'TaxID': annotation['Organism']['TaxID']} for ID, annotation in zip(r_m_coTFs_wo_orthologs, annotationsSummary)}\n",
    "r_m_coTFs_wo_orthologs_df = pd.DataFrame.from_dict(EntrezIDtoSymbol, orient='index')\n",
    "r_m_coTFs_wo_orthologs_df.index.name = 'ID'\n",
    "r_m_coTFs_wo_orthologs_df.to_csv(r_m_coTFs_wo_orthologs_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for the mouse/rat coTFs without human orthologs\n",
    "annotationsSummary = retrieve_annotations(r_m_coTFs_wo_orthologs)\n",
    "EntrezIDtoSymbol = {ID : {'Name': annotation[\"Name\"], 'TaxID': annotation['Organism']['TaxID']} for ID, annotation in zip(r_m_coTFs_wo_orthologs, annotationsSummary)}\n",
    "r_m_coTFs_wo_orthologs_df = pd.DataFrame.from_dict(EntrezIDtoSymbol, orient='index')\n",
    "r_m_coTFs_wo_orthologs_df.index.name = 'ID'\n",
    "r_m_coTFs_wo_orthologs_df.to_csv(r_m_coTFs_wo_orthologs_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send all coTFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method did not properly separate them: there were still several \"less likely\" coTFs in the \"likely\" list. Therefore, we use a different approach:\n",
    "1. Get all coTFs. Sort them alphabetically (case insensitive). \n",
    "2. Create 'likely' column, and mark it as \"less likely\" if the symbol is present in the list of less_likely_human_coTFs (case insensitive)\n",
    "3. Create an Excel, to recheck the likely columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all coTF Symbols from Entrez\n",
    "annotationsSummary = retrieve_annotations(coTF_ids)\n",
    "EntrezIDtoSymbol = {ID : {'Name': annotation[\"Name\"], 'TaxID': annotation['Organism']['TaxID']} for ID, annotation in zip(coTF_ids, annotationsSummary)}\n",
    "all_coTFs = pd.DataFrame.from_dict(EntrezIDtoSymbol, orient='index')\n",
    "all_coTFs.index.name = 'ID'\n",
    "all_coTFs.sort_values(by='Name', inplace=True, key=lambda col: col.str.lower())\n",
    "\n",
    "# Get list of less likely human coTFs\n",
    "data_path = '../../data/'\n",
    "orthologs_path               = data_path + 'external/human_HGNC_orthologs/'\n",
    "ll_human_coTFs_path = data_path + 'postprocessing/all_human_ortholog_coTFs_labelled_less_likely.txt'\n",
    "all_coTFs_likely_path =        data_path + 'tmp/all_coTFs_likely.tsv'\n",
    "\n",
    "# Get the set of the human IDs less likely to be coTFs\n",
    "human_coTFs_labelled = pd.read_csv(ll_human_coTFs_path, sep='\\t', header=0, dtype='str')\n",
    "h_ll_coTFs_symbol = set(human_coTFs_labelled[human_coTFs_labelled['less likely'] == 'less likely']['Name'])\n",
    "\n",
    "# Create a \"likely\" column\n",
    "all_coTFs['likely'] = all_coTFs['Name'].str.upper().isin(h_ll_coTFs_symbol).apply(lambda x: 'less likely' if x else 'likely')\n",
    "\n",
    "# Rename columns\n",
    "all_coTFs = all_coTFs.rename(columns={'Name': 'NCBI Symbol', 'ID': 'NCBI ID'})\n",
    "all_coTFs.index.name = 'NCBI ID'\n",
    "\n",
    "# Save\n",
    "all_coTFs.to_csv(all_coTFs_likely_path, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".general_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
