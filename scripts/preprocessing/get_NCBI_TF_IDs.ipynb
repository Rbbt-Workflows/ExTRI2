{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get List of TF NCBI IDs \n",
    "Get the list of IDs that will be considered TF, both from GO terms and from TFCheckpoint.\n",
    "\n",
    "The GO terms and columns from TFCheckpoint used for each of the TF types is detailed in the cell below. The specific procedure used is explained in their respective sections:  [Get GO terms](#get-go-terms) and [GET TFCheckpoint terms](#get-tfcheckpoint-terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3>Table of contents</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "[Get List of TF NCBI IDs](#Get-List-of-TF-NCBI-IDs)\n",
       "- [Setup](#Setup)\n",
       "- [Get GO terms](#Get-GO-terms)\n",
       "- [Get TFCheckpoint terms](#Get-TFCheckpoint-terms)\n",
       "- [Get final TF set for the pipeline](#Get-final-TF-set-for-the-pipeline)\n",
       "- [Create final TF table](#Create-final-TF-table)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "__import__('sys').path.append('../common/'); __import__('notebook_utils').table_of_contents('get_NCBI_TF_IDs.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import requests\n",
    "\n",
    "from Bio import Entrez\n",
    "# *Always* tell NCBI who you are\n",
    "Entrez.email = \"example24@gmail.com\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('../common')\n",
    "from notebook_utils import h3, h4, h5, md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GO TERM & TFCHECKPOINT VARIABLES\n",
    "\n",
    "# GO terms used:\n",
    "GO_dbTF = [\"GO:0003700\"]\n",
    "GO_coTF = [\"GO:0003712\", \"GO:0001098\", \"GO:0002039\" , \"GO:0008134\" , \"GO:0042393\", \"GO:0046332\", \"GO:0006325\", \"GO:0140993\"]\n",
    "\n",
    "# Columns from TFCheckpoint used:\n",
    "TFCheckpoint_cols = {\n",
    "    'dbTF': ['TFclass.present.merged', 'lambert_2018.present', 'Lovering_2021.present'],\n",
    "    'coTF': ['animal_tfdb_Homo_sapiens_cofactors.present', 'animal_tfdb_Mus_musculus_cofactors.present', 'animal_tfdb_Rattus_norvegicus_cofactors.present', \n",
    "             'tcof_cotf_human.present', 'tcof_cotf_mouse.present']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "in_data_path = '../../data/external/TF_id/'\n",
    "postprocessing_path = '../../data/postprocessing/'\n",
    "\n",
    "less_likely_coTFs_path = postprocessing_path + 'all_coTFs_likely_checked_updated_AL.txt'\n",
    "\n",
    "QuickGO_dbTF_path = in_data_path + \"QuickGO-annotations-dbTF.tsv\"\n",
    "QuickGO_coTF_path = in_data_path + \"QuickGO-annotations-coTF.tsv\"\n",
    "TFCheckpoint_path = in_data_path + \"TFCheckpoint.tsv\"\n",
    "\n",
    "# Define a function to construct the TF types path (ll_coTFs are introduced later)\n",
    "TF_types = [\"dbTF\", \"coTF\"]\n",
    "def get_TF_ids_path(TF_type, out_data_path):\n",
    "    return f\"{out_data_path}{TF_type}_entrez_code.list\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD LIKELY & LESS LIKELY COTFs\n",
    "ll_coTFs_db = pd.read_csv(less_likely_coTFs_path, sep=\"\\t\", dtype='str')\n",
    "m = ll_coTFs_db['likely'] == 'likely'\n",
    "ll_coTF = set(ll_coTFs_db[~m]['NCBI ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get GO terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Obtained the GO terms from [QuickGO](https://www.ebi.ac.uk/QuickGO/annotations?taxonId=10116,9606,10090&taxonUsage=exact&goId=GO:0140993,GO:0003712,GO:0003700,GO:0140223,GO:0001098,GO:0002039,GO:0008134,GO:0042393,GO:0046332,GO:0006325&goUsageRelationships=is_a,part_of,occurs_in&goUsage=descendants&geneProductSubset=Swiss-Prot&geneProductType=protein), using the terms shown below. Used as filters:\n",
    "\n",
    "* **Taxon:** 10116, 9606, 10090, Exact match (do not include descendants)\n",
    "* **Gene products:** Reviewed (not Unreviewed)\n",
    "* **GO terms:**.\n",
    "  * **dbTF:** GO:0003700 (DNA-binding transcription factor activity)\n",
    "  * **coTF:** GO:0140993 (histone modifying activity), GO:0008134 (transcription factor binding), GO:0003712 (transcription coregulator activity), GO:0001098 (basal transcription machinery binding), GO:0002039 (p53 binding), GO:0042393 (histone binding), GO:0046332 (SMAD binding) and GO:0006325 (chromatin organization)\n",
    "* **Export as:** tsv\n",
    "\n",
    "Downloaded separately a QuickGO tsv file for each TF type and renamed it as shown above in the setup section.\n",
    "\n",
    "As some terms can be identified as pertaining to more than 1 type, we have followed this hierarchy to remove duplicates:\n",
    "1. dbTF\n",
    "2. coTF\n",
    "\n",
    "That implies that if a protein is classified as both dbTF and coTF, the protein's classification will be dbTF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES\n",
    "# Species:\n",
    "organismToTaxID = {\n",
    "    \"hsapiens\": \"9606\",\n",
    "    \"mmusculus\": \"10090\",\n",
    "    \"rnorvegicus\": \"10116\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "def fetch_gene_ids_gprofiler(gene_symbols: list, organism: str) -> dict:\n",
    "    \"Get NCBI Gene IDs from GProfiler\"\n",
    "    symboltoID = {}\n",
    "\n",
    "    # Query the IDs from GProfiler\n",
    "    result = requests.post(\n",
    "        url='https://biit.cs.ut.ee/gprofiler/api/convert/convert/',\n",
    "        json={\n",
    "            'organism': organism,\n",
    "            'target':'ENTREZGENE_ACC',\n",
    "            'query': gene_symbols,\n",
    "        }\n",
    "        )\n",
    "    \n",
    "    # Create a list of extracted IDs per symbol\n",
    "    for r in result.json()['result']:\n",
    "        incoming = r['incoming']\n",
    "        converted = r['converted']\n",
    "\n",
    "        if incoming not in symboltoID:\n",
    "            symboltoID[incoming] = []\n",
    "        if converted != 'None':\n",
    "            symboltoID[incoming].append(converted)\n",
    "\n",
    "    return symboltoID\n",
    "\n",
    "def retrieve_annotations_entrez(id_list):\n",
    "    \"\"\"Annotates Entrez Gene IDs using Bio.Entrez, in particular epost (to\n",
    "    submit the data to NCBI) and esummary to retrieve the information.\n",
    "    Returns a list of dictionaries with the annotations.\"\"\"\n",
    "\n",
    "    request = Entrez.epost(\"gene\", id=\",\".join(id_list))\n",
    "    result = Entrez.read(request)\n",
    "    webEnv = result[\"WebEnv\"]\n",
    "    queryKey = result[\"QueryKey\"]\n",
    "    data = Entrez.esummary(db=\"gene\", webenv=webEnv, query_key=queryKey)\n",
    "    annotations = Entrez.read(data)\n",
    "    annotationsSummary = annotations['DocumentSummarySet']['DocumentSummary']\n",
    "\n",
    "    assert len(id_list) == len(annotationsSummary), f\"id_list and annotationsSummary are of different length: {len(id_list)} != {len(annotationsSummary)}\"\n",
    "\n",
    "    return annotationsSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25053 rows were retrieved.\n",
      "Removing duplicates, we retrieve 6005 symbols:\n",
      "\t2184 dbTFs\n",
      "\t3156 coTFs\n"
     ]
    }
   ],
   "source": [
    "# JOIN QUICKGO TSVs\n",
    "# Create joined DataFrame from the 3 TF types\n",
    "QuickGO_dbTF = pd.read_csv(QuickGO_dbTF_path, sep='\\t', header=0, keep_default_na=False, dtype='str')\n",
    "QuickGO_dbTF['TF type'] = 'dbTF'\n",
    "QuickGO_coTF = pd.read_csv(QuickGO_coTF_path, sep='\\t', header=0, keep_default_na=False, dtype='str')\n",
    "QuickGO_coTF['TF type'] = 'coTF' \n",
    "\n",
    "QuickGO = pd.concat([QuickGO_dbTF, QuickGO_coTF], axis=0)\n",
    "\n",
    "print(f\"{len(QuickGO['SYMBOL'])} rows were retrieved.\")\n",
    "\n",
    "# Only keep relevant columns\n",
    "QuickGO = QuickGO[['SYMBOL', 'TAXON ID', 'TF type', 'GO TERM']]\n",
    "\n",
    "# Drop repeated cells. Use the following priority if duplicates of different TF type\n",
    "priority = {'dbTF': 0, 'coTF': 1}\n",
    "QuickGO['priority'] = QuickGO['TF type'].map(priority)\n",
    "QuickGO = QuickGO.sort_values(by=['SYMBOL', 'TAXON ID', 'priority'])\n",
    "QuickGO = QuickGO.drop_duplicates(subset=['SYMBOL', 'TAXON ID', 'GO TERM'], keep='first')\n",
    "QuickGO = (\n",
    "    QuickGO\n",
    "    .groupby([\"SYMBOL\", \"TAXON ID\"], as_index=False)\n",
    "    .agg({\n",
    "        'TF type': lambda x: \";\".join(sorted(set(x.astype(str)))),\n",
    "        'GO TERM': 'first'   # take top-priority GO TERM\n",
    "    })\n",
    ")\n",
    "\n",
    "# Show results\n",
    "print(f\"Removing duplicates, we retrieve {len(QuickGO['SYMBOL'])} symbols:\")\n",
    "for TF_type in ('dbTF', 'coTF'):\n",
    "    print(f\"\\t{len(QuickGO[QuickGO['TF type'] == TF_type]['SYMBOL'])} {TF_type}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GProfiler retrieved 98.2% NCBI Gene IDs from the QuickGO symbols\n",
      "It couldn't retrieve 108 of them\n"
     ]
    }
   ],
   "source": [
    "# GET GENE IDs FROM GPROFILER\n",
    "for organism in ['hsapiens', 'mmusculus', 'rnorvegicus']:\n",
    "    # Get IDs from GProfiler\n",
    "    symbols = list(QuickGO[QuickGO['TAXON ID'] == organismToTaxID[organism]]['SYMBOL'].unique())\n",
    "    symboltoID = fetch_gene_ids_gprofiler(symbols, organism)\n",
    "\n",
    "    # Map them to QuickGO db\n",
    "    m = QuickGO['TAXON ID'] == organismToTaxID[organism]\n",
    "    QuickGO.loc[m, \"TF ID\"] = QuickGO[m]['SYMBOL'].apply(lambda symbol: symboltoID[symbol])\n",
    "\n",
    "m = ~(QuickGO['TF ID'].str.len() == 0)\n",
    "print(f'GProfiler retrieved {m.sum() / len(QuickGO):.1%} NCBI Gene IDs from the QuickGO symbols')\n",
    "print(f\"It couldn't retrieve {(~m).sum()} of them\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve the rest through a query to Entrez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9606\n",
      "9606\n",
      "9606\n",
      "10090\n",
      "10090\n",
      "10116\n",
      "10116\n",
      "10116\n",
      "10116\n"
     ]
    }
   ],
   "source": [
    "# QUERY THE REST FROM ENTREZ\n",
    "# Entrez often gets stuck, so it's best to give some time between queries\n",
    "import time\n",
    "ids = []\n",
    "for TaxID in ['9606', '10090', '10116']:\n",
    "\n",
    "    # Get the symbols with missing ID\n",
    "    m = (QuickGO['TF ID'].str.len() == 0) & (QuickGO['TAXON ID'] == TaxID)\n",
    "    missing_symbols = list(QuickGO[m]['SYMBOL'].unique())\n",
    "\n",
    "    nSymbols = 15 # Symbols per query\n",
    "    for i in range(0, len(missing_symbols), nSymbols):\n",
    "        symbols = missing_symbols[i:i+nSymbols]\n",
    "        # Query them from Entrez\n",
    "        symbolsQuery = sorted([s+'[Preferred Symbol]' for s in symbols])\n",
    "        query = f'({\" OR \".join(symbolsQuery)}) AND txid{TaxID}[Organism]'\n",
    "        handle = Entrez.esearch(db=\"gene\", term=query, retmode=\"xml\")\n",
    "        record = Entrez.read(handle)\n",
    "        ids.append(record.get(\"IdList\", []))\n",
    "        print(TaxID)\n",
    "        # Sleep between queries to not get blocked\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 out of the remaining 108 missing have been retrieved through Entrez.\n",
      "Combined with Entrez, we retrieved 99.4% NCBI Gene IDs from the QuickGO symbols\n",
      "There's 35 NCBI Gene IDs that couldn't be retrieved\n"
     ]
    }
   ],
   "source": [
    "# Map IDs back to its symbol & organism\n",
    "all_ids = [j for i in ids for j in i]\n",
    "remaining_annotations = retrieve_annotations_entrez(all_ids)\n",
    "\n",
    "# Make a map from symbol/TaxID to Gene ID\n",
    "symboltoID_entrez = {'9606': {}, '10116': {}, '10090': {}}\n",
    "for id, ann in zip(all_ids, remaining_annotations):\n",
    "    symbol = ann['Name']\n",
    "    TaxID = ann['Organism']['TaxID']\n",
    "\n",
    "    if symbol not in symboltoID_entrez[TaxID]:\n",
    "        symboltoID_entrez[TaxID][symbol] = [id]\n",
    "    else:\n",
    "        symboltoID_entrez[TaxID][symbol].append(id)\n",
    "\n",
    "# Check how many we retrieved from Entrez\n",
    "m = (QuickGO['TF ID'].str.len() == 0)\n",
    "print(f\"{len(all_ids)} out of the remaining {m.sum()} missing have been retrieved through Entrez.\")\n",
    "\n",
    "# Map the retrieved ones to the QuickGO db\n",
    "for TaxID in symboltoID_entrez.keys():\n",
    "    m = (QuickGO['TF ID'].str.len() == 0) & (QuickGO['TAXON ID'] == TaxID)\n",
    "    QuickGO.loc[m, \"TF ID\"] = QuickGO[m]['SYMBOL'].apply(lambda symbol: symboltoID_entrez[TaxID].get(symbol, []))\n",
    "\n",
    "m = ~(QuickGO['TF ID'].str.len() == 0)\n",
    "print(f'Combined with Entrez, we retrieved {m.sum() / len(QuickGO):.1%} NCBI Gene IDs from the QuickGO symbols')\n",
    "print(f\"There's {(~m).sum()} NCBI Gene IDs that couldn't be retrieved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Get TFCheckpoint terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4>EntrezIDs mapped to 2 species</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "There are 5 Entrez IDs that are mapped to both Rat and Mouse:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>EntrezIDs</th>\n",
       "      <th>Associated.Gene.Name</th>\n",
       "      <th>TaxaIDs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20851</td>\n",
       "      <td>STAT5A</td>\n",
       "      <td>10116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20851</td>\n",
       "      <td>STAT5B</td>\n",
       "      <td>10090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22764</td>\n",
       "      <td>ZFX</td>\n",
       "      <td>10090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22764</td>\n",
       "      <td>ZFY</td>\n",
       "      <td>10116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24918</td>\n",
       "      <td>STAT5A</td>\n",
       "      <td>10090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24918</td>\n",
       "      <td>STAT5B</td>\n",
       "      <td>10116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25126</td>\n",
       "      <td>STAT5A</td>\n",
       "      <td>10090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25126</td>\n",
       "      <td>STAT5B</td>\n",
       "      <td>10116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>367832</td>\n",
       "      <td>ZFX</td>\n",
       "      <td>10116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>367832</td>\n",
       "      <td>ZFY</td>\n",
       "      <td>10090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "They have been searched in the NCBI and corrected manually"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## LOAD & PREPROCESS TFCHECKPOINT TSV\n",
    "# Load TFCheckpoint dataset\n",
    "TFCheckpoint_df = pd.read_csv(TFCheckpoint_path, sep='\\t', header=0)\n",
    "str_cols = ['Associated.Gene.Name', 'Synonyms', 'Official name', 'Entrez.Taxa.ID', 'Entrez.Gene.ID', 'UniProt.SwissProt.Accession', 'Ensembl.Gene.ID']\n",
    "TFCheckpoint_df[str_cols] = TFCheckpoint_df[str_cols].astype(str)\n",
    "\n",
    "\n",
    "# Split Entrez, Taxa & UniProt into individual IDs\n",
    "TFCheckpoint_df['EntrezIDs'] = TFCheckpoint_df['Entrez.Gene.ID'].str.split('|')\n",
    "TFCheckpoint_df['TaxaIDs'] = TFCheckpoint_df['Entrez.Taxa.ID'].str.split('|')\n",
    "TFCheckpoint_df['UniProt'] = TFCheckpoint_df['UniProt.SwissProt.Accession'].str.split('|')\n",
    "TFCheckpoint_df['Ensembl'] = TFCheckpoint_df['Ensembl.Gene.ID'].str.split('|')\n",
    "\n",
    "# Explode the TF\n",
    "TFCheckpoint_exploded = TFCheckpoint_df.explode(['EntrezIDs', 'TaxaIDs', 'UniProt', 'Ensembl'])\n",
    "TFCheckpoint_exploded = TFCheckpoint_exploded[TFCheckpoint_exploded[\"EntrezIDs\"] != ''] # Drop empty rows (Appeared when | was present at the end, e.g. \"9454|3425|\")\n",
    "TFCheckpoint_exploded = TFCheckpoint_exploded[TFCheckpoint_exploded[\"UniProt\"] != '']\n",
    "TFCheckpoint_exploded = TFCheckpoint_exploded[TFCheckpoint_exploded[\"Ensembl\"] != '']\n",
    "\n",
    "# Check whether each EntrezID only matches to 1 TaxaID:\n",
    "gene_taxa_unique = TFCheckpoint_exploded.drop_duplicates(subset=[\"EntrezIDs\", \"TaxaIDs\"], keep='first')\n",
    "gene_taxa_mismatch = gene_taxa_unique[gene_taxa_unique.duplicated(subset=[\"EntrezIDs\"], keep=False)]\n",
    "h4(\"EntrezIDs mapped to 2 species\")\n",
    "md(f\"There are {len(gene_taxa_mismatch['EntrezIDs'].unique())} Entrez IDs that are mapped to both Rat and Mouse:\")\n",
    "display(HTML(gene_taxa_mismatch[[\"EntrezIDs\", \"Associated.Gene.Name\", \"TaxaIDs\"]].sort_values(by=['EntrezIDs']).to_html(index=False)))\n",
    "\n",
    "# DROP MISMATCHING ROWS\n",
    "rows_to_drop = [\n",
    "    ['STAT5A', '20851', '10116'],\n",
    "    ['STAT5A', '25126', '10090'],\n",
    "    ['ZFY', '367832', '10090'],\n",
    "    ['ZFY', '22764', '10116'],\n",
    "    ['STAT5B', '24918', '10116']\n",
    "]\n",
    "for row in rows_to_drop:\n",
    "    to_drop = (TFCheckpoint_exploded[\"Associated.Gene.Name\"] == row[0]) & (TFCheckpoint_exploded[\"EntrezIDs\"] == row[1])\n",
    "    assert to_drop.sum() == 1, f\"{to_drop.sum()} rows are being dropped instead of 1\"\n",
    "    TFCheckpoint_exploded = TFCheckpoint_exploded[~to_drop]\n",
    "to_change = (TFCheckpoint_exploded[\"Associated.Gene.Name\"] == \"STAT5A\") & (TFCheckpoint_exploded[\"EntrezIDs\"] == \"24918\")\n",
    "assert to_change.sum() == 1, f\"{to_change.sum()} rows are being dropped instead of 1\"\n",
    "TFCheckpoint_exploded.loc[to_change, \"TaxaIDs\"] = \"10116\"\n",
    "md(\"They have been searched in the NCBI and corrected manually\")\n",
    "\n",
    "# Assert there's no duplicates anymore\n",
    "gene_taxa_unique = TFCheckpoint_exploded.drop_duplicates(subset=[\"EntrezIDs\", \"TaxaIDs\"], keep='first')\n",
    "gene_taxa_mismatch = gene_taxa_unique[gene_taxa_unique.duplicated(subset=[\"EntrezIDs\"], keep=False)]\n",
    "assert len(gene_taxa_mismatch) == 0, f\"There's still {len(gene_taxa_mismatch)} duplicated rows\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In 16 TFs, one EntrezID is mapped to 2 different SwissProt Accession IDs. They have been joined by |. Example:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Associated.Gene.Name</th>\n",
       "      <th>Official name</th>\n",
       "      <th>EntrezIDs</th>\n",
       "      <th>TaxaIDs</th>\n",
       "      <th>UniProt</th>\n",
       "      <th>Ensembl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>ABL1</td>\n",
       "      <td>Tyrosine-protein kinase ABL1</td>\n",
       "      <td>100909750</td>\n",
       "      <td>10116</td>\n",
       "      <td>E9PT20|F1M0A6</td>\n",
       "      <td>ENSRNOG00000009371|ENSRNOG00000009371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CHCHD2</td>\n",
       "      <td>Coiled-coil-helix-coiled-coil-helix domain-containing protein 2</td>\n",
       "      <td>316643</td>\n",
       "      <td>10116</td>\n",
       "      <td>Q5BJB3|M0R785</td>\n",
       "      <td>ENSRNOG00000051180|ENSRNOG00000051180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GROUP DUPLICATED ROWS & GET FINAL TFCHECKPOINT DATASET\n",
    "# In some rows, EntrezID, TaxaID & Name are the same -> Only SwissProt changes. We will group those rows\n",
    "\n",
    "# Remove all useless columns\n",
    "columns_to_keep = TFCheckpoint_exploded.columns.tolist()\n",
    "columns_to_remove = ['Entrez.Taxa.ID', 'Entrez.Gene.ID', 'UniProt.SwissProt.Accession', 'Ensembl.Gene.ID', 'UniProt', 'Ensembl']\n",
    "for column in columns_to_remove:\n",
    "    columns_to_keep.remove(column)\n",
    "\n",
    "# Group duplicated rows, with a | in between for UniProt & Ensembl.\n",
    "TFCheckpoint_exploded = TFCheckpoint_exploded.groupby(columns_to_keep, dropna=False).agg({\n",
    "    \"UniProt\": lambda x: \"|\".join(x),\n",
    "    \"Ensembl\": lambda x: \"|\".join(x)\n",
    "}).reset_index()\n",
    "\n",
    "# Display one example\n",
    "mask = TFCheckpoint_exploded[\"UniProt\"].str.contains(\"\\|\")\n",
    "md(f\"In {mask.sum()} TFs, one EntrezID is mapped to 2 different SwissProt Accession IDs. They have been joined by |. Example:\")\n",
    "display(HTML(TFCheckpoint_exploded[mask][:2][[\"Associated.Gene.Name\", \"Official name\", \"EntrezIDs\", \"TaxaIDs\", \"UniProt\", \"Ensembl\"]].to_html(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# dbTF NCBI IDs in TFCheckpoint:  4390\n",
      "# coTF NCBI IDs in TFCheckpoint:  3598\n"
     ]
    }
   ],
   "source": [
    "TFCheckpoint_sets = {}\n",
    "for TF_type in TF_types:\n",
    "    mask = TFCheckpoint_exploded[TFCheckpoint_cols[TF_type]].notna().any(axis=1)  # Checks across the specified columns\n",
    "    TFCheckpoint_sets[TF_type] = set(TFCheckpoint_exploded[mask]['EntrezIDs'])\n",
    "    print(f\"# {TF_type} NCBI IDs in TFCheckpoint: {len(TFCheckpoint_sets[TF_type]):>5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get final TF set for the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We consider 9195 NCBI Gene IDs to be TFs\n"
     ]
    }
   ],
   "source": [
    "# Get TF sets for each TF type\n",
    "TF_IDs_dict = {}\n",
    "for TF_type in TF_types:\n",
    "    TF_IDs = set()\n",
    "\n",
    "    # Get QuickGO TFs\n",
    "    QuickGO_subset = QuickGO[QuickGO['TF type'] == TF_type]\n",
    "    QuickGO_IDs = [j for i in list(QuickGO_subset['TF ID']) for j in i]\n",
    "    TF_IDs.update(set(QuickGO_IDs))\n",
    "        \n",
    "    # Get TFCheckpoint TFs\n",
    "    TF_IDs.update(TFCheckpoint_sets[TF_type])\n",
    "\n",
    "    # Save into dictionary\n",
    "    TF_IDs_dict[TF_type] = TF_IDs\n",
    "\n",
    "\n",
    "# coTFs must not contain dbTFs.\n",
    "TF_IDs_dict['coTF'].difference_update(TF_IDs_dict['dbTF'])\n",
    "\n",
    "# Add the less likely coTFs as a subset of the coTFs\n",
    "TF_IDs_dict['ll_coTF'] = ll_coTF.intersection(TF_IDs_dict['coTF'])\n",
    "\n",
    "# Save each of them as a list\n",
    "for TF_type in TF_types + ['ll_coTF']:\n",
    "    path = get_TF_ids_path(TF_type, postprocessing_path)\n",
    "    with open(path, 'w') as f:\n",
    "        for TF in TF_IDs_dict[TF_type]:\n",
    "            f.write(TF + \"\\n\")\n",
    "\n",
    "# Combine all TFs & save them as a list\n",
    "all_TF_ids = TF_IDs_dict['dbTF'].union(TF_IDs_dict['coTF'])\n",
    "print(f\"We consider {len(all_TF_ids)} NCBI Gene IDs to be TFs\")\n",
    "\n",
    "with open(get_TF_ids_path('tf', postprocessing_path), 'w') as f:\n",
    "    for TF in all_TF_ids:\n",
    "        f.write(TF + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create final TF table\n",
    "Create TF table to use in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "import os\n",
    "from Bio import Entrez\n",
    "import difflib\n",
    "Entrez.email = \"example24@gmail.com\"\n",
    "\n",
    "def retrieve_annotations(id_list):\n",
    "    \"\"\"Annotates Entrez Gene IDs using Bio.Entrez, in particular epost (to\n",
    "    submit the data to NCBI) and esummary to retrieve the information.\n",
    "    Returns a list of dictionaries with the annotations.\"\"\"\n",
    "\n",
    "    request = Entrez.epost(\"gene\", id=\",\".join(id_list))\n",
    "    result = Entrez.read(request)\n",
    "    webEnv = result[\"WebEnv\"] # type: ignore\n",
    "    queryKey = result[\"QueryKey\"] # type: ignore\n",
    "    data = Entrez.esummary(db=\"gene\", webenv=webEnv, query_key=queryKey)\n",
    "    annotations = Entrez.read(data)\n",
    "    annotationsSummary = annotations['DocumentSummarySet']['DocumentSummary'] # type: ignore\n",
    "\n",
    "    assert len(id_list) == len(annotationsSummary), f\"id_list and annotationsSummary are of different length: {len(id_list)} != {len(annotationsSummary)}\"\n",
    "\n",
    "    return annotationsSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLS: ['lambert_2018.present', 'Lovering_2021.present', 'animal_tfdb_Homo_sapiens_cofactors.present', 'animal_tfdb_Mus_musculus_cofactors.present', 'animal_tfdb_Rattus_norvegicus_cofactors.present', 'tcof_cotf_human.present', 'tcof_cotf_mouse.present', 'TFclass_human', 'TFclass_mouse', 'TFclass_rat']\n",
      "17 TFs in ExTRI2 are not present in the orthologs table (0.3%)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "We will discard 528 TFs that are not classified into any TF type"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gene ID</th>\n",
       "      <th>TF type</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>TaxID</th>\n",
       "      <th>GO:0003700</th>\n",
       "      <th>GO:0003712</th>\n",
       "      <th>GO:0001098</th>\n",
       "      <th>GO:0002039</th>\n",
       "      <th>GO:0008134</th>\n",
       "      <th>GO:0042393</th>\n",
       "      <th>GO:0046332</th>\n",
       "      <th>GO:0006325</th>\n",
       "      <th>GO:0140993</th>\n",
       "      <th>GO:0006355</th>\n",
       "      <th>lambert_2018.present</th>\n",
       "      <th>Lovering_2021.present</th>\n",
       "      <th>animal_tfdb_Homo_sapiens_cofactors.present</th>\n",
       "      <th>animal_tfdb_Mus_musculus_cofactors.present</th>\n",
       "      <th>animal_tfdb_Rattus_norvegicus_cofactors.present</th>\n",
       "      <th>tcof_cotf_human.present</th>\n",
       "      <th>tcof_cotf_mouse.present</th>\n",
       "      <th>TFclass_human</th>\n",
       "      <th>TFclass_mouse</th>\n",
       "      <th>TFclass_rat</th>\n",
       "      <th>updated TF type</th>\n",
       "      <th>In ExTRI</th>\n",
       "      <th>human_gene_ID</th>\n",
       "      <th>human_symbol</th>\n",
       "      <th>hgnc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>54608</td>\n",
       "      <td>dbTF</td>\n",
       "      <td>Abhd2</td>\n",
       "      <td>10090</td>\n",
       "      <td>GO:0003707</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GO:0003707</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dbTF</td>\n",
       "      <td>True</td>\n",
       "      <td>11057</td>\n",
       "      <td>ABHD2</td>\n",
       "      <td>HGNC:18717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>11057</td>\n",
       "      <td>dbTF</td>\n",
       "      <td>ABHD2</td>\n",
       "      <td>9606</td>\n",
       "      <td>GO:0003707</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GO:0003707</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dbTF</td>\n",
       "      <td>True</td>\n",
       "      <td>11057</td>\n",
       "      <td>ABHD2</td>\n",
       "      <td>HGNC:18717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Gene ID TF type Symbol  TaxID  GO:0003700 GO:0003712 GO:0001098  \\\n",
       "2221   54608    dbTF  Abhd2  10090  GO:0003707        NaN        NaN   \n",
       "285    11057    dbTF  ABHD2   9606  GO:0003707        NaN        NaN   \n",
       "\n",
       "     GO:0002039 GO:0008134 GO:0042393 GO:0046332 GO:0006325 GO:0140993  \\\n",
       "2221        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "285         NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "      GO:0006355 lambert_2018.present Lovering_2021.present  \\\n",
       "2221  GO:0003707                  NaN                   NaN   \n",
       "285   GO:0003707                  NaN                   NaN   \n",
       "\n",
       "     animal_tfdb_Homo_sapiens_cofactors.present  \\\n",
       "2221                                        NaN   \n",
       "285                                         NaN   \n",
       "\n",
       "     animal_tfdb_Mus_musculus_cofactors.present  \\\n",
       "2221                                        NaN   \n",
       "285                                         NaN   \n",
       "\n",
       "     animal_tfdb_Rattus_norvegicus_cofactors.present tcof_cotf_human.present  \\\n",
       "2221                                             NaN                     NaN   \n",
       "285                                              NaN                     NaN   \n",
       "\n",
       "     tcof_cotf_mouse.present TFclass_human TFclass_mouse TFclass_rat  \\\n",
       "2221                     NaN           NaN           NaN         NaN   \n",
       "285                      NaN           NaN           NaN         NaN   \n",
       "\n",
       "     updated TF type  In ExTRI human_gene_ID human_symbol     hgnc_id  \n",
       "2221            dbTF      True         11057        ABHD2  HGNC:18717  \n",
       "285             dbTF      True         11057        ABHD2  HGNC:18717  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "updated TF type\n",
       "dbTF              4161\n",
       "coTF candidate    3573\n",
       "coTF               933\n",
       "                   528\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>updated TF type</th>\n",
       "      <th>dbTF</th>\n",
       "      <th>coTF</th>\n",
       "      <th>coTF candidate</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GO:0006355</th>\n",
       "      <td>3062</td>\n",
       "      <td>914</td>\n",
       "      <td>1657</td>\n",
       "      <td>5633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0003700</th>\n",
       "      <td>2646</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0006325</th>\n",
       "      <td>349</td>\n",
       "      <td>271</td>\n",
       "      <td>1168</td>\n",
       "      <td>1788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0008134</th>\n",
       "      <td>609</td>\n",
       "      <td>327</td>\n",
       "      <td>484</td>\n",
       "      <td>1420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0003712</th>\n",
       "      <td>209</td>\n",
       "      <td>933</td>\n",
       "      <td>0</td>\n",
       "      <td>1142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0140993</th>\n",
       "      <td>88</td>\n",
       "      <td>113</td>\n",
       "      <td>268</td>\n",
       "      <td>469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0042393</th>\n",
       "      <td>42</td>\n",
       "      <td>54</td>\n",
       "      <td>269</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0046332</th>\n",
       "      <td>74</td>\n",
       "      <td>30</td>\n",
       "      <td>92</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0002039</th>\n",
       "      <td>20</td>\n",
       "      <td>41</td>\n",
       "      <td>92</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0001098</th>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "      <td>95</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "updated TF type  dbTF  coTF  coTF candidate  Total\n",
       "GO term                                           \n",
       "GO:0006355       3062   914            1657   5633\n",
       "GO:0003700       2646     0               0   2646\n",
       "GO:0006325        349   271            1168   1788\n",
       "GO:0008134        609   327             484   1420\n",
       "GO:0003712        209   933               0   1142\n",
       "GO:0140993         88   113             268    469\n",
       "GO:0042393         42    54             269    365\n",
       "GO:0046332         74    30              92    196\n",
       "GO:0002039         20    41              92    153\n",
       "GO:0001098         28    19              95    142"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFs in ExTRI2 vs human_TF_type:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TF type         human_TF_type \n",
       "dbTF            dbTF              2652\n",
       "coTF candidate  coTF candidate    2190\n",
       "coTF            coTF               727\n",
       "coTF candidate  coTF               111\n",
       "                dbTF                60\n",
       "                NaN                 39\n",
       "dbTF            NaN                 27\n",
       "coTF            dbTF                15\n",
       "                coTF candidate      10\n",
       "                NaN                  9\n",
       "dbTF            coTF                 6\n",
       "                coTF candidate       5\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Join all TF IDs into one dataframe\n",
    "TFs_list = []\n",
    "for TF in ['dbTF', 'coTF', 'll_coTF']:\n",
    "    path = get_TF_ids_path(TF, postprocessing_path) \n",
    "    with open(path, 'r') as f:\n",
    "        all_gene_IDs = f.read().splitlines()\n",
    "        TFs_list.extend([(gene_id, TF) for gene_id in all_gene_IDs])\n",
    "TFs_df = pd.DataFrame(TFs_list, columns=[\"Gene ID\", \"TF type\"])\n",
    "\n",
    "# Drop coTFs that are also ll_coTFs\n",
    "coTFs_set = set(TFs_df[TFs_df['TF type'] == 'coTF']['Gene ID'])\n",
    "TFs_df = TFs_df[~((TFs_df['TF type'] == 'coTF') & (TFs_df['Gene ID'].isin(ll_coTF)))]\n",
    "assert len(TFs_df) == len(set(TFs_df['Gene ID'])), \"There are duplicated Gene IDs in the TFs_df\"\n",
    "\n",
    "# Use eutils to map each gene ID to the gene symbol & TF type\n",
    "annotationsSummary = retrieve_annotations(TFs_df['Gene ID'].tolist())\n",
    "EntrezIDtoSymbol = {ID : {'Name': annotation[\"Name\"], 'TaxID': annotation['Organism']['TaxID']} for ID, annotation in zip(TFs_df['Gene ID'].tolist(), annotationsSummary)}\n",
    "TFs_df['Symbol'] = TFs_df['Gene ID'].map(lambda ID: EntrezIDtoSymbol[ID]['Name'])\n",
    "TFs_df['TaxID'] = TFs_df['Gene ID'].map(lambda ID: EntrezIDtoSymbol[ID]['TaxID'])\n",
    "\n",
    "\n",
    "# --- JOIN WITH QUICKGO ---\n",
    "for GO_term in GO_dbTF + GO_coTF + [\"GO:0006355\"]:\n",
    "    # Load the GO term table\n",
    "    GO_table = pd.read_csv(in_data_path + f\"QuickGO-{GO_term.replace(':', '')}.tsv\", sep=\"\\t\", header=0, dtype='str')\n",
    "\n",
    "    # Process the GO term table\n",
    "    GO_table = (\n",
    "        GO_table[['SYMBOL', 'TAXON ID', 'GO TERM']].drop_duplicates()\n",
    "        .groupby([\"SYMBOL\", \"TAXON ID\"], as_index=False)\n",
    "        .agg({\"GO TERM\": lambda x: \";\".join(sorted(set(x.dropna().astype(str))))})\n",
    "        .rename(columns={\"GO TERM\": GO_term})\n",
    "    )\n",
    "\n",
    "    # Merge with TFs_df\n",
    "    TFs_df = TFs_df.merge(GO_table, left_on=['Symbol', 'TaxID'], right_on=['SYMBOL', 'TAXON ID'], how='left').drop(columns=[\"TAXON ID\", \"SYMBOL\"])\n",
    "\n",
    "\n",
    "# --- JOIN WITH TFCHECKPOINT ---\n",
    "# Join together duplicated rows in TFCheckpoint\n",
    "cols = [col for cols in TFCheckpoint_cols.values() for col in cols if ((\"GO:\" not in col) & (col != 'TFclass.present.merged'))] + ['TFclass_human', 'TFclass_mouse', 'TFclass_rat'] # Only include relevant columns\n",
    "print(\"COLS:\", cols)\n",
    "TFCheckpoint_agg = (\n",
    "    TFCheckpoint_exploded[['EntrezIDs'] + cols]\n",
    "    .groupby('EntrezIDs', as_index=False)\n",
    "    .agg({\n",
    "        c: (lambda x: \";\".join(sorted(set(x.dropna().astype(str)))))\n",
    "        for c in cols\n",
    "    })\n",
    ")\n",
    "\n",
    "# Merge with TFCheckpoint\n",
    "TFs_df = TFs_df.merge(TFCheckpoint_agg[cols + ['EntrezIDs']], left_on=['Gene ID'], right_on=['EntrezIDs'], how='left').drop(columns=[\"EntrezIDs\"])\n",
    "\n",
    "# Ensure sources only have values for the correct species\n",
    "import numpy as np\n",
    "TFs_df.loc[TFs_df['TaxID'] != '9606',  ['TFclass_human', 'animal_tfdb_Homo_sapiens_cofactors.present', 'tcof_cotf_human.present', 'lambert_2018.present', 'Lovering_2021.present']] = np.nan\n",
    "TFs_df.loc[TFs_df['TaxID'] != '10090', ['TFclass_mouse', 'animal_tfdb_Mus_musculus_cofactors.present', 'tcof_cotf_mouse.present']] = np.nan\n",
    "TFs_df.loc[TFs_df['TaxID'] != '10116', ['TFclass_rat',   'animal_tfdb_Rattus_norvegicus_cofactors.present']] = np.nan\n",
    "\n",
    "# --- CREATE NEW CATEGORISATION\n",
    "new_categorisation = {\n",
    "    'dbTF': GO_dbTF + [col for col in TFCheckpoint_cols['dbTF'] if col not in ['TFclass.present.merged']] + ['TFclass_human', 'TFclass_mouse', 'TFclass_rat'],\n",
    "    'coTF candidate': GO_coTF + TFCheckpoint_cols['coTF'] + [\"GO:0006355\"],\n",
    "    'coTF': [\"GO:0003712\"]\n",
    "}\n",
    "\n",
    "TFs_df = TFs_df.replace(\"\", np.nan) # Replace empty strings with NaN\n",
    "TFs_df[\"updated TF type\"] = '' # Default\n",
    "# dbTF will overwrite coTF, which will overwrite coTF candidate\n",
    "for tf_type in ['coTF candidate', 'coTF', 'dbTF']:\n",
    "    cols = new_categorisation[tf_type]\n",
    "    TFs_df.loc[TFs_df[cols].notna().any(axis=1), 'updated TF type'] = tf_type\n",
    "\n",
    "# --- CLEAN UP & SAVE ---\n",
    "\n",
    "# Create column if in ExTRI2 dataset\n",
    "ExTRI2_df = pd.read_csv(\"../../results/ExTRI2_final_resource.tsv\", sep=\"\\t\", dtype='str')\n",
    "TFs_df[\"In ExTRI\"] = TFs_df['Gene ID'].isin(ExTRI2_df['TF Id'])\n",
    "\n",
    "# Add orthologs information\n",
    "all_orthologs_df = pd.read_csv(\"../../data/postprocessing/tables/orthologs_final.tsv\", sep=\"\\t\", dtype='str').set_index('Gene_ID')\n",
    "TFs_df['human_gene_ID'] = TFs_df['Gene ID'].map(all_orthologs_df['unique_human_gene_ID'])\n",
    "TFs_df['human_symbol']      = TFs_df['Gene ID'].map(all_orthologs_df['unique_human_gene_symbol'])\n",
    "TFs_df['hgnc_id']           = TFs_df['Gene ID'].map(all_orthologs_df['unique_HGNC_ID'])\n",
    "\n",
    "m_missing = (TFs_df['In ExTRI'] & ~TFs_df['Gene ID'].isin(all_orthologs_df.index))\n",
    "print(f\"{m_missing.sum()} TFs in ExTRI2 are not present in the orthologs table ({m_missing.sum() / TFs_df['In ExTRI'].sum():.1%})\")\n",
    "\n",
    "\n",
    "# Assertions\n",
    "assert len(TFs_df) == len(set(TFs_df['Gene ID'])), \"There are duplicated Gene IDs in the TFs_df\"\n",
    "\n",
    "# Sort rows\n",
    "order = [\"dbTF\", \"coTF\", \"coTF candidate\", \"\"]\n",
    "TFs_df[\"updated TF type\"] = pd.Categorical(TFs_df[\"updated TF type\"], categories=order, ordered=True)\n",
    "TFs_df = TFs_df.sort_values(by=['In ExTRI', 'updated TF type', 'human_symbol', 'TaxID'], ascending=[False, True, True, True])\n",
    "\n",
    "# Save the complete dataset\n",
    "TFs_df.to_csv(\"../../analysis/tables/all_TFs.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# --- CREATE PAPER TF TABLES ---\n",
    "# Save the 2 tables we'll include in the paper\n",
    "\n",
    "md(f'We will discard {TFs_df[TFs_df[\"updated TF type\"] == \"\"].shape[0]} TFs that are not classified into any TF type')\n",
    "TFs_df[TFs_df['updated TF type'] != ''][['Gene ID', 'Symbol', 'TaxID']].to_csv(\"../../data/paper_tables/all_considered_TFs.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Create table with only TFs in ExTRI2\n",
    "TFs_in_ExTRI2 = (TFs_df\n",
    "    .drop(columns=['TF type']).rename(columns={'updated TF type': 'TF type'})\n",
    "    .loc[TFs_df['In ExTRI'] & (TFs_df['updated TF type'] != '')].drop(columns=['In ExTRI'])\n",
    ")\n",
    "\n",
    "# Create 'human_TF_type' column, which prioritises the human TF type, and otherwise, it follows the priority order:\n",
    "priority = {'dbTF': 3, 'coTF': 2, 'coTF candidate': 1} # dbTF > coTF > coTF candidate\n",
    "\n",
    "# Function to resolve TF type per human_gene_ID\n",
    "def resolve_human_tf_type(df):\n",
    "    # If only one unique TF type, keep it\n",
    "    if df['TF type'].nunique() == 1:\n",
    "        return df['TF type'].iloc[0]\n",
    "    \n",
    "    # Prefer the human ortholog (TaxID == 9606)\n",
    "    human_rows = df[df['TaxID'].astype(str).str.contains('9606')]\n",
    "    if not human_rows.empty:\n",
    "        return human_rows['TF type'].iloc[0]\n",
    "    \n",
    "    # Otherwise, choose by priority\n",
    "    tf_types = sorted(df['TF type'].unique(), key=lambda t: priority.get(t, 0), reverse=True)\n",
    "    return tf_types[0]\n",
    "\n",
    "# Create mapping from human_gene_ID → resolved TF type and add it to the main DataFrame\n",
    "tf_type_map = (\n",
    "    TFs_in_ExTRI2.groupby(['human_gene_ID'])[['TF type', 'TaxID']]\n",
    "    .apply(resolve_human_tf_type).to_dict()\n",
    ")\n",
    "TFs_in_ExTRI2['human_TF_type'] = TFs_in_ExTRI2['human_gene_ID'].map(tf_type_map)\n",
    "\n",
    "\n",
    "# Save the table\n",
    "TFs_in_ExTRI2.to_csv(\"../../data/paper_tables/TFs_in_ExTRI2.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# --- SHOW STATS ---\n",
    "pd.set_option('display.max_columns', None)\n",
    "display(TFs_df.head(2))\n",
    "display(TFs_df['updated TF type'].value_counts(dropna=False))\n",
    "\n",
    "# Show updated TF types per GO term\n",
    "summary = (\n",
    "    TFs_df\n",
    "    .melt(id_vars=\"updated TF type\", value_vars=[c for c in TFs_df.columns if c.startswith(\"GO:\")],\n",
    "          var_name=\"GO term\", value_name=\"present\")\n",
    "    .assign(present=lambda d: d[\"present\"].notna())\n",
    "    .query(\"present == True\")\n",
    "    .groupby([\"GO term\", \"updated TF type\"], observed=True)\n",
    "    .size()\n",
    "    .unstack(fill_value=0)       # columns = updated TF type\n",
    ")\n",
    "summary[\"Total\"] = summary.sum(axis=1)\n",
    "display(summary.sort_values(\"Total\", ascending=False))\n",
    "\n",
    "print(\"TFs in ExTRI2 vs human_TF_type:\")\n",
    "display(TFs_in_ExTRI2[['TF type', 'human_TF_type']].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFs without updated TF type: 528\n",
      "TFs in ExTRI2 without updated TF type:   149\n",
      "ExTRI2 sentences with TFs without updated TF type: 3372 0.004035317404280883\n"
     ]
    }
   ],
   "source": [
    "print(\"TFs without updated TF type:\", (TFs_df['updated TF type'] == '').sum())\n",
    "print(\"TFs in ExTRI2 without updated TF type:  \", (TFs_df[TFs_df['In ExTRI']]['updated TF type'] == '').sum())\n",
    "m = TFs_df['In ExTRI'] & (TFs_df['updated TF type'] == '')\n",
    "print(\"ExTRI2 sentences with TFs without updated TF type:\", ExTRI2_df['TF Id'].isin(TFs_df[m]['Gene ID']).sum(), (ExTRI2_df['TF Id'].isin(TFs_df[m]['Gene ID'])).sum() / len(ExTRI2_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique GO:0006355 entries: 7338\n",
      "Number considered: 5633\n"
     ]
    }
   ],
   "source": [
    "# Check how many GO:0006355 we retrieve\n",
    "GO_term = \"GO:0006355\"\n",
    "\n",
    "# Process the GO term table\n",
    "GO_table = pd.read_csv(in_data_path + f\"QuickGO-{GO_term.replace(':', '')}.tsv\", sep=\"\\t\", header=0, dtype='str')\n",
    "\n",
    "GO_table = (\n",
    "    GO_table[['SYMBOL', 'TAXON ID', 'GO TERM']].drop_duplicates()\n",
    "    .groupby([\"SYMBOL\", \"TAXON ID\"], as_index=False)\n",
    "    .agg({\"GO TERM\": lambda x: \";\".join(sorted(set(x.dropna().astype(str))))})\n",
    "    .rename(columns={\"GO TERM\": GO_term})\n",
    ")\n",
    "\n",
    "print(f\"Unique {GO_term} entries: {GO_table.shape[0]}\")\n",
    "print(f\"Number considered: {TFs_df[GO_term].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TF type         human_TF_type \n",
       "dbTF            dbTF              2652\n",
       "coTF candidate  coTF candidate    2190\n",
       "coTF            coTF               727\n",
       "coTF candidate  coTF               111\n",
       "                dbTF                60\n",
       "                NaN                 39\n",
       "dbTF            NaN                 27\n",
       "coTF            dbTF                15\n",
       "                coTF candidate      10\n",
       "                NaN                  9\n",
       "dbTF            coTF                 6\n",
       "                coTF candidate       5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'human_symbol'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/git/ExTRI2_gits/ExTRI2/.general_env/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'human_symbol'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m m \u001b[38;5;241m=\u001b[39m final_TFs_grouped[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTF type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoTF\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdbTF\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoTF candidate\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m|\u001b[39m (final_TFs_grouped[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman_symbol\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRows in ExTRI2 affected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mExTRI2_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhuman_symbol\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39misin(final_TFs_grouped[\u001b[38;5;241m~\u001b[39mm][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman_symbol\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mExTRI2_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman_symbol\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(final_TFs_grouped[\u001b[38;5;241m~\u001b[39mm][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman_symbol\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m display(final_TFs_grouped[\u001b[38;5;241m~\u001b[39mm][[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTF type\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalue_counts())\n\u001b[1;32m      4\u001b[0m display(final_TFs_grouped[\u001b[38;5;241m~\u001b[39mm])\n",
      "File \u001b[0;32m~/git/ExTRI2_gits/ExTRI2/.general_env/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/git/ExTRI2_gits/ExTRI2/.general_env/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'human_symbol'"
     ]
    }
   ],
   "source": [
    "m = final_TFs_grouped['TF type'].isin(['coTF', 'dbTF', 'coTF candidate']) | (final_TFs_grouped['human_symbol'] == 'None')\n",
    "print(f\"Rows in ExTRI2 affected: {ExTRI2_df['human_symbol'].isin(final_TFs_grouped[~m]['human_symbol']).sum()} ({ExTRI2_df['human_symbol'].isin(final_TFs_grouped[~m]['human_symbol']).mean():.2%})\")\n",
    "display(final_TFs_grouped[~m][['TF type']].value_counts())\n",
    "display(final_TFs_grouped[~m])\n",
    "\n",
    "# TODO - Create in \"TFs_in_ExTRI2\" an extra column where each TF_human_symbol is mapped to only one TF_type.\n",
    "# Clashes in a few sentences. We will exclude them from CollecTRI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".general_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
