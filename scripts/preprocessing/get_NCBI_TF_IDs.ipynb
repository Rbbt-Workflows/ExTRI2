{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get List of TF NCBI IDs \n",
    "Get the list of IDs that will be considered TF, both from GO terms and from TFCheckpoint.\n",
    "\n",
    "The GO terms and columns from TFCheckpoint used for each of the TF types is detailed in the cell below. The specific procedure used is explained in their respective sections:  [Get GO terms](#get-go-terms) and [GET TFCheckpoint terms](#get-tfcheckpoint-terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3>Table of contents</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "[Get List of TF NCBI IDs](#Get-List-of-TF-NCBI-IDs)\n",
       "- [Setup](#Setup)\n",
       "- [Get GO terms](#Get-GO-terms)\n",
       "- [Get TFCheckpoint terms](#Get-TFCheckpoint-terms)\n",
       "- [Get final TF set for the pipeline](#Get-final-TF-set-for-the-pipeline)\n",
       "- [Create final TF table](#Create-final-TF-table)\n",
       "- [Outdated - Prepare data to separate likely from less-likely coTFs](#Outdated---Prepare-data-to-separate-likely-from-less-likely-coTFs)\n",
       "  - [Get coTF human orthologs for filtering](#Get-coTF-human-orthologs-for-filtering)\n",
       "  - [Send all coTFs to be separated into coTF & ll-coTF](#Send-all-coTFs-to-be-separated-into-coTF-&-ll-coTF)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "__import__('sys').path.append('../common/'); __import__('notebook_utils').table_of_contents('get_NCBI_TF_IDs.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import requests\n",
    "\n",
    "from Bio import Entrez\n",
    "# *Always* tell NCBI who you are\n",
    "Entrez.email = \"example24@gmail.com\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('../common')\n",
    "from notebook_utils import h3, h4, h5, md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GO TERM & TFCHECKPOINT VARIABLES\n",
    "\n",
    "# GO terms used:\n",
    "GO_dbTF = [\"GO:0003700\"]\n",
    "GO_coTF = [\"GO:0003712\", \"GO:0001098\", \"GO:0002039\" , \"GO:0008134\" , \"GO:0042393\", \"GO:0046332\", \"GO:0006325\", \"GO:0140993\"]\n",
    "\n",
    "# Columns from TFCheckpoint used:\n",
    "TFCheckpoint_cols = {\n",
    "    'dbTF': ['TFclass.present.merged', 'lambert_2018.present', 'Lovering_2021.present'],\n",
    "    'coTF': ['animal_tfdb_Homo_sapiens_cofactors.present', 'animal_tfdb_Mus_musculus_cofactors.present', 'animal_tfdb_Rattus_norvegicus_cofactors.present', \n",
    "             'tcof_cotf_human.present', 'tcof_cotf_mouse.present']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "in_data_path = '../../data/external/TF_id/'\n",
    "postprocessing_path = '../../data/postprocessing/'\n",
    "\n",
    "less_likely_coTFs_path = postprocessing_path + 'all_coTFs_likely_checked_updated_AL.txt'\n",
    "\n",
    "QuickGO_dbTF_path = in_data_path + \"QuickGO-annotations-dbTF.tsv\"\n",
    "QuickGO_coTF_path = in_data_path + \"QuickGO-annotations-coTF.tsv\"\n",
    "TFCheckpoint_path = in_data_path + \"TFCheckpoint.tsv\"\n",
    "\n",
    "# Define a function to construct the TF types path (ll_coTFs are introduced later)\n",
    "TF_types = [\"dbTF\", \"coTF\"]\n",
    "def get_TF_ids_path(TF_type, out_data_path):\n",
    "    return f\"{out_data_path}{TF_type}_entrez_code.list\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD LIKELY & LESS LIKELY COTFs\n",
    "ll_coTFs_db = pd.read_csv(less_likely_coTFs_path, sep=\"\\t\", dtype='str')\n",
    "m = ll_coTFs_db['likely'] == 'likely'\n",
    "ll_coTF = set(ll_coTFs_db[~m]['NCBI ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get GO terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Obtained the GO terms from [QuickGO](https://www.ebi.ac.uk/QuickGO/annotations?taxonId=10116,9606,10090&taxonUsage=exact&goId=GO:0140993,GO:0003712,GO:0003700,GO:0140223,GO:0001098,GO:0002039,GO:0008134,GO:0042393,GO:0046332,GO:0006325&goUsageRelationships=is_a,part_of,occurs_in&goUsage=descendants&geneProductSubset=Swiss-Prot&geneProductType=protein), using the terms shown below. Used as filters:\n",
    "\n",
    "* **Taxon:** 10116, 9606, 10090, Exact match (do not include descendants)\n",
    "* **Gene products:** Reviewed (not Unreviewed)\n",
    "* **GO terms:**.\n",
    "  * **dbTF:** GO:0003700 (DNA-binding transcription factor activity)\n",
    "  * **coTF:** GO:0140993 (histone modifying activity), GO:0008134 (transcription factor binding), GO:0003712 (transcription coregulator activity), GO:0001098 (basal transcription machinery binding), GO:0002039 (p53 binding), GO:0042393 (histone binding), GO:0046332 (SMAD binding) and GO:0006325 (chromatin organization)\n",
    "* **Export as:** tsv\n",
    "\n",
    "Downloaded separately a QuickGO tsv file for each TF type and renamed it as shown above in the setup section.\n",
    "\n",
    "As some terms can be identified as pertaining to more than 1 type, we have followed this hierarchy to remove duplicates:\n",
    "1. dbTF\n",
    "2. coTF\n",
    "\n",
    "That implies that if a protein is classified as both dbTF and coTF, the protein's classification will be dbTF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES\n",
    "# Species:\n",
    "organismToTaxID = {\n",
    "    \"hsapiens\": \"9606\",\n",
    "    \"mmusculus\": \"10090\",\n",
    "    \"rnorvegicus\": \"10116\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "def fetch_gene_ids_gprofiler(gene_symbols: list, organism: str) -> dict:\n",
    "    \"Get NCBI Gene IDs from GProfiler\"\n",
    "    symboltoID = {}\n",
    "\n",
    "    # Query the IDs from GProfiler\n",
    "    result = requests.post(\n",
    "        url='https://biit.cs.ut.ee/gprofiler/api/convert/convert/',\n",
    "        json={\n",
    "            'organism': organism,\n",
    "            'target':'ENTREZGENE_ACC',\n",
    "            'query': gene_symbols,\n",
    "        }\n",
    "        )\n",
    "    \n",
    "    # Create a list of extracted IDs per symbol\n",
    "    for r in result.json()['result']:\n",
    "        incoming = r['incoming']\n",
    "        converted = r['converted']\n",
    "\n",
    "        if incoming not in symboltoID:\n",
    "            symboltoID[incoming] = []\n",
    "        if converted != 'None':\n",
    "            symboltoID[incoming].append(converted)\n",
    "\n",
    "    return symboltoID\n",
    "\n",
    "def retrieve_annotations_entrez(id_list):\n",
    "    \"\"\"Annotates Entrez Gene IDs using Bio.Entrez, in particular epost (to\n",
    "    submit the data to NCBI) and esummary to retrieve the information.\n",
    "    Returns a list of dictionaries with the annotations.\"\"\"\n",
    "\n",
    "    request = Entrez.epost(\"gene\", id=\",\".join(id_list))\n",
    "    result = Entrez.read(request)\n",
    "    webEnv = result[\"WebEnv\"]\n",
    "    queryKey = result[\"QueryKey\"]\n",
    "    data = Entrez.esummary(db=\"gene\", webenv=webEnv, query_key=queryKey)\n",
    "    annotations = Entrez.read(data)\n",
    "    annotationsSummary = annotations['DocumentSummarySet']['DocumentSummary']\n",
    "\n",
    "    assert len(id_list) == len(annotationsSummary), f\"id_list and annotationsSummary are of different length: {len(id_list)} != {len(annotationsSummary)}\"\n",
    "\n",
    "    return annotationsSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25053 rows were retrieved.\n",
      "Removing duplicates, we retrieve 6005 symbols:\n",
      "\t2184 dbTFs\n",
      "\t3156 coTFs\n"
     ]
    }
   ],
   "source": [
    "# JOIN QUICKGO TSVs\n",
    "# Create joined DataFrame from the 3 TF types\n",
    "QuickGO_dbTF = pd.read_csv(QuickGO_dbTF_path, sep='\\t', header=0, keep_default_na=False, dtype='str')\n",
    "QuickGO_dbTF['TF type'] = 'dbTF'\n",
    "QuickGO_coTF = pd.read_csv(QuickGO_coTF_path, sep='\\t', header=0, keep_default_na=False, dtype='str')\n",
    "QuickGO_coTF['TF type'] = 'coTF' \n",
    "\n",
    "QuickGO = pd.concat([QuickGO_dbTF, QuickGO_coTF], axis=0)\n",
    "\n",
    "print(f\"{len(QuickGO['SYMBOL'])} rows were retrieved.\")\n",
    "\n",
    "# Only keep relevant columns\n",
    "QuickGO = QuickGO[['SYMBOL', 'TAXON ID', 'TF type', 'GO TERM']]\n",
    "\n",
    "# Drop repeated cells. Use the following priority if duplicates of different TF type\n",
    "priority = {'dbTF': 0, 'coTF': 1}\n",
    "QuickGO['priority'] = QuickGO['TF type'].map(priority)\n",
    "QuickGO = QuickGO.sort_values(by=['SYMBOL', 'TAXON ID', 'priority'])\n",
    "QuickGO = QuickGO.drop_duplicates(subset=['SYMBOL', 'TAXON ID', 'GO TERM'], keep='first')\n",
    "QuickGO = (\n",
    "    QuickGO\n",
    "    .groupby([\"SYMBOL\", \"TAXON ID\"], as_index=False)\n",
    "    .agg({\n",
    "        'TF type': lambda x: \";\".join(sorted(set(x.astype(str)))),\n",
    "        'GO TERM': 'first'   # take top-priority GO TERM\n",
    "    })\n",
    ")\n",
    "\n",
    "# Show results\n",
    "print(f\"Removing duplicates, we retrieve {len(QuickGO['SYMBOL'])} symbols:\")\n",
    "for TF_type in ('dbTF', 'coTF'):\n",
    "    print(f\"\\t{len(QuickGO[QuickGO['TF type'] == TF_type]['SYMBOL'])} {TF_type}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GProfiler retrieved 98.2% NCBI Gene IDs from the QuickGO symbols\n",
      "It couldn't retrieve 108 of them\n"
     ]
    }
   ],
   "source": [
    "# GET GENE IDs FROM GPROFILER\n",
    "for organism in ['hsapiens', 'mmusculus', 'rnorvegicus']:\n",
    "    # Get IDs from GProfiler\n",
    "    symbols = list(QuickGO[QuickGO['TAXON ID'] == organismToTaxID[organism]]['SYMBOL'].unique())\n",
    "    symboltoID = fetch_gene_ids_gprofiler(symbols, organism)\n",
    "\n",
    "    # Map them to QuickGO db\n",
    "    m = QuickGO['TAXON ID'] == organismToTaxID[organism]\n",
    "    QuickGO.loc[m, \"TF ID\"] = QuickGO[m]['SYMBOL'].apply(lambda symbol: symboltoID[symbol])\n",
    "\n",
    "m = ~(QuickGO['TF ID'].str.len() == 0)\n",
    "print(f'GProfiler retrieved {m.sum() / len(QuickGO):.1%} NCBI Gene IDs from the QuickGO symbols')\n",
    "print(f\"It couldn't retrieve {(~m).sum()} of them\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve the rest through a query to Entrez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9606\n",
      "9606\n",
      "9606\n",
      "10090\n",
      "10090\n",
      "10116\n",
      "10116\n",
      "10116\n",
      "10116\n"
     ]
    }
   ],
   "source": [
    "# QUERY THE REST FROM ENTREZ\n",
    "# Entrez often gets stuck, so it's best to give some time between queries\n",
    "import time\n",
    "ids = []\n",
    "for TaxID in ['9606', '10090', '10116']:\n",
    "\n",
    "    # Get the symbols with missing ID\n",
    "    m = (QuickGO['TF ID'].str.len() == 0) & (QuickGO['TAXON ID'] == TaxID)\n",
    "    missing_symbols = list(QuickGO[m]['SYMBOL'].unique())\n",
    "\n",
    "    nSymbols = 15 # Symbols per query\n",
    "    for i in range(0, len(missing_symbols), nSymbols):\n",
    "        symbols = missing_symbols[i:i+nSymbols]\n",
    "        # Query them from Entrez\n",
    "        symbolsQuery = sorted([s+'[Preferred Symbol]' for s in symbols])\n",
    "        query = f'({\" OR \".join(symbolsQuery)}) AND txid{TaxID}[Organism]'\n",
    "        handle = Entrez.esearch(db=\"gene\", term=query, retmode=\"xml\")\n",
    "        record = Entrez.read(handle)\n",
    "        ids.append(record.get(\"IdList\", []))\n",
    "        print(TaxID)\n",
    "        # Sleep between queries to not get blocked\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 out of the remaining 108 missing have been retrieved through Entrez.\n",
      "Combined with Entrez, we retrieved 99.4% NCBI Gene IDs from the QuickGO symbols\n",
      "There's 35 NCBI Gene IDs that couldn't be retrieved\n"
     ]
    }
   ],
   "source": [
    "# Map IDs back to its symbol & organism\n",
    "all_ids = [j for i in ids for j in i]\n",
    "remaining_annotations = retrieve_annotations_entrez(all_ids)\n",
    "\n",
    "# Make a map from symbol/TaxID to Gene ID\n",
    "symboltoID_entrez = {'9606': {}, '10116': {}, '10090': {}}\n",
    "for id, ann in zip(all_ids, remaining_annotations):\n",
    "    symbol = ann['Name']\n",
    "    TaxID = ann['Organism']['TaxID']\n",
    "\n",
    "    if symbol not in symboltoID_entrez[TaxID]:\n",
    "        symboltoID_entrez[TaxID][symbol] = [id]\n",
    "    else:\n",
    "        symboltoID_entrez[TaxID][symbol].append(id)\n",
    "\n",
    "# Check how many we retrieved from Entrez\n",
    "m = (QuickGO['TF ID'].str.len() == 0)\n",
    "print(f\"{len(all_ids)} out of the remaining {m.sum()} missing have been retrieved through Entrez.\")\n",
    "\n",
    "# Map the retrieved ones to the QuickGO db\n",
    "for TaxID in symboltoID_entrez.keys():\n",
    "    m = (QuickGO['TF ID'].str.len() == 0) & (QuickGO['TAXON ID'] == TaxID)\n",
    "    QuickGO.loc[m, \"TF ID\"] = QuickGO[m]['SYMBOL'].apply(lambda symbol: symboltoID_entrez[TaxID].get(symbol, []))\n",
    "\n",
    "m = ~(QuickGO['TF ID'].str.len() == 0)\n",
    "print(f'Combined with Entrez, we retrieved {m.sum() / len(QuickGO):.1%} NCBI Gene IDs from the QuickGO symbols')\n",
    "print(f\"There's {(~m).sum()} NCBI Gene IDs that couldn't be retrieved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Get TFCheckpoint terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4>EntrezIDs mapped to 2 species</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "There are 5 Entrez IDs that are mapped to both Rat and Mouse:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>EntrezIDs</th>\n",
       "      <th>Associated.Gene.Name</th>\n",
       "      <th>TaxaIDs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20851</td>\n",
       "      <td>STAT5A</td>\n",
       "      <td>10116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20851</td>\n",
       "      <td>STAT5B</td>\n",
       "      <td>10090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22764</td>\n",
       "      <td>ZFX</td>\n",
       "      <td>10090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22764</td>\n",
       "      <td>ZFY</td>\n",
       "      <td>10116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24918</td>\n",
       "      <td>STAT5A</td>\n",
       "      <td>10090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24918</td>\n",
       "      <td>STAT5B</td>\n",
       "      <td>10116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25126</td>\n",
       "      <td>STAT5A</td>\n",
       "      <td>10090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25126</td>\n",
       "      <td>STAT5B</td>\n",
       "      <td>10116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>367832</td>\n",
       "      <td>ZFX</td>\n",
       "      <td>10116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>367832</td>\n",
       "      <td>ZFY</td>\n",
       "      <td>10090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "They have been searched in the NCBI and corrected manually"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## LOAD & PREPROCESS TFCHECKPOINT TSV\n",
    "# Load TFCheckpoint dataset\n",
    "TFCheckpoint_df = pd.read_csv(TFCheckpoint_path, sep='\\t', header=0)\n",
    "str_cols = ['Associated.Gene.Name', 'Synonyms', 'Official name', 'Entrez.Taxa.ID', 'Entrez.Gene.ID', 'UniProt.SwissProt.Accession', 'Ensembl.Gene.ID']\n",
    "TFCheckpoint_df[str_cols] = TFCheckpoint_df[str_cols].astype(str)\n",
    "\n",
    "\n",
    "# Split Entrez, Taxa & UniProt into individual IDs\n",
    "TFCheckpoint_df['EntrezIDs'] = TFCheckpoint_df['Entrez.Gene.ID'].str.split('|')\n",
    "TFCheckpoint_df['TaxaIDs'] = TFCheckpoint_df['Entrez.Taxa.ID'].str.split('|')\n",
    "TFCheckpoint_df['UniProt'] = TFCheckpoint_df['UniProt.SwissProt.Accession'].str.split('|')\n",
    "TFCheckpoint_df['Ensembl'] = TFCheckpoint_df['Ensembl.Gene.ID'].str.split('|')\n",
    "\n",
    "# Explode the TF\n",
    "TFCheckpoint_exploded = TFCheckpoint_df.explode(['EntrezIDs', 'TaxaIDs', 'UniProt', 'Ensembl'])\n",
    "TFCheckpoint_exploded = TFCheckpoint_exploded[TFCheckpoint_exploded[\"EntrezIDs\"] != ''] # Drop empty rows (Appeared when | was present at the end, e.g. \"9454|3425|\")\n",
    "TFCheckpoint_exploded = TFCheckpoint_exploded[TFCheckpoint_exploded[\"UniProt\"] != '']\n",
    "TFCheckpoint_exploded = TFCheckpoint_exploded[TFCheckpoint_exploded[\"Ensembl\"] != '']\n",
    "\n",
    "# Check whether each EntrezID only matches to 1 TaxaID:\n",
    "gene_taxa_unique = TFCheckpoint_exploded.drop_duplicates(subset=[\"EntrezIDs\", \"TaxaIDs\"], keep='first')\n",
    "gene_taxa_mismatch = gene_taxa_unique[gene_taxa_unique.duplicated(subset=[\"EntrezIDs\"], keep=False)]\n",
    "h4(\"EntrezIDs mapped to 2 species\")\n",
    "md(f\"There are {len(gene_taxa_mismatch['EntrezIDs'].unique())} Entrez IDs that are mapped to both Rat and Mouse:\")\n",
    "display(HTML(gene_taxa_mismatch[[\"EntrezIDs\", \"Associated.Gene.Name\", \"TaxaIDs\"]].sort_values(by=['EntrezIDs']).to_html(index=False)))\n",
    "\n",
    "# DROP MISMATCHING ROWS\n",
    "rows_to_drop = [\n",
    "    ['STAT5A', '20851', '10116'],\n",
    "    ['STAT5A', '25126', '10090'],\n",
    "    ['ZFY', '367832', '10090'],\n",
    "    ['ZFY', '22764', '10116'],\n",
    "    ['STAT5B', '24918', '10116']\n",
    "]\n",
    "for row in rows_to_drop:\n",
    "    to_drop = (TFCheckpoint_exploded[\"Associated.Gene.Name\"] == row[0]) & (TFCheckpoint_exploded[\"EntrezIDs\"] == row[1])\n",
    "    assert to_drop.sum() == 1, f\"{to_drop.sum()} rows are being dropped instead of 1\"\n",
    "    TFCheckpoint_exploded = TFCheckpoint_exploded[~to_drop]\n",
    "to_change = (TFCheckpoint_exploded[\"Associated.Gene.Name\"] == \"STAT5A\") & (TFCheckpoint_exploded[\"EntrezIDs\"] == \"24918\")\n",
    "assert to_change.sum() == 1, f\"{to_change.sum()} rows are being dropped instead of 1\"\n",
    "TFCheckpoint_exploded.loc[to_change, \"TaxaIDs\"] = \"10116\"\n",
    "md(\"They have been searched in the NCBI and corrected manually\")\n",
    "\n",
    "# Assert there's no duplicates anymore\n",
    "gene_taxa_unique = TFCheckpoint_exploded.drop_duplicates(subset=[\"EntrezIDs\", \"TaxaIDs\"], keep='first')\n",
    "gene_taxa_mismatch = gene_taxa_unique[gene_taxa_unique.duplicated(subset=[\"EntrezIDs\"], keep=False)]\n",
    "assert len(gene_taxa_mismatch) == 0, f\"There's still {len(gene_taxa_mismatch)} duplicated rows\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In 16 TFs, one EntrezID is mapped to 2 different SwissProt Accession IDs. They have been joined by |. Example:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Associated.Gene.Name</th>\n",
       "      <th>Official name</th>\n",
       "      <th>EntrezIDs</th>\n",
       "      <th>TaxaIDs</th>\n",
       "      <th>UniProt</th>\n",
       "      <th>Ensembl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>ABL1</td>\n",
       "      <td>Tyrosine-protein kinase ABL1</td>\n",
       "      <td>100909750</td>\n",
       "      <td>10116</td>\n",
       "      <td>E9PT20|F1M0A6</td>\n",
       "      <td>ENSRNOG00000009371|ENSRNOG00000009371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CHCHD2</td>\n",
       "      <td>Coiled-coil-helix-coiled-coil-helix domain-containing protein 2</td>\n",
       "      <td>316643</td>\n",
       "      <td>10116</td>\n",
       "      <td>Q5BJB3|M0R785</td>\n",
       "      <td>ENSRNOG00000051180|ENSRNOG00000051180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GROUP DUPLICATED ROWS & GET FINAL TFCHECKPOINT DATASET\n",
    "# In some rows, EntrezID, TaxaID & Name are the same -> Only SwissProt changes. We will group those rows\n",
    "\n",
    "# Remove all useless columns\n",
    "columns_to_keep = TFCheckpoint_exploded.columns.tolist()\n",
    "columns_to_remove = ['Entrez.Taxa.ID', 'Entrez.Gene.ID', 'UniProt.SwissProt.Accession', 'Ensembl.Gene.ID', 'UniProt', 'Ensembl']\n",
    "for column in columns_to_remove:\n",
    "    columns_to_keep.remove(column)\n",
    "\n",
    "# Group duplicated rows, with a | in between for UniProt & Ensembl.\n",
    "TFCheckpoint_exploded = TFCheckpoint_exploded.groupby(columns_to_keep, dropna=False).agg({\n",
    "    \"UniProt\": lambda x: \"|\".join(x),\n",
    "    \"Ensembl\": lambda x: \"|\".join(x)\n",
    "}).reset_index()\n",
    "\n",
    "# Display one example\n",
    "mask = TFCheckpoint_exploded[\"UniProt\"].str.contains(\"\\|\")\n",
    "md(f\"In {mask.sum()} TFs, one EntrezID is mapped to 2 different SwissProt Accession IDs. They have been joined by |. Example:\")\n",
    "display(HTML(TFCheckpoint_exploded[mask][:2][[\"Associated.Gene.Name\", \"Official name\", \"EntrezIDs\", \"TaxaIDs\", \"UniProt\", \"Ensembl\"]].to_html(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# dbTF NCBI IDs in TFCheckpoint:  4390\n",
      "# coTF NCBI IDs in TFCheckpoint:  3598\n"
     ]
    }
   ],
   "source": [
    "TFCheckpoint_sets = {}\n",
    "for TF_type in TF_types:\n",
    "    mask = TFCheckpoint_exploded[TFCheckpoint_cols[TF_type]].notna().any(axis=1)  # Checks across the specified columns\n",
    "    TFCheckpoint_sets[TF_type] = set(TFCheckpoint_exploded[mask]['EntrezIDs'])\n",
    "    print(f\"# {TF_type} NCBI IDs in TFCheckpoint: {len(TFCheckpoint_sets[TF_type]):>5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get final TF set for the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We consider 9195 NCBI Gene IDs to be TFs\n"
     ]
    }
   ],
   "source": [
    "# Get TF sets for each TF type\n",
    "TF_IDs_dict = {}\n",
    "for TF_type in TF_types:\n",
    "    TF_IDs = set()\n",
    "\n",
    "    # Get QuickGO TFs\n",
    "    QuickGO_subset = QuickGO[QuickGO['TF type'] == TF_type]\n",
    "    QuickGO_IDs = [j for i in list(QuickGO_subset['TF ID']) for j in i]\n",
    "    TF_IDs.update(set(QuickGO_IDs))\n",
    "        \n",
    "    # Get TFCheckpoint TFs\n",
    "    TF_IDs.update(TFCheckpoint_sets[TF_type])\n",
    "\n",
    "    # Save into dictionary\n",
    "    TF_IDs_dict[TF_type] = TF_IDs\n",
    "\n",
    "\n",
    "# coTFs must not contain dbTFs.\n",
    "TF_IDs_dict['coTF'].difference_update(TF_IDs_dict['dbTF'])\n",
    "\n",
    "# Add the less likely coTFs as a subset of the coTFs\n",
    "TF_IDs_dict['ll_coTF'] = ll_coTF.intersection(TF_IDs_dict['coTF'])\n",
    "\n",
    "# Save each of them as a list\n",
    "for TF_type in TF_types + ['ll_coTF']:\n",
    "    path = get_TF_ids_path(TF_type, postprocessing_path)\n",
    "    with open(path, 'w') as f:\n",
    "        for TF in TF_IDs_dict[TF_type]:\n",
    "            f.write(TF + \"\\n\")\n",
    "\n",
    "# Combine all TFs & save them as a list\n",
    "all_TF_ids = TF_IDs_dict['dbTF'].union(TF_IDs_dict['coTF'])\n",
    "print(f\"We consider {len(all_TF_ids)} NCBI Gene IDs to be TFs\")\n",
    "\n",
    "with open(get_TF_ids_path('tf', postprocessing_path), 'w') as f:\n",
    "    for TF in all_TF_ids:\n",
    "        f.write(TF + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create final TF table\n",
    "Create TF table to use in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "import os\n",
    "from Bio import Entrez\n",
    "import difflib\n",
    "Entrez.email = \"example24@gmail.com\"\n",
    "\n",
    "def retrieve_annotations(id_list):\n",
    "    \"\"\"Annotates Entrez Gene IDs using Bio.Entrez, in particular epost (to\n",
    "    submit the data to NCBI) and esummary to retrieve the information.\n",
    "    Returns a list of dictionaries with the annotations.\"\"\"\n",
    "\n",
    "    request = Entrez.epost(\"gene\", id=\",\".join(id_list))\n",
    "    result = Entrez.read(request)\n",
    "    webEnv = result[\"WebEnv\"]\n",
    "    queryKey = result[\"QueryKey\"]\n",
    "    data = Entrez.esummary(db=\"gene\", webenv=webEnv, query_key=queryKey)\n",
    "    annotations = Entrez.read(data)\n",
    "    annotationsSummary = annotations['DocumentSummarySet']['DocumentSummary']\n",
    "\n",
    "    assert len(id_list) == len(annotationsSummary), f\"id_list and annotationsSummary are of different length: {len(id_list)} != {len(annotationsSummary)}\"\n",
    "\n",
    "    return annotationsSummary\n",
    "\n",
    "# TODO - Old function for safekeeping. Remove once I ensure the new function works \n",
    "def add_HGNC_symbols_old(TF_df: pd.DataFrame, orthologs_path: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    use ortholog dicts in orthologs_path (downloaded from HGNC) to get HGNC orthologs for mouse, rat, & human HGNC IDs\n",
    "    '''\n",
    "\n",
    "    orthologs = pd.DataFrame()\n",
    "\n",
    "    # Get mouse & rat orthologs\n",
    "    for rodent in ['mouse', 'rat']:\n",
    "        hgnc_df = pd.read_csv(orthologs_path + f\"human_{rodent}_hcop_fifteen_column.txt\", sep='\\t', keep_default_na=False, dtype='str')\n",
    "        hgnc_df = hgnc_df.rename(columns={f\"{rodent}_entrez_gene\": \"entrez_gene\", f\"{rodent}_symbol\": \"symbol\"})\n",
    "        orthologs = pd.concat([orthologs, hgnc_df])\n",
    "\n",
    "    # Get human HNGC symbols\n",
    "    h_hgnc_df = pd.read_csv(orthologs_path + \"hgnc_human.tsv\", sep='\\t', keep_default_na=False, dtype='str')\n",
    "    h_hgnc_df = h_hgnc_df.rename(columns={\"HGNC ID\": \"hgnc_id\", \"NCBI Gene ID\": \"entrez_gene\", \"Approved symbol\": \"symbol\"})\n",
    "    h_hgnc_df['human_entrez_gene'] = h_hgnc_df['entrez_gene']\n",
    "    h_hgnc_df['human_symbol'] = h_hgnc_df['symbol']\n",
    "    orthologs = pd.concat([orthologs, h_hgnc_df])\n",
    "\n",
    "    # Keep only IDs present in the TF_df\n",
    "    TF_ids = {j for id in TF_df['Gene ID'].unique() for j in id.split(';')}\n",
    "    orthologs = orthologs[orthologs['entrez_gene'].isin(TF_ids)]\n",
    "\n",
    "    # Remove all rows that don't have a human entrez ID or hgnc ID\n",
    "    m = (orthologs['human_entrez_gene'] != '-') | (orthologs['hgnc_id'] != '-')\n",
    "    orthologs = orthologs[m]\n",
    "\n",
    "    # Join with ';' when an EntrezID has more than 1 human ortholog\n",
    "    agg_funcs = {\n",
    "        \"symbol\": lambda x: ';'.join(x.unique()),\n",
    "        \"human_entrez_gene\": lambda x: ';'.join(x.unique()),\n",
    "        \"hgnc_id\": lambda x: ';'.join(x.unique()),\n",
    "        \"human_symbol\": lambda x: ';'.join(x.unique())\n",
    "    }\n",
    "    orthologs = orthologs.groupby(['entrez_gene']).agg(agg_funcs).reset_index()\n",
    "\n",
    "    # Show how many we get\n",
    "    print(f\"We get ortholog info for {len(orthologs)}/{len(TF_ids)} Gene IDs\\n\")\n",
    "\n",
    "    # Fill in ortholog columns\n",
    "    orthologs_map = orthologs.set_index('entrez_gene').to_dict(orient='index')\n",
    "\n",
    "    # Add NFKB & AP1 orthologs\n",
    "    for dimer in ['NFKB', 'AP1']:\n",
    "        orthologs_map[f'Complex:{dimer}'] = {}\n",
    "        orthologs_map[f'Complex:{dimer}']['human_entrez_gene'] = f'Complex:{dimer}'\n",
    "        orthologs_map[f'Complex:{dimer}']['hgnc_id'] = f'Complex:{dimer}'\n",
    "        orthologs_map[f'Complex:{dimer}']['human_symbol'] = dimer\n",
    "\n",
    "    def fill_ortholog_column(id, column):\n",
    "        '''Helper function to fill ortholog columns'''\n",
    "        result = []\n",
    "        for entrez_gene in id.split(\";\"):\n",
    "            result.append(orthologs_map[entrez_gene][column]) if entrez_gene in orthologs_map else \"-\"\n",
    "        return \";;\".join(result)\n",
    "\n",
    "    # Fill in ortholog columns\n",
    "    TF_df[f\"human_entrez_gene\"] = TF_df[f'Gene ID'].apply(lambda id: fill_ortholog_column(id, \"human_entrez_gene\"))\n",
    "    TF_df[f\"hgnc_id\"]           = TF_df[f'Gene ID'].apply(lambda id: fill_ortholog_column(id, \"hgnc_id\"))\n",
    "    TF_df[f\"human_symbol\"]      = TF_df[f'Gene ID'].apply(lambda id: fill_ortholog_column(id, \"human_symbol\"))\n",
    "\n",
    "    return TF_df\n",
    "\n",
    "# TODO - This is outdated, too. We are not using HCOP HGNC symbols anymore, we're using Ensembl ones. Question, though: do we need the HGNC IDs? I added them just because. Are they needed?\n",
    "def add_HGNC_symbols(TF_df: pd.DataFrame, orthologs_path: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Adapted from 'postprocessing.py'\n",
    "    use ortholog dicts in orthologs_path (downloaded from HGNC) to get HGNC orthologs for mouse, rat, & human HGNC IDs\n",
    "    '''\n",
    "\n",
    "    ## HELPER FUNCTIONS\n",
    "    def get_unique_HGNC_symbol_index(row):\n",
    "        '''\n",
    "        Helper function to get a unique HGNC symbol for each Entrez ID, when there are multiple options.\n",
    "        1) Return exact lowercase match, if any\n",
    "        2) Closest string match (case-insensitive)\n",
    "        3) If no match, return NaN\n",
    "        '''\n",
    "\n",
    "        # If there's only one symbol, return index 0\n",
    "        if ';' not in row['human_symbol']:\n",
    "            return 0\n",
    "        \n",
    "        # Get all candidates\n",
    "        candidates = row[\"human_symbol\"].split(\";\")\n",
    "\n",
    "        # 1) Return exact lowercase match, if any\n",
    "        for i, c in enumerate(candidates):\n",
    "            if c.lower() == row['symbol'].lower():\n",
    "                return i\n",
    "\n",
    "        # 2) Closest string match (case-insensitive)\n",
    "        matches = difflib.get_close_matches(row['symbol'].upper(), [c.upper() for c in candidates], n=1, cutoff=0.0)\n",
    "        if matches:\n",
    "            # return the original candidate (not the uppercased one)\n",
    "            for i, c in enumerate(candidates):\n",
    "                if c.upper() == matches[0]:\n",
    "                    return i\n",
    "\n",
    "        # 3) No matches found - raise error\n",
    "        raise ValueError(f\"No match found for {row['symbol']}\")\n",
    "    \n",
    "    def assign_unique_fields(row):\n",
    "        '''Helper function to assign unique fields based on index'''\n",
    "        idx = get_unique_HGNC_symbol_index(row)\n",
    "\n",
    "        return pd.Series({\n",
    "            \"unique_human_symbol\": row[\"human_symbol\"].split(\";\")[idx],\n",
    "            \"unique_hgnc_id\": row[\"hgnc_id\"].split(\";\")[idx],\n",
    "            \"unique_human_entrez_gene\": row[\"human_entrez_gene\"].split(\";\")[idx],\n",
    "        })\n",
    "\n",
    "    def fill_ortholog_column(id, column):\n",
    "        '''Helper function to fill ortholog columns'''\n",
    "        result = []\n",
    "        for entrez_gene in id.split(\";\"):\n",
    "            result.append(orthologs_map[entrez_gene][column]) if entrez_gene in orthologs_map else \"-\"\n",
    "        return \";\".join(result)\n",
    "\n",
    "    # Create empty df to store orthologs\n",
    "    orthologs = pd.DataFrame()\n",
    "\n",
    "    # Get mouse & rat orthologs\n",
    "    for rodent in ['mouse', 'rat']:\n",
    "        # Load as string\n",
    "        hgnc_df = pd.read_csv(orthologs_path + f\"human_{rodent}_hcop_fifteen_column.txt\", sep='\\t', keep_default_na=False, dtype='str')\n",
    "        hgnc_df = hgnc_df.rename(columns={f\"{rodent}_entrez_gene\": \"entrez_gene\", f\"{rodent}_symbol\": \"symbol\"})\n",
    "        hgnc_df['TaxID'] = '10090' if rodent == 'mouse' else '10116'\n",
    "        orthologs = pd.concat([orthologs, hgnc_df])\n",
    "\n",
    "    # Get human HNGC symbols\n",
    "    h_hgnc_df = pd.read_csv(orthologs_path + \"hgnc_human.tsv\", sep='\\t', keep_default_na=False, dtype='str')\n",
    "    h_hgnc_df = h_hgnc_df.rename(columns={\"HGNC ID\": \"hgnc_id\", \"NCBI Gene ID\": \"entrez_gene\", \"Approved symbol\": \"symbol\"})\n",
    "    h_hgnc_df['human_entrez_gene'] = h_hgnc_df['entrez_gene']\n",
    "    h_hgnc_df['human_symbol'] = h_hgnc_df['symbol']\n",
    "    h_hgnc_df['TaxID'] = '9606'\n",
    "    orthologs = pd.concat([orthologs, h_hgnc_df])\n",
    "\n",
    "    # Keep only IDs present in the TF_df\n",
    "    TF_ids = {j for id in TF_df['Gene ID'].unique() for j in id.split(';')}\n",
    "    orthologs = orthologs[orthologs['entrez_gene'].isin(TF_ids)]\n",
    "\n",
    "    # Remove all rows that don't have a human entrez ID or hgnc ID\n",
    "    m = (orthologs['human_entrez_gene'] != '-') | (orthologs['hgnc_id'] != '-')\n",
    "    orthologs = orthologs[m]\n",
    "\n",
    "    # === Resolve cases where entrez_gene has a 1-to-many mapping with symbol. Can be mostly fixed by eliminating all \"LOCXXXX\" & \"GmXXXX\" ones===\n",
    "    # Get entrez IDs with multiple symbols\n",
    "    entrezIDs_with_multiple_symbols = (\n",
    "        orthologs[['entrez_gene', 'symbol']]\n",
    "        .drop_duplicates()\n",
    "        .groupby('entrez_gene')\n",
    "        .filter(lambda g: len(g) > 1)['entrez_gene']\n",
    "        .unique()\n",
    "    )\n",
    "    # Discard all rows with 'LOCXXXX' or 'GmXXXX' symbols for these entrez IDs (& assert this solves all cases)\n",
    "    m_to_discard = (\n",
    "        orthologs['entrez_gene'].isin(entrezIDs_with_multiple_symbols)\n",
    "        & (\n",
    "            orthologs['symbol'].str.contains(r'^(?:LOC|Gm\\d+)', regex=True)\n",
    "            | (orthologs['symbol'] == \"Fam220a\")    # 122526778\tis mapped to both Sagsin1 & Fam220a. NCBI states \"Sagsin1\" as \"also known as\", while \"Fam220a\" is not mentioned anywhere. Fam220a will be discarded (https://www.ncbi.nlm.nih.gov/gene/?term=122526778)\n",
    "        )\n",
    "    )\n",
    "    print(f\"Discarding {m_to_discard.sum()} rows with 'LOCXXXX' or 'GmXXXX' symbols: {', '.join(orthologs[m_to_discard]['symbol'].unique())}\\n\")\n",
    "    orthologs = orthologs[~m_to_discard]\n",
    "\n",
    "    # display(orthologs[['entrez_gene', 'symbol']].drop_duplicates()[orthologs[['entrez_gene', 'symbol']].drop_duplicates()['entrez_gene'].duplicated(keep=False)] )\n",
    "    assert (orthologs[['entrez_gene', 'symbol']].drop_duplicates()['entrez_gene'].duplicated().sum() == 0), \"There are still entrez IDs with multiple symbols after discarding 'LOCXXXX' and 'GmXXXX' ones\"\n",
    "\n",
    "    # Join with ';' when an EntrezID has more than 1 human ortholog\n",
    "    agg_funcs = {\n",
    "        \"symbol\": lambda x: ';'.join(x.unique()),\n",
    "        \"TaxID\": lambda x: ';'.join(x.unique()),\n",
    "        \"human_entrez_gene\": lambda x: ';'.join(x),\n",
    "        \"hgnc_id\": lambda x: ';'.join(x),\n",
    "        \"human_symbol\": lambda x: ';'.join(x),\n",
    "    }\n",
    "    orthologs = orthologs.groupby(['entrez_gene']).agg(agg_funcs).reset_index()\n",
    "\n",
    "    # Add NFKB & AP1 orthologs\n",
    "    for dimer in ['NFKB', 'AP1']:\n",
    "        orthologs = pd.concat([orthologs, pd.DataFrame([{\n",
    "            'entrez_gene': f'Complex:{dimer}',\n",
    "            'symbol': dimer,\n",
    "            'TaxID': '9606',\n",
    "            'human_entrez_gene': f'Complex:{dimer}',\n",
    "            'hgnc_id': f'Complex:{dimer}',\n",
    "            'human_symbol': dimer,\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "    # Add columns with 1-to-1 mapping (unique_human_symbol, unique_hgnc_id, unique_human_entrez_gene), by using either exact match or closest match of symbol wrt human_symbol\n",
    "    orthologs = orthologs.join(orthologs.apply(assign_unique_fields, axis=1))\n",
    "\n",
    "    # Show how many orthologs we get\n",
    "    print(f\"We get ortholog info for {len(orthologs)}/{len(TF_ids)} Gene IDs\\n\")\n",
    "\n",
    "    # Fill in ortholog columns for TFs and TGs using the \"unique\" fields for a 1-to-1 mapping\n",
    "    orthologs_map = orthologs.set_index('entrez_gene').to_dict(orient='index')\n",
    "    TF_df[f\"human_entrez_gene\"] = TF_df[f'Gene ID'].apply(lambda id: fill_ortholog_column(id, \"unique_human_entrez_gene\"))\n",
    "    TF_df[f\"hgnc_id\"]           = TF_df[f'Gene ID'].apply(lambda id: fill_ortholog_column(id, \"unique_hgnc_id\"))\n",
    "    TF_df[f\"human_symbol\"]      = TF_df[f'Gene ID'].apply(lambda id: fill_ortholog_column(id, \"unique_human_symbol\"))\n",
    "\n",
    "    return TF_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLS: ['lambert_2018.present', 'Lovering_2021.present', 'animal_tfdb_Homo_sapiens_cofactors.present', 'animal_tfdb_Mus_musculus_cofactors.present', 'animal_tfdb_Rattus_norvegicus_cofactors.present', 'tcof_cotf_human.present', 'tcof_cotf_mouse.present', 'TFclass_human', 'TFclass_mouse', 'TFclass_rat']\n",
      "Discarding 6 rows with 'LOCXXXX' or 'GmXXXX' symbols: Fam220a, LOC100912163, LOC100912068, LOC100911668\n",
      "\n",
      "We get ortholog info for 9091/9195 Gene IDs\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "We will discard 528 TFs that are not classified into any TF type"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gene ID</th>\n",
       "      <th>TF type</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>TaxID</th>\n",
       "      <th>GO:0003700</th>\n",
       "      <th>GO:0003712</th>\n",
       "      <th>GO:0001098</th>\n",
       "      <th>GO:0002039</th>\n",
       "      <th>GO:0008134</th>\n",
       "      <th>GO:0042393</th>\n",
       "      <th>GO:0046332</th>\n",
       "      <th>GO:0006325</th>\n",
       "      <th>GO:0140993</th>\n",
       "      <th>GO:0006355</th>\n",
       "      <th>lambert_2018.present</th>\n",
       "      <th>Lovering_2021.present</th>\n",
       "      <th>animal_tfdb_Homo_sapiens_cofactors.present</th>\n",
       "      <th>animal_tfdb_Mus_musculus_cofactors.present</th>\n",
       "      <th>animal_tfdb_Rattus_norvegicus_cofactors.present</th>\n",
       "      <th>tcof_cotf_human.present</th>\n",
       "      <th>tcof_cotf_mouse.present</th>\n",
       "      <th>TFclass_human</th>\n",
       "      <th>TFclass_mouse</th>\n",
       "      <th>TFclass_rat</th>\n",
       "      <th>updated TF type</th>\n",
       "      <th>In ExTRI</th>\n",
       "      <th>human_entrez_gene</th>\n",
       "      <th>hgnc_id</th>\n",
       "      <th>human_symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>18617</td>\n",
       "      <td>dbTF</td>\n",
       "      <td>Rhox5</td>\n",
       "      <td>10090</td>\n",
       "      <td>GO:0001228</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GO:0000122;GO:0001228;GO:0006357;GO:0045893;GO...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dbTF</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>245525</td>\n",
       "      <td>dbTF</td>\n",
       "      <td>Hsf3</td>\n",
       "      <td>10090</td>\n",
       "      <td>GO:0003700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GO:0003700;GO:0006355;GO:0045944</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TFclass_mouse.present</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dbTF</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Gene ID TF type Symbol  TaxID  GO:0003700 GO:0003712 GO:0001098  \\\n",
       "249    18617    dbTF  Rhox5  10090  GO:0001228        NaN        NaN   \n",
       "1138  245525    dbTF   Hsf3  10090  GO:0003700        NaN        NaN   \n",
       "\n",
       "     GO:0002039 GO:0008134 GO:0042393 GO:0046332 GO:0006325 GO:0140993  \\\n",
       "249         NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "1138        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "                                             GO:0006355 lambert_2018.present  \\\n",
       "249   GO:0000122;GO:0001228;GO:0006357;GO:0045893;GO...                  NaN   \n",
       "1138                   GO:0003700;GO:0006355;GO:0045944                  NaN   \n",
       "\n",
       "     Lovering_2021.present animal_tfdb_Homo_sapiens_cofactors.present  \\\n",
       "249                    NaN                                        NaN   \n",
       "1138                   NaN                                        NaN   \n",
       "\n",
       "     animal_tfdb_Mus_musculus_cofactors.present  \\\n",
       "249                                         NaN   \n",
       "1138                                        NaN   \n",
       "\n",
       "     animal_tfdb_Rattus_norvegicus_cofactors.present tcof_cotf_human.present  \\\n",
       "249                                              NaN                     NaN   \n",
       "1138                                             NaN                     NaN   \n",
       "\n",
       "     tcof_cotf_mouse.present TFclass_human          TFclass_mouse TFclass_rat  \\\n",
       "249                      NaN           NaN                    NaN         NaN   \n",
       "1138                     NaN           NaN  TFclass_mouse.present         NaN   \n",
       "\n",
       "     updated TF type  In ExTRI human_entrez_gene hgnc_id human_symbol  \n",
       "249             dbTF      True                                         \n",
       "1138            dbTF      True                                         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "updated TF type\n",
       "dbTF              4161\n",
       "coTF candidate    3573\n",
       "coTF               933\n",
       "                   528\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61401/3225740463.py:121: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  .groupby([\"GO term\", \"updated TF type\"])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>updated TF type</th>\n",
       "      <th>dbTF</th>\n",
       "      <th>coTF</th>\n",
       "      <th>coTF candidate</th>\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GO:0006355</th>\n",
       "      <td>3062</td>\n",
       "      <td>914</td>\n",
       "      <td>1657</td>\n",
       "      <td>0</td>\n",
       "      <td>5633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0003700</th>\n",
       "      <td>2646</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0006325</th>\n",
       "      <td>349</td>\n",
       "      <td>271</td>\n",
       "      <td>1168</td>\n",
       "      <td>0</td>\n",
       "      <td>1788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0008134</th>\n",
       "      <td>609</td>\n",
       "      <td>327</td>\n",
       "      <td>484</td>\n",
       "      <td>0</td>\n",
       "      <td>1420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0003712</th>\n",
       "      <td>209</td>\n",
       "      <td>933</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0140993</th>\n",
       "      <td>88</td>\n",
       "      <td>113</td>\n",
       "      <td>268</td>\n",
       "      <td>0</td>\n",
       "      <td>469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0042393</th>\n",
       "      <td>42</td>\n",
       "      <td>54</td>\n",
       "      <td>269</td>\n",
       "      <td>0</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0046332</th>\n",
       "      <td>74</td>\n",
       "      <td>30</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0002039</th>\n",
       "      <td>20</td>\n",
       "      <td>41</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0001098</th>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "updated TF type  dbTF  coTF  coTF candidate     Total\n",
       "GO term                                              \n",
       "GO:0006355       3062   914            1657  0   5633\n",
       "GO:0003700       2646     0               0  0   2646\n",
       "GO:0006325        349   271            1168  0   1788\n",
       "GO:0008134        609   327             484  0   1420\n",
       "GO:0003712        209   933               0  0   1142\n",
       "GO:0140993         88   113             268  0    469\n",
       "GO:0042393         42    54             269  0    365\n",
       "GO:0046332         74    30              92  0    196\n",
       "GO:0002039         20    41              92  0    153\n",
       "GO:0001098         28    19              95  0    142"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Join all TF IDs into one dataframe\n",
    "TFs_list = []\n",
    "for TF in ['dbTF', 'coTF', 'll_coTF']:\n",
    "    path = get_TF_ids_path(TF, postprocessing_path) \n",
    "    with open(path, 'r') as f:\n",
    "        all_gene_IDs = f.read().splitlines()\n",
    "        TFs_list.extend([(gene_id, TF) for gene_id in all_gene_IDs])\n",
    "TFs_df = pd.DataFrame(TFs_list, columns=[\"Gene ID\", \"TF type\"])\n",
    "\n",
    "# Drop coTFs that are also ll_coTFs\n",
    "coTFs_set = set(TFs_df[TFs_df['TF type'] == 'coTF']['Gene ID'])\n",
    "TFs_df = TFs_df[~((TFs_df['TF type'] == 'coTF') & (TFs_df['Gene ID'].isin(ll_coTF)))]\n",
    "assert len(TFs_df) == len(set(TFs_df['Gene ID'])), \"There are duplicated Gene IDs in the TFs_df\"\n",
    "\n",
    "# Use eutils to map each gene ID to the gene symbol & TF type\n",
    "annotationsSummary = retrieve_annotations(TFs_df['Gene ID'].tolist())\n",
    "EntrezIDtoSymbol = {ID : {'Name': annotation[\"Name\"], 'TaxID': annotation['Organism']['TaxID']} for ID, annotation in zip(TFs_df['Gene ID'].tolist(), annotationsSummary)}\n",
    "TFs_df['Symbol'] = TFs_df['Gene ID'].map(lambda ID: EntrezIDtoSymbol[ID]['Name'])\n",
    "TFs_df['TaxID'] = TFs_df['Gene ID'].map(lambda ID: EntrezIDtoSymbol[ID]['TaxID'])\n",
    "\n",
    "\n",
    "# --- JOIN WITH QUICKGO ---\n",
    "for GO_term in GO_dbTF + GO_coTF + [\"GO:0006355\"]:\n",
    "    # Load the GO term table\n",
    "    GO_table = pd.read_csv(in_data_path + f\"QuickGO-{GO_term.replace(':', '')}.tsv\", sep=\"\\t\", header=0, dtype='str')\n",
    "\n",
    "    # Process the GO term table\n",
    "    GO_table = (\n",
    "        GO_table[['SYMBOL', 'TAXON ID', 'GO TERM']].drop_duplicates()\n",
    "        .groupby([\"SYMBOL\", \"TAXON ID\"], as_index=False)\n",
    "        .agg({\"GO TERM\": lambda x: \";\".join(sorted(set(x.dropna().astype(str))))})\n",
    "        .rename(columns={\"GO TERM\": GO_term})\n",
    "    )\n",
    "\n",
    "    # Merge with TFs_df\n",
    "    TFs_df = TFs_df.merge(GO_table, left_on=['Symbol', 'TaxID'], right_on=['SYMBOL', 'TAXON ID'], how='left').drop(columns=[\"TAXON ID\", \"SYMBOL\"])\n",
    "\n",
    "\n",
    "# --- JOIN WITH TFCHECKPOINT ---\n",
    "# Join together duplicated rows in TFCheckpoint\n",
    "cols = [col for cols in TFCheckpoint_cols.values() for col in cols if ((\"GO:\" not in col) & (col != 'TFclass.present.merged'))] + ['TFclass_human', 'TFclass_mouse', 'TFclass_rat'] # Only include relevant columns\n",
    "print(\"COLS:\", cols)\n",
    "TFCheckpoint_agg = (\n",
    "    TFCheckpoint_exploded[['EntrezIDs'] + cols]\n",
    "    .groupby('EntrezIDs', as_index=False)\n",
    "    .agg({\n",
    "        c: (lambda x: \";\".join(sorted(set(x.dropna().astype(str)))))\n",
    "        for c in cols\n",
    "    })\n",
    ")\n",
    "\n",
    "# Merge with TFCheckpoint\n",
    "TFs_df = TFs_df.merge(TFCheckpoint_agg[cols + ['EntrezIDs']], left_on=['Gene ID'], right_on=['EntrezIDs'], how='left').drop(columns=[\"EntrezIDs\"])\n",
    "\n",
    "# Ensure sources only have values for the correct species\n",
    "import numpy as np\n",
    "TFs_df.loc[TFs_df['TaxID'] != '9606',  ['TFclass_human', 'animal_tfdb_Homo_sapiens_cofactors.present', 'tcof_cotf_human.present', 'lambert_2018.present', 'Lovering_2021.present']] = np.nan\n",
    "TFs_df.loc[TFs_df['TaxID'] != '10090', ['TFclass_mouse', 'animal_tfdb_Mus_musculus_cofactors.present', 'tcof_cotf_mouse.present']] = np.nan\n",
    "TFs_df.loc[TFs_df['TaxID'] != '10116', ['TFclass_rat',   'animal_tfdb_Rattus_norvegicus_cofactors.present']] = np.nan\n",
    "\n",
    "# --- CREATE NEW CATEGORISATION\n",
    "new_categorisation = {\n",
    "    'dbTF': GO_dbTF + [col for col in TFCheckpoint_cols['dbTF'] if col not in ['TFclass.present.merged']] + ['TFclass_human', 'TFclass_mouse', 'TFclass_rat'],\n",
    "    'coTF candidate': GO_coTF + TFCheckpoint_cols['coTF'] + [\"GO:0006355\"],\n",
    "    'coTF': [\"GO:0003712\"]\n",
    "}\n",
    "\n",
    "TFs_df = TFs_df.replace(\"\", np.nan) # Replace empty strings with NaN\n",
    "TFs_df[\"updated TF type\"] = '' # Default\n",
    "# dbTF will overwrite coTF, which will overwrite coTF candidate\n",
    "for tf_type in ['coTF candidate', 'coTF', 'dbTF']:\n",
    "    cols = new_categorisation[tf_type]\n",
    "    TFs_df.loc[TFs_df[cols].notna().any(axis=1), 'updated TF type'] = tf_type\n",
    "\n",
    "# --- CLEAN UP & SAVE ---\n",
    "\n",
    "# Create column if in ExTRI2 dataset\n",
    "ExTRI2_df = pd.read_csv(\"../../results/ExTRI2_final_resource.tsv\", sep=\"\\t\", dtype='str')\n",
    "TFs_df[\"In ExTRI\"] = TFs_df['Gene ID'].isin(ExTRI2_df['TF Id'])\n",
    "\n",
    "# Add HGNC ID for human genes\n",
    "orthologs_path =       '../../data/external/human_HGNC_orthologs/'\n",
    "TFs_df = add_HGNC_symbols(TFs_df, orthologs_path)\n",
    "\n",
    "# Assertions\n",
    "assert len(TFs_df) == len(set(TFs_df['Gene ID'])), \"There are duplicated Gene IDs in the TFs_df\"\n",
    "\n",
    "# Sort rows\n",
    "order = [\"dbTF\", \"coTF\", \"coTF candidate\", \"\"]\n",
    "TFs_df[\"updated TF type\"] = pd.Categorical(TFs_df[\"updated TF type\"], categories=order, ordered=True)\n",
    "TFs_df = TFs_df.sort_values(by=['In ExTRI', 'updated TF type', 'human_symbol', 'TaxID'], ascending=[False, True, True, True])\n",
    "\n",
    "# Save the complete dataset\n",
    "TFs_df.to_csv(\"../../analysis/tables/all_TFs.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Save the 2 tables we'll include in the paper\n",
    "md(f'We will discard {TFs_df[TFs_df[\"updated TF type\"] == \"\"].shape[0]} TFs that are not classified into any TF type')\n",
    "TFs_df[TFs_df['updated TF type'] != ''][['Gene ID', 'Symbol', 'TaxID']].to_csv(\"../../data/paper_tables/all_considered_TFs.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "(\n",
    "    TFs_df\n",
    "    .drop(columns=['TF type'])\n",
    "    .rename(columns={'updated TF type': 'TF type'})\n",
    "    .loc[TFs_df['In ExTRI'] & (TFs_df['updated TF type'] != '')]\n",
    "    .drop(columns=['In ExTRI'])\n",
    "    .to_csv(\"../../data/paper_tables/TFs_in_ExTRI2.tsv\", sep=\"\\t\", index=False)\n",
    ")\n",
    "\n",
    "# Show stats\n",
    "pd.set_option('display.max_columns', None)\n",
    "display(TFs_df.head(2))\n",
    "display(TFs_df['updated TF type'].value_counts(dropna=False))\n",
    "\n",
    "# Show updated TF types per GO term\n",
    "summary = (\n",
    "    TFs_df\n",
    "    .melt(id_vars=\"updated TF type\", value_vars=[c for c in TFs_df.columns if c.startswith(\"GO:\")],\n",
    "          var_name=\"GO term\", value_name=\"present\")\n",
    "    .assign(present=lambda d: d[\"present\"].notna())\n",
    "    .query(\"present == True\")\n",
    "    .groupby([\"GO term\", \"updated TF type\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)       # columns = updated TF type\n",
    ")\n",
    "summary[\"Total\"] = summary.sum(axis=1)\n",
    "display(summary.sort_values(\"Total\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFs without updated TF type: 528\n",
      "TFs in ExTRI2 without updated TF type:   149\n",
      "ExTRI2 with TFs without updated TF type: 3372 0.004035317404280883\n"
     ]
    }
   ],
   "source": [
    "print(\"TFs without updated TF type:\", (TFs_df['updated TF type'] == '').sum())\n",
    "print(\"TFs in ExTRI2 without updated TF type:  \", (TFs_df[TFs_df['In ExTRI']]['updated TF type'] == '').sum())\n",
    "m = TFs_df['In ExTRI'] & (TFs_df['updated TF type'] == '')\n",
    "print(\"ExTRI2 with TFs without updated TF type:\", ExTRI2_df['TF Id'].isin(TFs_df[m]['Gene ID']).sum(), (ExTRI2_df['TF Id'].isin(TFs_df[m]['Gene ID'])).sum() / len(ExTRI2_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique GO:0006355 entries: 7338\n",
      "Number considered: 5633\n"
     ]
    }
   ],
   "source": [
    "# Check how many GO:0006355 we retrieve\n",
    "GO_term = \"GO:0006355\"\n",
    "\n",
    "# Process the GO term table\n",
    "GO_table = pd.read_csv(in_data_path + f\"QuickGO-{GO_term.replace(':', '')}.tsv\", sep=\"\\t\", header=0, dtype='str')\n",
    "\n",
    "GO_table = (\n",
    "    GO_table[['SYMBOL', 'TAXON ID', 'GO TERM']].drop_duplicates()\n",
    "    .groupby([\"SYMBOL\", \"TAXON ID\"], as_index=False)\n",
    "    .agg({\"GO TERM\": lambda x: \";\".join(sorted(set(x.dropna().astype(str))))})\n",
    "    .rename(columns={\"GO TERM\": GO_term})\n",
    ")\n",
    "\n",
    "print(f\"Unique {GO_term} entries: {GO_table.shape[0]}\")\n",
    "print(f\"Number considered: {TFs_df[GO_term].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outdated - Prepare data to separate likely from less-likely coTFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get coTF human orthologs for filtering\n",
    "coTFs are converted to their human orthologs to then apply a filtering and discard non-coTFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous list of coTFs is polluted with proteins that do not act as coTFs. They are filtered out by analysing their human orthologs. For that:\n",
    "1. coTFs are separarted in human & mouse/rat coTFs.\n",
    "2. Human orthologs for mouse/rat coTFs are oubtained. Those mouse/rat coTFs without human orthologs (24/2822 cases) are discarded.\n",
    "3. Mouse/rat IDs frequently have +1 human orthologs. To not pollute the final coTF list with unnecessary orthologs, we check, for each mouse/rat coTF, if any of the human orthologs is already in the human coTF list. The majority has at least one, so the other orthologs were not added.\n",
    "4. For those that did not have any ortholog in the human coTF list (99 cases), we add all their human orthologs (115 additional ones).\n",
    "5. We create a TSV file with the resulting 1876 human coTF IDs + their NCBI Symbol, which is used to find filtering strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "data_path = '../../data/'\n",
    "orthologs_path =       data_path + 'external/human_HGNC_orthologs/'\n",
    "all_human_coTFs_path = data_path + 'tmp/all_human_ortholog_coTFs.tsv'\n",
    "r_m_coTFs_wo_orthologs_path = data_path + 'tmp/r_m_coTFs_wo_orthologs.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "from Bio import Entrez\n",
    "import os\n",
    "Entrez.email = \"example24@gmail.com\"\n",
    "\n",
    "def retrieve_annotations(id_list):\n",
    "    \"\"\"Annotates Entrez Gene IDs using Bio.Entrez, in particular epost (to\n",
    "    submit the data to NCBI) and esummary to retrieve the information.\n",
    "    Returns a list of dictionaries with the annotations.\"\"\"\n",
    "\n",
    "    request = Entrez.epost(\"gene\", id=\",\".join(id_list))\n",
    "    result = Entrez.read(request)\n",
    "    webEnv = result[\"WebEnv\"]\n",
    "    queryKey = result[\"QueryKey\"]\n",
    "    data = Entrez.esummary(db=\"gene\", webenv=webEnv, query_key=queryKey)\n",
    "    annotations = Entrez.read(data)\n",
    "    annotationsSummary = annotations['DocumentSummarySet']['DocumentSummary']\n",
    "\n",
    "    assert len(id_list) == len(annotationsSummary), f\"id_list and annotationsSummary are of different length: {len(id_list)} != {len(annotationsSummary)}\"\n",
    "\n",
    "    return annotationsSummary\n",
    "\n",
    "def get_rodent_HGNC_orthologs(coTF_ids: list, orthologs_path: str):\n",
    "    orthologs = pd.DataFrame()\n",
    "\n",
    "    for rodent in ['mouse', 'rat']:\n",
    "        # Load dataframe\n",
    "        path = os.path.join(orthologs_path, f\"human_{rodent}_hcop_fifteen_column.txt\")\n",
    "        hgnc_df = pd.read_csv(path, sep='\\t', header=0, keep_default_na=False, dtype=str)\n",
    "\n",
    "        # Rename\n",
    "        hgnc_df = hgnc_df.rename(columns={f\"{rodent}_entrez_gene\": \"entrez_gene\", f\"{rodent}_symbol\": \"symbol\"})\n",
    "        \n",
    "        orthologs = pd.concat([orthologs, hgnc_df])\n",
    "\n",
    "    # Keep only IDs present in the coTF list\n",
    "    orthologs = orthologs[orthologs['entrez_gene'].isin(coTF_ids)]\n",
    "\n",
    "    # Remove all rows that don't have a human entrez ID or hgnc ID\n",
    "    m = (orthologs['human_entrez_gene'] != '-') | (orthologs['hgnc_id'] != '-')\n",
    "    orthologs = orthologs[m]\n",
    "\n",
    "    # Join with ';' when an EntrezID has more than 1 human ortholog\n",
    "    agg_funcs = {\n",
    "        \"symbol\": lambda x: ';'.join(x.unique()),\n",
    "        \"human_entrez_gene\": lambda x: ';'.join(x.unique()),\n",
    "        \"hgnc_id\": lambda x: ';'.join(x.unique()),\n",
    "        \"human_symbol\": lambda x: ';'.join(x.unique())\n",
    "    }\n",
    "    orthologs = orthologs.groupby(['entrez_gene']).agg(agg_funcs).reset_index()\n",
    "\n",
    "    # Show how many we get\n",
    "    print(f\"We get ortholog info for {len(orthologs)}/{len(coTF_ids)} Gene IDs\")\n",
    "\n",
    "    # Convert into dictionary\n",
    "    orthologs_map = orthologs.set_index('entrez_gene').to_dict(orient='index')\n",
    "\n",
    "    return orthologs_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We get ortholog info for 2875/2902 Gene IDs\n",
      "In 2777 / 2875 mouse/rat IDs, at least one of their orthologs is also in the human_coTFs list.\n",
      "For those that don't, we'll take all its human orthologs: 112 extra human IDs\n"
     ]
    }
   ],
   "source": [
    "# Load coTF IDs list\n",
    "path = get_TF_ids_path('coTF', postprocessing_path)\n",
    "with open(path, 'r') as f:\n",
    "    coTF_ids = [l.strip('\\n') for l in f]\n",
    "\n",
    "# Create map between the ID & its Name / TaxID\n",
    "annotationsSummary = retrieve_annotations(coTF_ids)\n",
    "EntrezIDtoSymbol = {ID : {'Name': annotation[\"Name\"], 'TaxID': annotation['Organism']['TaxID']} for ID, annotation in zip(coTF_ids, annotationsSummary)}\n",
    "\n",
    "# Separate coTFs between human & mouse/rat\n",
    "human_coTFs = {id for id, val in EntrezIDtoSymbol.items() if val['TaxID'] == '9606'}\n",
    "m_r_coTFs   = {id for id, val in EntrezIDtoSymbol.items() if val['TaxID'] != '9606'}\n",
    "\n",
    "# Get mouse/rat human orthologs as a dataframe\n",
    "orthologs_map = get_rodent_HGNC_orthologs(m_r_coTFs, orthologs_path)\n",
    "orth_df = pd.DataFrame.from_dict(orthologs_map, orient='index')\n",
    "orth_df.index.name = 'ID'\n",
    "r_m_coTFs_wo_orthologs = m_r_coTFs.difference(set(orth_df.index))\n",
    "\n",
    "# Mask for mouse/rat IDs with an ortholog inside the human_coTFs list\n",
    "def contains_coTF(ids: str, coTFs: set):\n",
    "    return any(id in coTFs for id in ids.split(';'))\n",
    "m = orth_df['human_entrez_gene'].apply(lambda ids: contains_coTF(ids, human_coTFs))\n",
    "print(f\"In {m.sum()} / {len(orth_df)} mouse/rat IDs, at least one of their orthologs is also in the human_coTFs list.\")\n",
    "\n",
    "# Get all human orthologs of mouse/rat IDs w/o orthologs already present in human_coTFs\n",
    "remaining_m_r_human_orthologs = {id for id_list in orth_df[~m]['human_entrez_gene'] for id in id_list.split(';')}\n",
    "print(f\"For those that don't, we'll take all its human orthologs: {len(remaining_m_r_human_orthologs)} extra human IDs\")\n",
    "\n",
    "# Join human coTFs + the other human orthologs.\n",
    "all_human_IDs = remaining_m_r_human_orthologs | human_coTFs\n",
    "\n",
    "# Get the symbol from Entrez & save the list as a tsv file.\n",
    "annotationsSummary = retrieve_annotations(all_human_IDs)\n",
    "EntrezIDtoSymbol = {ID : {'Name': annotation[\"Name\"], 'TaxID': annotation['Organism']['TaxID']} for ID, annotation in zip(coTF_ids, annotationsSummary)}\n",
    "all_human_coTFs_df = pd.DataFrame.from_dict(EntrezIDtoSymbol, orient='index')\n",
    "all_human_coTFs_df.index.name = 'ID'\n",
    "all_human_coTFs_df.to_csv(all_human_coTFs_path, sep='\\t')\n",
    "\n",
    "# Do the same for the mouse/rat coTFs without human orthologs\n",
    "annotationsSummary = retrieve_annotations(r_m_coTFs_wo_orthologs)\n",
    "EntrezIDtoSymbol = {ID : {'Name': annotation[\"Name\"], 'TaxID': annotation['Organism']['TaxID']} for ID, annotation in zip(r_m_coTFs_wo_orthologs, annotationsSummary)}\n",
    "r_m_coTFs_wo_orthologs_df = pd.DataFrame.from_dict(EntrezIDtoSymbol, orient='index')\n",
    "r_m_coTFs_wo_orthologs_df.index.name = 'ID'\n",
    "r_m_coTFs_wo_orthologs_df.to_csv(r_m_coTFs_wo_orthologs_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for the mouse/rat coTFs without human orthologs\n",
    "annotationsSummary = retrieve_annotations(r_m_coTFs_wo_orthologs)\n",
    "EntrezIDtoSymbol = {ID : {'Name': annotation[\"Name\"], 'TaxID': annotation['Organism']['TaxID']} for ID, annotation in zip(r_m_coTFs_wo_orthologs, annotationsSummary)}\n",
    "r_m_coTFs_wo_orthologs_df = pd.DataFrame.from_dict(EntrezIDtoSymbol, orient='index')\n",
    "r_m_coTFs_wo_orthologs_df.index.name = 'ID'\n",
    "r_m_coTFs_wo_orthologs_df.to_csv(r_m_coTFs_wo_orthologs_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send all coTFs to be separated into coTF & ll-coTF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method did not properly separate them: there were still several \"less likely\" coTFs in the \"likely\" list. Therefore, we use a different approach:\n",
    "1. Get all coTFs. Sort them alphabetically (case insensitive). \n",
    "2. Create 'likely' column, and mark it as \"less likely\" if the symbol is present in the list of less_likely_human_coTFs (case insensitive)\n",
    "3. Create an Excel, to recheck the likely columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all coTF Symbols from Entrez\n",
    "annotationsSummary = retrieve_annotations(coTF_ids)\n",
    "EntrezIDtoSymbol = {ID : {'Name': annotation[\"Name\"], 'TaxID': annotation['Organism']['TaxID']} for ID, annotation in zip(coTF_ids, annotationsSummary)}\n",
    "all_coTFs = pd.DataFrame.from_dict(EntrezIDtoSymbol, orient='index')\n",
    "all_coTFs.index.name = 'ID'\n",
    "all_coTFs.sort_values(by='Name', inplace=True, key=lambda col: col.str.lower())\n",
    "\n",
    "# Get list of less likely human coTFs\n",
    "data_path = '../../data/'\n",
    "orthologs_path               = data_path + 'external/human_HGNC_orthologs/'\n",
    "ll_human_coTFs_path = data_path + 'postprocessing/all_human_ortholog_coTFs_labelled_less_likely.txt'\n",
    "all_coTFs_likely_path =        data_path + 'tmp/all_coTFs_likely.tsv'\n",
    "\n",
    "# Get the set of the human IDs less likely to be coTFs\n",
    "human_coTFs_labelled = pd.read_csv(ll_human_coTFs_path, sep='\\t', header=0, dtype='str')\n",
    "h_ll_coTFs_symbol = set(human_coTFs_labelled[human_coTFs_labelled['less likely'] == 'less likely']['Name'])\n",
    "\n",
    "# Create a \"likely\" column\n",
    "all_coTFs['likely'] = all_coTFs['Name'].str.upper().isin(h_ll_coTFs_symbol).apply(lambda x: 'less likely' if x else 'likely')\n",
    "\n",
    "# Rename columns\n",
    "all_coTFs = all_coTFs.rename(columns={'Name': 'NCBI Symbol', 'ID': 'NCBI ID'})\n",
    "all_coTFs.index.name = 'NCBI ID'\n",
    "\n",
    "# Save\n",
    "all_coTFs.to_csv(all_coTFs_likely_path, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".general_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
