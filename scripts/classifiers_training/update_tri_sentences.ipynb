{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df4c0c1a-3b10-451f-8884-d231ec89b734",
   "metadata": {},
   "source": [
    "# Update raw data and NTNU dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302e995d-7420-47a1-a0e8-91cc46c6fbfc",
   "metadata": {},
   "source": [
    "modify `original_tri_sentences.tsv` to obtain `tri_sentences.tsv` with the updated annotations, including:\n",
    "* The re-annotations\n",
    "* Negation sentences from the extended NTNU dataset\n",
    "\n",
    "After running this notebook, `make_train_data.ipynb` must also be run, which converts the created `tri_sentences.tsv` into the datasets used as input for training the TRI and MoR classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e5a66afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3>Table of contents</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "[Update raw data and NTNU dataset](#Update-raw-data-and-NTNU-dataset)\n",
       "- [Setup](#Setup)\n",
       "- [Clean the dataset](#Clean-the-dataset)\n",
       "  - [Modify joined sentences](#Modify-joined-sentences)\n",
       "- [Reannotations](#Reannotations)\n",
       "- [Save the dataset](#Save-the-dataset)\n",
       "- [Deprecated - Add negative sentences](#Deprecated---Add-negative-sentences)\n",
       "  - [Enhance with negations from the extended NTNU dataset](#Enhance-with-negations-from-the-extended-NTNU-dataset)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "__import__('sys').path.append('../common/'); __import__('notebook_utils').table_of_contents('update_tri_sentences.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd45d45",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1b82aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from IPython.display import display, HTML, display_html\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# My functions\n",
    "import sys\n",
    "sys.path.append('../common/') \n",
    "from analysis_utils import prettify_plots\n",
    "from notebook_utils import table_from_dict, md, h3, h4, highlight_words\n",
    "\n",
    "prettify_plots()\n",
    "\n",
    "# %pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "49e4c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS  & LOAD DATASETS\n",
    "# Inputs\n",
    "data_path = '../../data/external/'\n",
    "original_data_path = data_path + 'original_tri_sentences.tsv'\n",
    "NTNU_extended_path = data_path + 'NTNU_extended.tsv'\n",
    "\n",
    "dataset_improvement_path = '../../data/dataset_improvement/'\n",
    "iter_1_2_path      = dataset_improvement_path + 'to_reannotate/iter1_iter2_worst_predictions_validated.txt'\n",
    "iter_3_path        = dataset_improvement_path + 'to_reannotate/iter3_worst_preds_AL_ver2.txt'\n",
    "negations_to_add_path = dataset_improvement_path + 'negations_NTNU.tsv'\n",
    "\n",
    "# Load datasets\n",
    "original_data   = pd.read_csv(original_data_path, sep='\\t', header=1, index_col=None, dtype='str')\n",
    "iter_1_2        = pd.read_csv(iter_1_2_path, sep='\\t', header=0, index_col=None, dtype='str')\n",
    "iter_3          = pd.read_csv(iter_3_path, sep='\\t', header=0, index_col=None, dtype='str')\n",
    "NTNU_extended   = pd.read_csv(NTNU_extended_path, sep='\\t', header=1, index_col=None, dtype='str')\n",
    "negations_to_add   = pd.read_csv(negations_to_add_path, sep='\\t', header=0, index_col=None, dtype='str')\n",
    "\n",
    "\n",
    "# Outputs\n",
    "splitted_sentences_path = dataset_improvement_path + 'to_reannotate/splitted_sentences_ids.txt'\n",
    "\n",
    "classifiers_data_path = '../../classifiers_training/data/'\n",
    "training_data_path  = classifiers_data_path + 'tri_sentences.tsv'\n",
    "NTNU_dataset_path = '../../results/NTNU_dataset.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c12411",
   "metadata": {},
   "source": [
    "## Clean the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b79f86",
   "metadata": {},
   "source": [
    "There are a few duplicated sentences: same PMID and sentence number, but different TF or TG ID. Some also differ in their TRI/MoR annotations. They will be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6d0f85b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Information on duplicated rows</h4><table><tr><td><b>Total number of sentences</b></td><td>22135</td></tr><tr><td><b>Unique sentences</b></td><td>21763</td></tr><tr><td><b>Duplicated sentences</b></td><td>735 (3.3%)</td></tr><tr><td><b>.. where TRI or MoR differs</b></td><td>278 (1.3%)</td></tr><tr><td><b>Unique duplicated sentences</b></td><td>363</td></tr><tr><td><b>.. where TRI or MOR differs</b></td><td>137</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Two sets of duplicated sentences where either Label or Type differ:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#TRI ID</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>TF</th>\n",
       "      <th>TG</th>\n",
       "      <th>Label</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20620</th>\n",
       "      <td>21056029:3:NFKB:CCND1</td>\n",
       "      <td>-catenin activated transcription from the [TG] promoter, while co-expression of NF-B [TF] reduced -catenin-induced transcription.</td>\n",
       "      <td>p65</td>\n",
       "      <td>cyclin D1</td>\n",
       "      <td>false</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10925</th>\n",
       "      <td>21056029:3:RELA:CCND1</td>\n",
       "      <td>-catenin activated transcription from the [TG] promoter, while co-expression of NF-B [TF] reduced -catenin-induced transcription.</td>\n",
       "      <td>p65</td>\n",
       "      <td>cyclin D1</td>\n",
       "      <td>true</td>\n",
       "      <td>REPRESSION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12854</th>\n",
       "      <td>21720709:0:TCF7L2:AKT1</td>\n",
       "      <td>-catenin/[TF] complex transcriptionally regulates [TG] in glioma.</td>\n",
       "      <td>Tcf-4</td>\n",
       "      <td>AKT1</td>\n",
       "      <td>true</td>\n",
       "      <td>UNDEFINED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19541</th>\n",
       "      <td>21720709:0:TCF4:AKT1</td>\n",
       "      <td>-catenin/[TF] complex transcriptionally regulates [TG] in glioma.</td>\n",
       "      <td>Tcf-4</td>\n",
       "      <td>AKT1</td>\n",
       "      <td>false</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      #TRI ID  \\\n",
       "20620   21056029:3:NFKB:CCND1   \n",
       "10925   21056029:3:RELA:CCND1   \n",
       "12854  21720709:0:TCF7L2:AKT1   \n",
       "19541    21720709:0:TCF4:AKT1   \n",
       "\n",
       "                                                                                                                                Sentence  \\\n",
       "20620  -catenin activated transcription from the [TG] promoter, while co-expression of NF-B [TF] reduced -catenin-induced transcription.   \n",
       "10925  -catenin activated transcription from the [TG] promoter, while co-expression of NF-B [TF] reduced -catenin-induced transcription.   \n",
       "12854                                                                  -catenin/[TF] complex transcriptionally regulates [TG] in glioma.   \n",
       "19541                                                                  -catenin/[TF] complex transcriptionally regulates [TG] in glioma.   \n",
       "\n",
       "          TF         TG  Label        Type  \n",
       "20620    p65  cyclin D1  false         NaN  \n",
       "10925    p65  cyclin D1   true  REPRESSION  \n",
       "12854  Tcf-4       AKT1   true   UNDEFINED  \n",
       "19541  Tcf-4       AKT1  false         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SHOW NUMBER OF DUPLICATE ROWS\n",
    "unique_rows = original_data['Sentence'].unique()\n",
    "duplicated_rows = original_data[original_data['Sentence'].duplicated(keep=False)]\n",
    "unique_duplicated_rows = duplicated_rows.drop_duplicates(subset='Sentence')\n",
    "duplicates_with_diff_label_or_type = original_data.groupby('Sentence').filter(lambda x: x['Label'].nunique() > 1 or x['Type'].nunique() > 1)\n",
    "\n",
    "table_from_dict('Information on duplicated rows', {\n",
    "    'Total number of sentences': len(original_data),\n",
    "    'Unique sentences': len(unique_rows),\n",
    "    'Duplicated sentences': f'{len(duplicated_rows)} ({round(len(duplicated_rows)/len(original_data)*100,1)}%)',\n",
    "    '.. where TRI or MoR differs': f'{len(duplicates_with_diff_label_or_type)} ({round(len(duplicates_with_diff_label_or_type)/len(original_data)*100,1)}%)',\n",
    "    'Unique duplicated sentences': len(unique_duplicated_rows),\n",
    "    '.. where TRI or MOR differs': len(duplicates_with_diff_label_or_type['Sentence'].unique()),\n",
    "}, heading='h4')\n",
    "\n",
    "md('Two sets of duplicated sentences where either Label or Type differ:')\n",
    "display(duplicates_with_diff_label_or_type.sort_values(by='Sentence').head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e352814c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "463 sentences are duplicated but have the same Label & MoR. Only the first is kept (234 are dropped)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "275 sentences are duplicated and differ on their Label or MoR. They're all dropped."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DROP DUPLICATES\n",
    "# Drop duplicates in which Sentence, TRI and MoR are the same\n",
    "m1 = original_data.duplicated(subset=['Sentence', 'Label', 'Type'], keep=False)\n",
    "m2 = original_data.duplicated(subset=['Sentence', 'Label', 'Type'], keep='first')\n",
    "md(f\"{m1.sum()} sentences are duplicated but have the same Label & MoR. Only the first is kept ({m2.sum()} are dropped).\")\n",
    "original_data = original_data.drop_duplicates(subset=['Sentence', 'Label', 'Type'], keep='first')\n",
    "\n",
    "# Drop rows where Sentence is duplicated, but Label and/or Type differ\n",
    "m = original_data.duplicated(subset=['Sentence'], keep=False)\n",
    "md(f\"{m.sum()} sentences are duplicated and differ on their Label or MoR. They're all dropped.\")\n",
    "original_data = original_data[~m]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aab1c40",
   "metadata": {},
   "source": [
    "### Modify joined sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870b2ef3",
   "metadata": {},
   "source": [
    "The splitter we will use for the pipeline is `en_core_sci_md`. We will now make sure that the dataset we use for training is separated according to their standards. \n",
    "\n",
    "As explained in `scripts/preprocessing/spacy_splitter_analysis.ipynb`, the spacy splitter makes some mistakes, namely separating a sentence in 2 sometimes when a \".\" is combined by a lowercase letter, or when there is no \".\". To avoid these cases, we use a rule-based method to re-merge those sentences, defined in the `merge_sentences()` function.\n",
    "\n",
    "We pass all sentences in the dataset through the splitter, obtaining a 0.3% of cases where the sentences were incorrectly splitted. They can be divided into 2 groups:\n",
    "* **[TF] and [TG] are in the same sentence:** Those sentences have been revised, and are included in the `iter_1_2` dataset. They will be updated.\n",
    "* **[TF] and [TG] are in separate sentences:** Those sentences will be dropped, as they are no longer valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "076c1655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 14s, sys: 1.59 s, total: 1min 15s\n",
      "Wall time: 1min 15s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "28 rows (0.13%) have [TF] and [TG] in separate sentences. They are discarded.<br>Example:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#TRI ID</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4132</th>\n",
       "      <td>12525489:0:activator protein 1:tissue inhibitors of metalloproteinases</td>\n",
       "      <td>The comparative role of <strong>[TF]</strong> and Smad factors in the regulation of Timp-1 and MMP-1 gene expression by transforming growth factor-beta 1.<br> The balance between matrix metalloproteinases (MMPs) and their inhibitors, the <strong>[TG]</strong> (TIMPs), is pivotal in the remodeling of extracellular matrix.<br></td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4133</th>\n",
       "      <td>12525489:0:activator protein 1:TIMPs</td>\n",
       "      <td>The comparative role of <strong>[TF]</strong> and Smad factors in the regulation of Timp-1 and MMP-1 gene expression by transforming growth factor-beta 1.<br> The balance between matrix metalloproteinases (MMPs) and their inhibitors, the tissue inhibitors of metalloproteinases (<strong>[TG]</strong>), is pivotal in the remodeling of extracellular matrix.<br></td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "26 rows (0.12%) have [TF] and [TG] in the same sentence. The correct sentence is kept.<br>Example:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#TRI ID</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>1655749:0:ATF-1:cAMP-dependent protein kinase A</td>\n",
       "      <td>The cAMP-regulated enhancer-binding protein <strong>[TF]</strong> activates transcription in response to <strong>[TG]</strong>.<br> Many promoters respond transcriptionally to elevated levels of cAMP through the cAMP-responsive enhancer (CRE).<br></td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128</th>\n",
       "      <td>12525489:0:activator protein 1:Smad</td>\n",
       "      <td>The comparative role of <strong>[TF]</strong> and <strong>[TG]</strong> factors in the regulation of Timp-1 and MMP-1 gene expression by transforming growth factor-beta 1.<br> The balance between matrix metalloproteinases (MMPs) and their inhibitors, the tissue inhibitors of metalloproteinases (TIMPs), is pivotal in the remodeling of extracellular matrix.<br></td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_sci_md\")\n",
    "\n",
    "def merge_sentences(doc):\n",
    "    merged_sentences = []\n",
    "    temp_sentence = \"\"\n",
    "\n",
    "    sentences = [i for i in doc.sents]  # Convert generator to a list for easier handling\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        current_text = sentence.text.strip()\n",
    "        temp_sentence += current_text + \" \"\n",
    "\n",
    "        # Separate to a new sentence if: \n",
    "        #     it is the last sentence, or\n",
    "        #     The sentence ends with \". \" and next sentence starts with uppercase\n",
    "        if i == len(sentences) - 1 or ( current_text.endswith(('.', '!', '?')) \n",
    "                                       and sentences[i + 1].text.strip()[0].isupper()\n",
    "                                       and sentences[i + 1].start_char == sentence.end_char + 1\n",
    "                                      ):\n",
    "            merged_sentences.append(temp_sentence.strip())\n",
    "            temp_sentence = \"\"\n",
    "    return merged_sentences\n",
    "\n",
    "\n",
    "# Use spaCy's pipe for efficient batch processing\n",
    "sentences = original_data['Sentence'].tolist()  # Convert column to list for efficient processing\n",
    "%time docs = list(nlp.pipe(sentences))\n",
    "\n",
    "# Get list of IDs to keep and to discard:\n",
    "discard_ids = []\n",
    "keep_ids = {}\n",
    "for doc, id in zip(docs, original_data['#TRI ID']):\n",
    "    merged_sents = merge_sentences(doc)\n",
    "    if len(merged_sents) > 1:\n",
    "        for sentence in merged_sents:\n",
    "            if '[TF]' in sentence:\n",
    "                if '[TG]' in sentence:\n",
    "                    # TF and TG are together. Keep the sentence\n",
    "                    keep_ids[id] = sentence\n",
    "                    pass\n",
    "                else:\n",
    "                    # TF and TG are in separate sentences. Discard.\n",
    "                    discard_ids.append(id)\n",
    "\n",
    "# Show examples of discarded rows:\n",
    "md(f\"{len(discard_ids)} rows ({len(discard_ids) / len(original_data):.2%}) have [TF] and [TG] in separate sentences. They are discarded.<br>Example:\")\n",
    "m = original_data['#TRI ID'].isin(discard_ids)\n",
    "df = original_data[m][['#TRI ID', 'Sentence', 'Label']]\n",
    "df['Sentence'] = df['Sentence'].apply(lambda x: highlight_words(x, ['[TF]', '[TG]']))\n",
    "display(HTML(f'{df[:2].to_html(escape=False)}'))                                    \n",
    "\n",
    "md(f'{len(keep_ids)} rows ({len(keep_ids) / len(original_data):.2%}) have [TF] and [TG] in the same sentence. The correct sentence is kept.<br>Example:')\n",
    "m = original_data['#TRI ID'].isin(keep_ids)\n",
    "df = original_data[m][['#TRI ID', 'Sentence', 'Label']]\n",
    "df['Sentence'] = df['Sentence'].apply(lambda x: highlight_words(x, ['[TF]', '[TG]']))\n",
    "display(HTML(f'{df[:2].to_html(escape=False)}'))      \n",
    "\n",
    "# Save the ids of the splitted sentences to keep to be reannotated:\n",
    "with open(splitted_sentences_path, 'w') as f:\n",
    "    f.writelines([id+'\\n' for id in keep_ids.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6cd15cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We discard 54 rows because they have been split\n"
     ]
    }
   ],
   "source": [
    "# DROP SPLIT SENTENCES\n",
    "print(f\"We discard {len(discard_ids) + len(keep_ids)} rows because they have been split\")\n",
    "original_data = original_data[~original_data['#TRI ID'].isin(discard_ids)]\n",
    "\n",
    "original_data = original_data[~original_data['#TRI ID'].isin(keep_ids.keys())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8714d624",
   "metadata": {},
   "source": [
    "## Reannotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "44d2fef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4>ACTIVATION & REPRESSION</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "There are 5 sentences that have been noted as <b>both ACTIVATION and REPRESSION</b><br>\n",
       "Ideally, they should have a BOTH label, but as we don't have enough data, we'll convert them into UNDEFINED.<br>\n",
       "Resulting rows:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#TRI ID</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>MoR</th>\n",
       "      <th>Valid?</th>\n",
       "      <th>true_label</th>\n",
       "      <th>true_MoR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>8994186:6:NR4A1:POMC</td>\n",
       "      <td>Finally, we provide evidence that the nurr1/[TF] response sequence is pivotal to both nurr1/nur77-dependent positive regulation and glucocorticoid receptor-dependent negative regulation of the [TG] gene.</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>ACTIVATION</td>\n",
       "      <td>F</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>UNDEFINED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>9514889:7:NFKB:PTGS2</td>\n",
       "      <td>These results suggest that the [TF] site is involved in both the LPS-induced expression of the [TG] gene and its suppression by DEX and herbimycin A in a differentiation-dependent manner.</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>ACTIVATION</td>\n",
       "      <td>F</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>UNDEFINED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>9256061:7:TFEC:HMOX1</td>\n",
       "      <td>By transient coexpression assays, we showed that [TF] is able to activate or inhibit transcription of a reporter gene linked to either the tyrosinase or the [TG] gene promoter, depending on cell types.</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>UNDEFINED</td>\n",
       "      <td>T</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>UNDEFINED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>9819390:0:TLX1:ALDH1A1</td>\n",
       "      <td>The T-cell oncogenic protein [TF] activates [TG] expression in NIH 3T3 cells but represses its expression in mouse spleen development.</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>UNDEFINED</td>\n",
       "      <td>T</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>UNDEFINED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>15660126:4:NFKB:SLC1A2</td>\n",
       "      <td>Herein, we demonstrate that both TNFalpha-mediated repression and EGF-mediated activation of [TG] expression require [TF].</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>UNDEFINED</td>\n",
       "      <td>T</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>UNDEFINED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     #TRI ID  \\\n",
       "1578    8994186:6:NR4A1:POMC   \n",
       "1581    9514889:7:NFKB:PTGS2   \n",
       "1915    9256061:7:TFEC:HMOX1   \n",
       "1918  9819390:0:TLX1:ALDH1A1   \n",
       "1941  15660126:4:NFKB:SLC1A2   \n",
       "\n",
       "                                                                                                                                                                                                         Sentence  \\\n",
       "1578  Finally, we provide evidence that the nurr1/[TF] response sequence is pivotal to both nurr1/nur77-dependent positive regulation and glucocorticoid receptor-dependent negative regulation of the [TG] gene.   \n",
       "1581                  These results suggest that the [TF] site is involved in both the LPS-induced expression of the [TG] gene and its suppression by DEX and herbimycin A in a differentiation-dependent manner.   \n",
       "1915    By transient coexpression assays, we showed that [TF] is able to activate or inhibit transcription of a reporter gene linked to either the tyrosinase or the [TG] gene promoter, depending on cell types.   \n",
       "1918                                                                       The T-cell oncogenic protein [TF] activates [TG] expression in NIH 3T3 cells but represses its expression in mouse spleen development.   \n",
       "1941                                                                                   Herein, we demonstrate that both TNFalpha-mediated repression and EGF-mediated activation of [TG] expression require [TF].   \n",
       "\n",
       "     Label         MoR Valid? true_label   true_MoR  \n",
       "1578  TRUE  ACTIVATION      F       TRUE  UNDEFINED  \n",
       "1581  TRUE  ACTIVATION      F       TRUE  UNDEFINED  \n",
       "1915  TRUE   UNDEFINED      T       TRUE  UNDEFINED  \n",
       "1918  TRUE   UNDEFINED      T       TRUE  UNDEFINED  \n",
       "1941  TRUE   UNDEFINED      T       TRUE  UNDEFINED  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Suboptimal</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "There are 160 sentences that are suboptimal and will be converted to 'False' for training"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PREPROCESS ITER DATASETS\n",
    "\n",
    "# Remove empty rows & columns\n",
    "iter_1_2.dropna(how='all', inplace=True)\n",
    "iter_3.dropna(subset = iter_3.columns.difference(['Problem with:']), how='all', inplace=True)\n",
    "iter_3.drop(columns=['Unnamed: 17'], inplace=True)\n",
    "\n",
    "## Fix columns/cells\n",
    "# If 'Iter' is neither 1 nor 2, it belongs to the Splitter.\n",
    "iter_1_2.loc[iter_1_2['Iter'].isna(), 'Iter'] = 'Spliter'\n",
    "iter_3['Iter'] = '3'\n",
    "# These specific cells were incorrectly annotated\n",
    "m = iter_1_2['#TRI ID'] == '24276245:6:E2F1:ABCG2'\n",
    "iter_1_2.loc[m, 'true_MoR'] = 'UNDEFINED'\n",
    "m = iter_1_2['#TRI ID'] == '21813016:5:CTCF:CDKN1A'\n",
    "iter_1_2.loc[m, 'true_label'] = 'TRUE'\n",
    "\n",
    "## Join the 'Problem_with' in one only row\n",
    "columns_to_join = ['Problem with:', 'Unnamed: 10']\n",
    "iter_1_2['Problem_with'] = iter_1_2.apply(lambda row: '|'.join(sorted(row[col] for col in columns_to_join if not pd.isna(row[col]))), axis=1)\n",
    "iter_1_2.drop(columns=columns_to_join, inplace=True)\n",
    "\n",
    "columns_to_join = ['Problem with:', 'Unnamed: 11', 'Suboptimal?']\n",
    "iter_3['Problem_with'] = iter_3.apply(lambda row: '|'.join(sorted([row[col] for col in columns_to_join if not pd.isna(row[col])])), axis=1)\n",
    "iter_3.drop(columns=columns_to_join, inplace=True)\n",
    "\n",
    "## Remove duplicates (annotated both in Iteration 1 and 2)\n",
    "iter_1_2.drop_duplicates(subset=['#TRI ID', 'Sentence', 'Label', 'MoR'], keep='first', inplace=True)\n",
    "\n",
    "## Convert 'both activation & repression' into UNDEFINED\n",
    "m = iter_1_2['Explanation'] == 'BOTH activation AND repression'\n",
    "h4('ACTIVATION & REPRESSION')\n",
    "md(f'''There are {m.sum()} sentences that have been noted as <b>both ACTIVATION and REPRESSION</b><br>\n",
    "Ideally, they should have a BOTH label, but as we don't have enough data, we'll convert them into UNDEFINED.<br>\n",
    "Resulting rows:''')\n",
    "iter_1_2.loc[m, 'Valid?'] = iter_1_2.loc[m, 'MoR'].apply(lambda MoR: 'T' if MoR == 'UNDEFINED' else 'F')\n",
    "iter_1_2.loc[m, 'true_label'] = 'TRUE'\n",
    "iter_1_2.loc[m, 'true_MoR'] = 'UNDEFINED'\n",
    "display(iter_1_2[m][['#TRI ID', 'Sentence', 'Label', 'MoR', 'Valid?', 'true_label', 'true_MoR']])\n",
    "\n",
    "# Join iter 1,2,3\n",
    "validated_sents = pd.concat([iter_1_2, iter_3], axis=0)\n",
    "# Drop duplicates between iters 2 & 3 (keep Iteration 3)\n",
    "validated_sents = validated_sents.sort_values(by='Iter').drop_duplicates(subset=['#TRI ID', 'Sentence', 'Label', 'MoR'], keep='last')\n",
    "\n",
    "# Remove validated rows that have been split\n",
    "validated_sents = validated_sents[~validated_sents['#TRI ID'].isin(keep_ids.keys())]\n",
    "\n",
    "# SUBOPTIMAL SENTENCES\n",
    "h4(\"Suboptimal\")\n",
    "# Suboptimal sentences are only modified for the training dataset, not the NTNU dataset.\n",
    "# Create a copy of the validated sentences for the NTNU\n",
    "validated_sents_for_NTNU = validated_sents.copy()\n",
    "\n",
    "# For the training dataset, modify the suboptimal sentences to \"Label\" = \"False\"\n",
    "m_suboptimal = validated_sents['Problem_with'].str.contains('Suboptimal')\n",
    "md(f\"There are {m_suboptimal.sum()} sentences that are suboptimal and will be converted to 'False' for training\")\n",
    "\n",
    "validated_sents.loc[m_suboptimal, 'Valid?'] = np.where(validated_sents.loc[m_suboptimal, 'Label'] == 'FALSE', 'T', 'F')\n",
    "validated_sents.loc[m_suboptimal, 'true_label'] = 'FALSE'\n",
    "validated_sents.loc[m_suboptimal, 'true_MoR'] = np.nan\n",
    "\n",
    "\n",
    "## Assertions\n",
    "assert validated_sents.duplicated(subset=['#TRI ID']).sum() == 0\n",
    "assert all(iter_3.columns == iter_1_2.columns), 'Iter 1_2 and Iter_3 have different columns'\n",
    "assert validated_sents['Valid?'].isna().sum() == 0, 'Some rows have no Valid? value. Check the data.'\n",
    "assert ((validated_sents['Valid?'] == 'F') & (validated_sents['true_label'].isna())).sum() == 0\n",
    "assert ((validated_sents['true_label'] == 'TRUE') & (validated_sents['true_MoR'].isna())).sum() == 0\n",
    "assert ((validated_sents['true_label'] == 'FALSE') & ~(validated_sents['true_MoR'].isna())).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3a8dea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE TRAINING DATASET\n",
    "\n",
    "def update_training_dataset(original_data, validated_sents):\n",
    "    '''\n",
    "    Update the training dataset with validated sentences.\n",
    "    '''\n",
    "\n",
    "    updated_data = original_data.copy()\n",
    "\n",
    "    # Get data to update in the same format as the training dataset\n",
    "    m = validated_sents['Valid?'] == 'F'\n",
    "    to_update = validated_sents[m]\n",
    "    to_update.loc[:, 'true_label'] = to_update['true_label'].replace({'FALSE': 'false', 'TRUE': 'true'})\n",
    "\n",
    "    # Create mappings\n",
    "    label_map = to_update.set_index('#TRI ID')['true_label']\n",
    "    MoR_map   = to_update.set_index('#TRI ID')['true_MoR']\n",
    "\n",
    "    # Apply mappings to update 'Label' and 'Type' where '#TRI ID' matches\n",
    "    mask = updated_data['#TRI ID'].isin(to_update['#TRI ID'])\n",
    "    updated_data.loc[mask, 'Label'] = updated_data.loc[mask, '#TRI ID'].map(label_map)\n",
    "    updated_data.loc[mask, 'Type']  = updated_data.loc[mask, '#TRI ID'].map(MoR_map)\n",
    "\n",
    "    # Create column to keep track of validated rows\n",
    "    updated_data['validated?'] = updated_data['#TRI ID'].isin(validated_sents['#TRI ID'])\n",
    "\n",
    "    return updated_data\n",
    "\n",
    "# Update training data\n",
    "training_data = update_training_dataset(original_data, validated_sents)\n",
    "\n",
    "# Update the NTNU dataset\n",
    "NTNU_dataset = update_training_dataset(original_data, validated_sents_for_NTNU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "6804fdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We discard 43 sentences as duplicates, 14 sentences where labels differ, and 14 unvalid sentences where the valid version has different labels.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#TRI ID</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>TF</th>\n",
       "      <th>TG</th>\n",
       "      <th>Label</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16373364:0:Ets1:GATA-3</td>\n",
       "      <td>A role for Ets1, synergizing with AP-1 and GATA-3 in the regulation of IL-5 transcription in mouse Th2 lymphocytes.</td>\n",
       "      <td>Ets1</td>\n",
       "      <td>GATA-3</td>\n",
       "      <td>false</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16373364:0:Ets1:IL-5</td>\n",
       "      <td>A role for Ets1, synergizing with AP-1 and GATA-3 in the regulation of IL-5 transcription in mouse Th2 lymphocytes.</td>\n",
       "      <td>Ets1</td>\n",
       "      <td>IL-5</td>\n",
       "      <td>true</td>\n",
       "      <td>UNDEFINED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16373364:0:Ets1:AP-1</td>\n",
       "      <td>A role for Ets1, synergizing with AP-1 and GATA-3 in the regulation of IL-5 transcription in mouse Th2 lymphocytes.</td>\n",
       "      <td>Ets1</td>\n",
       "      <td>AP-1</td>\n",
       "      <td>false</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16373364:0:GATA-3:Ets1</td>\n",
       "      <td>A role for Ets1, synergizing with AP-1 and GATA-3 in the regulation of IL-5 transcription in mouse Th2 lymphocytes.</td>\n",
       "      <td>GATA-3</td>\n",
       "      <td>Ets1</td>\n",
       "      <td>false</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16373364:0:GATA-3:IL-5</td>\n",
       "      <td>A role for Ets1, synergizing with AP-1 and GATA-3 in the regulation of IL-5 transcription in mouse Th2 lymphocytes.</td>\n",
       "      <td>GATA-3</td>\n",
       "      <td>IL-5</td>\n",
       "      <td>true</td>\n",
       "      <td>UNDEFINED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  #TRI ID  \\\n",
       "0  16373364:0:Ets1:GATA-3   \n",
       "1    16373364:0:Ets1:IL-5   \n",
       "2    16373364:0:Ets1:AP-1   \n",
       "3  16373364:0:GATA-3:Ets1   \n",
       "4  16373364:0:GATA-3:IL-5   \n",
       "\n",
       "                                                                                                              Sentence  \\\n",
       "0  A role for Ets1, synergizing with AP-1 and GATA-3 in the regulation of IL-5 transcription in mouse Th2 lymphocytes.   \n",
       "1  A role for Ets1, synergizing with AP-1 and GATA-3 in the regulation of IL-5 transcription in mouse Th2 lymphocytes.   \n",
       "2  A role for Ets1, synergizing with AP-1 and GATA-3 in the regulation of IL-5 transcription in mouse Th2 lymphocytes.   \n",
       "3  A role for Ets1, synergizing with AP-1 and GATA-3 in the regulation of IL-5 transcription in mouse Th2 lymphocytes.   \n",
       "4  A role for Ets1, synergizing with AP-1 and GATA-3 in the regulation of IL-5 transcription in mouse Th2 lymphocytes.   \n",
       "\n",
       "       TF      TG  Label       Type  \n",
       "0    Ets1  GATA-3  false        NaN  \n",
       "1    Ets1    IL-5   true  UNDEFINED  \n",
       "2    Ets1    AP-1  false        NaN  \n",
       "3  GATA-3    Ets1  false        NaN  \n",
       "4  GATA-3    IL-5   true  UNDEFINED  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CREATE THE NTNU DATASET\n",
    "\n",
    "# Return to the original sentence\n",
    "NTNU_dataset['Sentence'] = NTNU_dataset.apply(lambda row: row['Sentence'].replace('[TF]', row['TF']), axis=1)\n",
    "NTNU_dataset['Sentence'] = NTNU_dataset.apply(lambda row: row['Sentence'].replace('[TG]', row['TG']), axis=1)\n",
    "\n",
    "# When the mask is removed, there are some duplicated sentences. They are handled here:\n",
    "\n",
    "# 1) Discard complete duplicates\n",
    "m1 = NTNU_dataset[['Sentence', 'TF', 'TG', 'Label', 'Type']].duplicated()\n",
    "NTNU_dataset = NTNU_dataset[~m1]\n",
    "\n",
    "# 2) If both are validated or unvalidated, discard\n",
    "m2 = NTNU_dataset[['Sentence', 'TF', 'TG', 'validated?']].duplicated(keep=False)\n",
    "NTNU_dataset = NTNU_dataset[~m2]\n",
    "\n",
    "\n",
    "# 3) If Labels differ, keep the validated version\n",
    "m3 = NTNU_dataset[['Sentence', 'TF', 'TG']].duplicated(keep=False) & ~NTNU_dataset['validated?']\n",
    "NTNU_dataset = NTNU_dataset[~m3]\n",
    "\n",
    "# Discard 'validated?' column\n",
    "NTNU_dataset = NTNU_dataset.drop(columns=['validated?'])\n",
    "\n",
    "print(f\"We discard {m1.sum()} sentences as duplicates, {m2.sum()} sentences where labels differ, and {m3.sum()} unvalid sentences where the valid version has different labels.\")\n",
    "NTNU_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "be46c955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2024/3086 (66%) reannotated sentences have changed their labels"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Issues due to TRI label"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Valid?  Label  true_label\n",
       "F       FALSE  TRUE          1067\n",
       "        TRUE   TRUE           485\n",
       "               FALSE          472\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Erroneous sentences due to suboptimal TRI"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Valid?  Label  true_label\n",
       "F       TRUE   FALSE         149\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Other problems encountered:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            All data        Non valid rows \n",
      "Splitter    11              0              \n",
      "PPI         19              0              \n",
      "dir-gene    27              0              \n",
      "dir-syntax  3050            149            \n",
      "Suboptimal  160             149            \n",
      "negation    60              0              \n"
     ]
    }
   ],
   "source": [
    "# SHOW STATISTICS\n",
    "\n",
    "# Sentences reannotated\n",
    "m = validated_sents['Valid?'] == 'F'\n",
    "md(f'{m.sum()}/{len(m)} ({m.sum() / len(m):.0%}) reannotated sentences have changed their labels')\n",
    "\n",
    "# TRI - Non-TRI issues\n",
    "md(\"Issues due to TRI label\")\n",
    "m = validated_sents['Valid?'] == 'F'\n",
    "display(validated_sents[m][[\"Valid?\", \"Label\", \"true_label\"]].value_counts(dropna=False))\n",
    "\n",
    "md(\"Erroneous sentences due to suboptimal TRI\")\n",
    "m = validated_sents['Problem_with'].str.contains('Suboptimal') & (validated_sents['Valid?'] == 'F')\n",
    "display(validated_sents[m][[\"Valid?\", \"Label\", \"true_label\"]].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "md('Other problems encountered:')\n",
    "problems = set(p for unique in validated_sents['Problem_with'].unique() for p in unique.split('|'))\n",
    "problems.discard('')\n",
    "print(f\"{'':<11} {'All data':<15} {'Non valid rows':<15}\")\n",
    "for problem in problems:\n",
    "    in_all_data  = validated_sents['Problem_with'].str.contains(problem).sum()\n",
    "    in_incorrect = validated_sents[m]['Problem_with'].str.contains(problem).sum()\n",
    "    print(f\"{problem:<11} {in_all_data:<15} {in_incorrect:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a1a608",
   "metadata": {},
   "source": [
    "## Save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8d1b31df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure data has expected values\n",
    "assert set(training_data['Label']).issubset({'false', 'true'})\n",
    "assert set(training_data['Type']).issubset({np.nan, 'UNDEFINED', 'ACTIVATION', 'REPRESSION'})\n",
    "assert ((training_data['Label'] == 'False') & ~(training_data['Type'].isna())).sum() == 0\n",
    "\n",
    "# Convert it to False and True\n",
    "training_data['Label'] = training_data['Label'].map({'false': False, 'true': True})\n",
    "NTNU_dataset['Label'] = NTNU_dataset['Label'].map({'false': 'Not TRI', 'true': 'TRI'})\n",
    "\n",
    "# Save the datasets\n",
    "training_data.to_csv(training_data_path, sep='\\t')\n",
    "NTNU_dataset.to_csv(NTNU_dataset_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "26076f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "NTNU dataset, 21501 sentences"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Label    Type      \n",
       "Not TRI  NaN           9980\n",
       "TRI      ACTIVATION    5395\n",
       "         UNDEFINED     4073\n",
       "         REPRESSION    2053\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Training data, 21572 sentences"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Label  Type      \n",
       "False  NaN           10169\n",
       "True   ACTIVATION     5344\n",
       "       UNDEFINED      4044\n",
       "       REPRESSION     2015\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "md(f\"NTNU dataset, {len(NTNU_dataset)} sentences\")\n",
    "display(NTNU_dataset[['Label', 'Type']].value_counts(dropna=False))\n",
    "\n",
    "md(f\"Training data, {len(training_data)} sentences\")\n",
    "display(training_data[['Label', 'Type']].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84951ad3",
   "metadata": {},
   "source": [
    "## Old code - Add negative sentences\n",
    "We tried to add negative sentences to the dataset in an attempt for the classifier to flag them as negative. It did not give good results, so we abandoned the idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de13c9ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4>Negations</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "There are 60 sentences that are negations, considered as 'True'"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NEGATIONS - Only mentioned, kept as positive\n",
    "h4('Negations')\n",
    "m_negations =  validated_sents['Problem_with'].str.contains('negation')\n",
    "md(f\"There are {m_negations.sum()} sentences that are negations, considered as 'True'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fe1f1b",
   "metadata": {},
   "source": [
    "### Enhance with negations from the extended NTNU dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf720de4",
   "metadata": {},
   "source": [
    "`original_tri_sentences.tsv` is a refined subset of the NTNU dataset, a set of 40K validated sentences from ExTRI. Between other things, it did not include sentences marked as 'negations'. Without those, the model doesn't learn that sentences such as:\n",
    "> TF has been observed to have no effect on TG\n",
    "Are marked as 'Valid' by the model. \n",
    "\n",
    "Thus, some of the negation sentences from the original NTNU dataset are recovered to enhance training data with negations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c473683f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#TRI</th>\n",
       "      <th>Valid</th>\n",
       "      <th>Sign</th>\n",
       "      <th>Negation</th>\n",
       "      <th>QC</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Source file</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18728219:2:ARNTL:CRY1</td>\n",
       "      <td>true</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>true</td>\n",
       "      <td>---{\"transcription factor (associated gene name)\":[\"ARNTL\"],\"target gene (associated gene name)\":[\"CRY1\"],\"valid\":[\"TRUE\"],\"sign\":[],\"final_negation_220621\":[],\"qc\":[\"TRUE\"],\"sentence\":[\"CLOCK and ARNTL are transcriptional activators that regulate Per and Cry gene expression.\"],\"Valid\":[true],\"Sign\":[null],\"Negation\":[null],\"QC\":[true]}---</td>\n",
       "      <td>AGS_24h_transcriptome_QCed_MANUAL_curation_sept21.tsv</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19605937:1:ARNTL:CRY1</td>\n",
       "      <td>true</td>\n",
       "      <td>+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>true</td>\n",
       "      <td>---{\"transcription factor (associated gene name)\":[\"ARNTL\"],\"target gene (associated gene name)\":[\"CRY1\"],\"valid\":[\"TRUE\"],\"sign\":[\"a\"],\"final_negation_220621\":[],\"qc\":[\"TRUE\"],\"sentence\":[\"\\\"In the molecular oscillatory mechanism governing circadian rhythms, positive regulators, including CLOCK and BMAL1, transactivate Per and Cry genes through E-box elements, and translated PER and CRY proteins negatively regulate their own transactivation.\\\"\"],\"Valid\":[true],\"Sign\":[\"+\"],\"Negation\":[null],\"QC\":[true]}---</td>\n",
       "      <td>AGS_24h_transcriptome_QCed_MANUAL_curation_sept21.tsv</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    #TRI Valid Sign Negation    QC  \\\n",
       "0  18728219:2:ARNTL:CRY1  true  NaN      NaN  true   \n",
       "1  19605937:1:ARNTL:CRY1  true    +      NaN  true   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Comment  \\\n",
       "0                                                                                                                                                                             ---{\"transcription factor (associated gene name)\":[\"ARNTL\"],\"target gene (associated gene name)\":[\"CRY1\"],\"valid\":[\"TRUE\"],\"sign\":[],\"final_negation_220621\":[],\"qc\":[\"TRUE\"],\"sentence\":[\"CLOCK and ARNTL are transcriptional activators that regulate Per and Cry gene expression.\"],\"Valid\":[true],\"Sign\":[null],\"Negation\":[null],\"QC\":[true]}---   \n",
       "1  ---{\"transcription factor (associated gene name)\":[\"ARNTL\"],\"target gene (associated gene name)\":[\"CRY1\"],\"valid\":[\"TRUE\"],\"sign\":[\"a\"],\"final_negation_220621\":[],\"qc\":[\"TRUE\"],\"sentence\":[\"\\\"In the molecular oscillatory mechanism governing circadian rhythms, positive regulators, including CLOCK and BMAL1, transactivate Per and Cry genes through E-box elements, and translated PER and CRY proteins negatively regulate their own transactivation.\\\"\"],\"Valid\":[true],\"Sign\":[\"+\"],\"Negation\":[null],\"QC\":[true]}---   \n",
       "\n",
       "                                             Source file Sentence  \n",
       "0  AGS_24h_transcriptome_QCed_MANUAL_curation_sept21.tsv      NaN  \n",
       "1  AGS_24h_transcriptome_QCed_MANUAL_curation_sept21.tsv      NaN  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 終 TODO - I will remove it from here, but there's some NTNU analysis in the one in ExTRI_classifiers\n",
    "NTNU_extended[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23338d94",
   "metadata": {},
   "source": [
    "The NTNU extended dataset did not keep the original TF/TG mentions in the text (only their normalizations). Only those sentences that are marked as negations, and where the exact mention of the normalized TF and TG are present in the text, are considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d26da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 496 sentences marked as negations.\n",
      "From those, both normalized TF&TG are present in 28 rows.\n"
     ]
    }
   ],
   "source": [
    "# Extract negations from NTNU\n",
    "m = (NTNU_extended['Negation'] == 'true') & ~NTNU_extended['Sentence'].isna()\n",
    "NTNU_negations = NTNU_extended[m].copy()\n",
    "NTNU_negations['TF'] = NTNU_negations['#TRI'].str.split(':').str[2]\n",
    "NTNU_negations['TG'] = NTNU_negations['#TRI'].str.split(':').str[3]\n",
    "\n",
    "m = NTNU_negations.apply(lambda row: (row['TF'] in row['Sentence']) & (row['TG'] in row['Sentence']), axis=1)\n",
    "\n",
    "print(f\"There are {len(NTNU_negations)} sentences marked as negations.\")\n",
    "print(f\"From those, both normalized TF&TG are present in {m.sum()} rows.\")\n",
    "#NTNU_negations[m].to_csv('negations.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b980cc58",
   "metadata": {},
   "source": [
    "In most cases, the sentence contains more than 1 mention of the TF or TG, making the process of swapping the TF/TG for their `[TF]` and `[TG]` tokens difficult to automate. It has been done manually, and saved in `negations_NTNU.tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba56f724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the negations in the format of the updated data\n",
    "negations_to_add = negations_to_add[['#TRI', 'Sentence', 'TF', 'TG']]\n",
    "negations_to_add.rename(columns={\"#TRI\": \"#TRI ID\"}, inplace=True)\n",
    "negations_to_add['Label'] = 'false'\n",
    "negations_to_add['Type'] = np.nan\n",
    "negations_to_add['validated?'] = True\n",
    "\n",
    "# Combine it with the updated data\n",
    "training_data = pd.concat([training_data, negations_to_add], axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".general_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
