{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df4c0c1a-3b10-451f-8884-d231ec89b734",
   "metadata": {},
   "source": [
    "# Update raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302e995d-7420-47a1-a0e8-91cc46c6fbfc",
   "metadata": {},
   "source": [
    "modify `original_tri_sentences.tsv` to obtain `tri_sentences.tsv` with the updated annotations, including:\n",
    "* The re-annotations\n",
    "* Negation sentences from the extended NTNU dataset\n",
    "\n",
    "After running this notebook, `make_train_data.ipynb` must also be run, which converts the created `tri_sentences.tsv` into the datasets used as input for training the TRI and MoR classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5a66afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3>Table of contents</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "[Update raw data](#Update-raw-data)\n",
       "- [Setup](#Setup)\n",
       "- [Clean the dataset](#Clean-the-dataset)\n",
       "  - [Modify joined sentences](#Modify-joined-sentences)\n",
       "- [Reannotations](#Reannotations)\n",
       "- [Enhance with negations from the extended NTNU dataset](#Enhance-with-negations-from-the-extended-NTNU-dataset)\n",
       "- [Save the dataset](#Save-the-dataset)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "__import__('sys').path.append('../common/'); __import__('notebook_utils').table_of_contents('update_tri_sentences.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd45d45",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1b82aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from IPython.display import display, HTML, display_html\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# My functions\n",
    "import sys\n",
    "sys.path.append('../common/') \n",
    "from analysis_utils import prettify_plots\n",
    "from notebook_utils import table_from_dict, md, h3, h4, highlight_words\n",
    "\n",
    "prettify_plots()\n",
    "\n",
    "# %pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "49e4c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "# Inputs\n",
    "data_path = '../../data/external/'\n",
    "original_data_path = data_path + 'original_tri_sentences.tsv'\n",
    "NTNU_extended_path = data_path + 'NTNU_extended.tsv'\n",
    "\n",
    "\n",
    "dataset_improvement_path = '../../data/dataset_improvement/'\n",
    "iter_1_2_path      = dataset_improvement_path + 'iter1_iter2_worst_predictions_validated.txt'\n",
    "iter_3_path        = dataset_improvement_path + 'iter3_worst_preds_AL_ver2.txt'\n",
    "negations_to_add_path = dataset_improvement_path + 'negations_NTNU.tsv'\n",
    "\n",
    "# Load datasets\n",
    "original_data   = pd.read_csv(original_data_path, sep='\\t', header=1, index_col=None, dtype='str')\n",
    "iter_1_2        = pd.read_csv(iter_1_2_path, sep='\\t', header=0, index_col=None, dtype='str')\n",
    "iter_3          = pd.read_csv(iter_3_path, sep='\\t', header=0, index_col=None, dtype='str')\n",
    "NTNU_extended   = pd.read_csv(NTNU_extended_path, sep='\\t', header=1, index_col=None, dtype='str')\n",
    "negations_to_add   = pd.read_csv(negations_to_add_path, sep='\\t', header=0, index_col=None, dtype='str')\n",
    "\n",
    "\n",
    "# Outputs\n",
    "splitted_sentences_path = dataset_improvement_path + 'to_reannotate/splitted_sentences_ids.txt'\n",
    "\n",
    "classifiers_data_path = '../../classifiers_training/data/'\n",
    "updated_data_path  = classifiers_data_path + 'tri_sentences.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c12411",
   "metadata": {},
   "source": [
    "## Clean the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b79f86",
   "metadata": {},
   "source": [
    "There are a few duplicated sentences: same PMID and sentence number, but different TF or TG ID. Some also differ in their TRI/MoR annotations. They will be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6d0f85b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Information on duplicated rows</h4><table><tr><td><b>Total number of sentences</b></td><td>22135</td></tr><tr><td><b>Unique sentences</b></td><td>21763</td></tr><tr><td><b>Duplicated sentences</b></td><td>735 (3.3%)</td></tr><tr><td><b>.. where TRI or MoR differs</b></td><td>278 (1.3%)</td></tr><tr><td><b>Unique duplicated sentences</b></td><td>363</td></tr><tr><td><b>.. where TRI or MOR differs</b></td><td>137</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Two sets of duplicated sentences where either Label or Type differ:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#TRI ID</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>TF</th>\n",
       "      <th>TG</th>\n",
       "      <th>Label</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20620</th>\n",
       "      <td>21056029:3:NFKB:CCND1</td>\n",
       "      <td>-catenin activated transcription from the [TG]...</td>\n",
       "      <td>p65</td>\n",
       "      <td>cyclin D1</td>\n",
       "      <td>false</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10925</th>\n",
       "      <td>21056029:3:RELA:CCND1</td>\n",
       "      <td>-catenin activated transcription from the [TG]...</td>\n",
       "      <td>p65</td>\n",
       "      <td>cyclin D1</td>\n",
       "      <td>true</td>\n",
       "      <td>REPRESSION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12854</th>\n",
       "      <td>21720709:0:TCF7L2:AKT1</td>\n",
       "      <td>-catenin/[TF] complex transcriptionally regula...</td>\n",
       "      <td>Tcf-4</td>\n",
       "      <td>AKT1</td>\n",
       "      <td>true</td>\n",
       "      <td>UNDEFINED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19541</th>\n",
       "      <td>21720709:0:TCF4:AKT1</td>\n",
       "      <td>-catenin/[TF] complex transcriptionally regula...</td>\n",
       "      <td>Tcf-4</td>\n",
       "      <td>AKT1</td>\n",
       "      <td>false</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      #TRI ID  \\\n",
       "20620   21056029:3:NFKB:CCND1   \n",
       "10925   21056029:3:RELA:CCND1   \n",
       "12854  21720709:0:TCF7L2:AKT1   \n",
       "19541    21720709:0:TCF4:AKT1   \n",
       "\n",
       "                                                Sentence     TF         TG  \\\n",
       "20620  -catenin activated transcription from the [TG]...    p65  cyclin D1   \n",
       "10925  -catenin activated transcription from the [TG]...    p65  cyclin D1   \n",
       "12854  -catenin/[TF] complex transcriptionally regula...  Tcf-4       AKT1   \n",
       "19541  -catenin/[TF] complex transcriptionally regula...  Tcf-4       AKT1   \n",
       "\n",
       "       Label        Type  \n",
       "20620  false         NaN  \n",
       "10925   true  REPRESSION  \n",
       "12854   true   UNDEFINED  \n",
       "19541  false         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SHOW NUMBER OF DUPLICATE ROWS\n",
    "unique_rows = original_data['Sentence'].unique()\n",
    "duplicated_rows = original_data[original_data['Sentence'].duplicated(keep=False)]\n",
    "unique_duplicated_rows = duplicated_rows.drop_duplicates(subset='Sentence')\n",
    "duplicates_with_diff_label_or_type = original_data.groupby('Sentence').filter(lambda x: x['Label'].nunique() > 1 or x['Type'].nunique() > 1)\n",
    "\n",
    "table_from_dict('Information on duplicated rows', {\n",
    "    'Total number of sentences': len(original_data),\n",
    "    'Unique sentences': len(unique_rows),\n",
    "    'Duplicated sentences': f'{len(duplicated_rows)} ({round(len(duplicated_rows)/len(original_data)*100,1)}%)',\n",
    "    '.. where TRI or MoR differs': f'{len(duplicates_with_diff_label_or_type)} ({round(len(duplicates_with_diff_label_or_type)/len(original_data)*100,1)}%)',\n",
    "    'Unique duplicated sentences': len(unique_duplicated_rows),\n",
    "    '.. where TRI or MOR differs': len(duplicates_with_diff_label_or_type['Sentence'].unique()),\n",
    "}, heading='h4')\n",
    "\n",
    "md('Two sets of duplicated sentences where either Label or Type differ:')\n",
    "display(duplicates_with_diff_label_or_type.sort_values(by='Sentence').head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e352814c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "463 sentences are duplicated but have the same Label & MoR. Only the first is kept (234 are dropped)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "275 sentences are duplicated and differ on their Label or MoR. They're all dropped."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DROP DUPLICATES\n",
    "# Drop duplicates in which Sentence, TRI and MoR are the same\n",
    "m1 = original_data.duplicated(subset=['Sentence', 'Label', 'Type'], keep=False)\n",
    "m2 = original_data.duplicated(subset=['Sentence', 'Label', 'Type'], keep='first')\n",
    "md(f\"{m1.sum()} sentences are duplicated but have the same Label & MoR. Only the first is kept ({m2.sum()} are dropped).\")\n",
    "original_data = original_data.drop_duplicates(subset=['Sentence', 'Label', 'Type'], keep='first')\n",
    "\n",
    "# Drop rows where Sentence is duplicated, but Label and/or Type differ\n",
    "m = original_data.duplicated(subset=['Sentence'], keep=False)\n",
    "md(f\"{m.sum()} sentences are duplicated and differ on their Label or MoR. They're all dropped.\")\n",
    "original_data = original_data[~m]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aab1c40",
   "metadata": {},
   "source": [
    "### Modify joined sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870b2ef3",
   "metadata": {},
   "source": [
    "The splitter we will use for the pipeline is `en_core_sci_md`. We will now make sure that the dataset we use for training is separated according to their standards. \n",
    "\n",
    "As explained in `scripts/preprocessing/spacy_splitter_analysis.ipynb`, the spacy splitter makes some mistakes, namely separating a sentence in 2 sometimes when a \".\" is combined by a lowercase letter, or when there is no \".\". To avoid these cases, we use a rule-based method to re-merge those sentences, defined in the `merge_sentences()` function.\n",
    "\n",
    "We pass all sentences in the dataset through the splitter, obtaining a 0.3% of cases where the sentences were incorrectly splitted. They can be divided into 2 groups:\n",
    "* **[TF] and [TG] are in the same sentence:** Those sentences have been revised, and are included in the `iter_1_2` dataset. They will be updated.\n",
    "* **[TF] and [TG] are in separate sentences:** Those sentences will be dropped, as they are no longer valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076c1655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdt_wsl/git/ExTRI2_gits/ExTRI2/.general_env/lib/python3.10/site-packages/spacy/language.py:2195: FutureWarning: Possible set union at position 6328\n",
      "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 38s, sys: 1.58 s, total: 1min 39s\n",
      "Wall time: 1min 39s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "28 rows (0.13%) have [TF] and [TG] in separate sentences. They are discarded.<br>Example:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#TRI ID</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4132</th>\n",
       "      <td>12525489:0:activator protein 1:tissue inhibitors of metalloproteinases</td>\n",
       "      <td>The comparative role of <strong>[TF]</strong> and Smad factors in the regulation of Timp-1 and MMP-1 gene expression by transforming growth factor-beta 1.<br> The balance between matrix metalloproteinases (MMPs) and their inhibitors, the <strong>[TG]</strong> (TIMPs), is pivotal in the remodeling of extracellular matrix.<br></td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4133</th>\n",
       "      <td>12525489:0:activator protein 1:TIMPs</td>\n",
       "      <td>The comparative role of <strong>[TF]</strong> and Smad factors in the regulation of Timp-1 and MMP-1 gene expression by transforming growth factor-beta 1.<br> The balance between matrix metalloproteinases (MMPs) and their inhibitors, the tissue inhibitors of metalloproteinases (<strong>[TG]</strong>), is pivotal in the remodeling of extracellular matrix.<br></td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "26 rows (0.12%) have [TF] and [TG] in the same sentence. The correct sentence is kept.<br>Example:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#TRI ID</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>1655749:0:ATF-1:cAMP-dependent protein kinase A</td>\n",
       "      <td>The cAMP-regulated enhancer-binding protein <strong>[TF]</strong> activates transcription in response to <strong>[TG]</strong>.<br> Many promoters respond transcriptionally to elevated levels of cAMP through the cAMP-responsive enhancer (CRE).<br></td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128</th>\n",
       "      <td>12525489:0:activator protein 1:Smad</td>\n",
       "      <td>The comparative role of <strong>[TF]</strong> and <strong>[TG]</strong> factors in the regulation of Timp-1 and MMP-1 gene expression by transforming growth factor-beta 1.<br> The balance between matrix metalloproteinases (MMPs) and their inhibitors, the tissue inhibitors of metalloproteinases (TIMPs), is pivotal in the remodeling of extracellular matrix.<br></td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_sci_md\")\n",
    "\n",
    "def merge_sentences(doc):\n",
    "    merged_sentences = []\n",
    "    temp_sentence = \"\"\n",
    "\n",
    "    sentences = [i for i in doc.sents]  # Convert generator to a list for easier handling\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        current_text = sentence.text.strip()\n",
    "        temp_sentence += current_text + \" \"\n",
    "\n",
    "        # Separate to a new sentence if: \n",
    "        #     it is the last sentence, or\n",
    "        #     The sentence ends with \". \" and next sentence starts with uppercase\n",
    "        if i == len(sentences) - 1 or ( current_text.endswith(('.', '!', '?')) \n",
    "                                       and sentences[i + 1].text.strip()[0].isupper()\n",
    "                                       and sentences[i + 1].start_char == sentence.end_char + 1\n",
    "                                      ):\n",
    "            merged_sentences.append(temp_sentence.strip())\n",
    "            temp_sentence = \"\"\n",
    "    return merged_sentences\n",
    "\n",
    "\n",
    "# Use spaCy's pipe for efficient batch processing\n",
    "sentences = original_data['Sentence'].tolist()  # Convert column to list for efficient processing\n",
    "%time docs = list(nlp.pipe(sentences))\n",
    "\n",
    "# Get list of IDs to keep and to discard:\n",
    "discard_ids = []\n",
    "keep_ids = {}\n",
    "for doc, id in zip(docs, original_data['#TRI ID']):\n",
    "    merged_sents = merge_sentences(doc)\n",
    "    if len(merged_sents) > 1:\n",
    "        for sentence in merged_sents:\n",
    "            if '[TF]' in sentence:\n",
    "                if '[TG]' in sentence:\n",
    "                    # TF and TG are together. Keep the sentence\n",
    "                    keep_ids[id] = sentence\n",
    "                    pass\n",
    "                else:\n",
    "                    # TF and TG are in separate sentences. Discard.\n",
    "                    discard_ids.append(id)\n",
    "\n",
    "# Show examples of discarded rows:\n",
    "md(f\"{len(discard_ids)} rows ({len(discard_ids) / len(original_data):.2%}) have [TF] and [TG] in separate sentences. They are discarded.<br>Example:\")\n",
    "m = original_data['#TRI ID'].isin(discard_ids)\n",
    "df = original_data[m][['#TRI ID', 'Sentence', 'Label']]\n",
    "df['Sentence'] = df['Sentence'].apply(lambda x: highlight_words(x, ['[TF]', '[TG]']))\n",
    "display(HTML(f'{df[:2].to_html(escape=False)}'))                                    \n",
    "\n",
    "md(f'{len(keep_ids)} rows ({len(keep_ids) / len(original_data):.2%}) have [TF] and [TG] in the same sentence. The correct sentence is kept.<br>Example:')\n",
    "m = original_data['#TRI ID'].isin(keep_ids)\n",
    "df = original_data[m][['#TRI ID', 'Sentence', 'Label']]\n",
    "df['Sentence'] = df['Sentence'].apply(lambda x: highlight_words(x, ['[TF]', '[TG]']))\n",
    "display(HTML(f'{df[:2].to_html(escape=False)}'))      \n",
    "\n",
    "# Save the ids of the splitted sentences to keep to be reannotated:\n",
    "with open(splitted_sentences_path, 'w') as f:\n",
    "    f.writelines([id+'\\n' for id in keep_ids.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6cd15cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We discard 54 rows because they have been split\n"
     ]
    }
   ],
   "source": [
    "# DROP SPLIT SENTENCES\n",
    "print(f\"We discard {len(discard_ids) + len(keep_ids)}  () rows because they have been split\")\n",
    "original_data = original_data[~original_data['#TRI ID'].isin(discard_ids)]\n",
    "\n",
    "original_data = original_data[~original_data['#TRI ID'].isin(keep_ids.keys())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8714d624",
   "metadata": {},
   "source": [
    "## Reannotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "44d2fef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4>ACTIVATION & REPRESSION</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "There are 5 sentences that have been noted as <b>both ACTIVATION and REPRESSION</b><br>\n",
       "Ideally, they should have a BOTH label, but as we don't have enough data, we'll convert them into UNDEFINED.<br>\n",
       "Resulting rows:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#TRI ID</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>MoR</th>\n",
       "      <th>Valid?</th>\n",
       "      <th>true_label</th>\n",
       "      <th>true_MoR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>8994186:6:NR4A1:POMC</td>\n",
       "      <td>Finally, we provide evidence that the nurr1/[T...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>ACTIVATION</td>\n",
       "      <td>F</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>UNDEFINED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>9514889:7:NFKB:PTGS2</td>\n",
       "      <td>These results suggest that the [TF] site is in...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>ACTIVATION</td>\n",
       "      <td>F</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>UNDEFINED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>9256061:7:TFEC:HMOX1</td>\n",
       "      <td>By transient coexpression assays, we showed th...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>UNDEFINED</td>\n",
       "      <td>T</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>UNDEFINED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>9819390:0:TLX1:ALDH1A1</td>\n",
       "      <td>The T-cell oncogenic protein [TF] activates [T...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>UNDEFINED</td>\n",
       "      <td>T</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>UNDEFINED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>15660126:4:NFKB:SLC1A2</td>\n",
       "      <td>Herein, we demonstrate that both TNFalpha-medi...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>UNDEFINED</td>\n",
       "      <td>T</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>UNDEFINED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     #TRI ID  \\\n",
       "1578    8994186:6:NR4A1:POMC   \n",
       "1581    9514889:7:NFKB:PTGS2   \n",
       "1915    9256061:7:TFEC:HMOX1   \n",
       "1918  9819390:0:TLX1:ALDH1A1   \n",
       "1941  15660126:4:NFKB:SLC1A2   \n",
       "\n",
       "                                               Sentence Label         MoR  \\\n",
       "1578  Finally, we provide evidence that the nurr1/[T...  TRUE  ACTIVATION   \n",
       "1581  These results suggest that the [TF] site is in...  TRUE  ACTIVATION   \n",
       "1915  By transient coexpression assays, we showed th...  TRUE   UNDEFINED   \n",
       "1918  The T-cell oncogenic protein [TF] activates [T...  TRUE   UNDEFINED   \n",
       "1941  Herein, we demonstrate that both TNFalpha-medi...  TRUE   UNDEFINED   \n",
       "\n",
       "     Valid? true_label   true_MoR  \n",
       "1578      F       TRUE  UNDEFINED  \n",
       "1581      F       TRUE  UNDEFINED  \n",
       "1915      T       TRUE  UNDEFINED  \n",
       "1918      T       TRUE  UNDEFINED  \n",
       "1941      T       TRUE  UNDEFINED  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4>Negations & suboptimal</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "There are 60 sentences that are negations and will be converted to 'False'"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "There are 160 sentences that are suboptimal and will be converted to 'False'"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#TRI ID</th>\n",
       "      <th>TF_norm</th>\n",
       "      <th>TG_norm</th>\n",
       "      <th>TF</th>\n",
       "      <th>TG</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>MoR</th>\n",
       "      <th>Valid?</th>\n",
       "      <th>true_label</th>\n",
       "      <th>true_MoR</th>\n",
       "      <th>Explanation</th>\n",
       "      <th>pred_prob_TRI</th>\n",
       "      <th>pred_prob_MoR</th>\n",
       "      <th>Iter</th>\n",
       "      <th>Problem_with</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [#TRI ID, TF_norm, TG_norm, TF, TG, Sentence, Label, MoR, Valid?, true_label, true_MoR, Explanation, pred_prob_TRI, pred_prob_MoR, Iter, Problem_with]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PREPROCESS ITER DATASETS\n",
    "\n",
    "# Remove empty rows & columns\n",
    "iter_1_2.dropna(how='all', inplace=True)\n",
    "iter_3.dropna(subset = iter_3.columns.difference(['Problem with:']), how='all', inplace=True)\n",
    "iter_3.drop(columns=['Unnamed: 17'], inplace=True)\n",
    "\n",
    "## Fix columns/cells\n",
    "# If 'Iter' is neither 1 nor 2, it belongs to the Splitter.\n",
    "iter_1_2.loc[iter_1_2['Iter'].isna(), 'Iter'] = 'Spliter'\n",
    "iter_3['Iter'] = '3'\n",
    "# These specific cells were incorrectly annotated\n",
    "m = iter_1_2['#TRI ID'] == '24276245:6:E2F1:ABCG2'\n",
    "iter_1_2.loc[m, 'true_MoR'] = 'UNDEFINED'\n",
    "m = iter_1_2['#TRI ID'] == '21813016:5:CTCF:CDKN1A'\n",
    "iter_1_2.loc[m, 'true_label'] = 'TRUE'\n",
    "\n",
    "## Join the 'Problem_with' in one only row\n",
    "columns_to_join = ['Problem with:', 'Unnamed: 10']\n",
    "iter_1_2['Problem_with'] = iter_1_2.apply(lambda row: '|'.join(sorted(row[col] for col in columns_to_join if not pd.isna(row[col]))), axis=1)\n",
    "iter_1_2.drop(columns=columns_to_join, inplace=True)\n",
    "\n",
    "columns_to_join = ['Problem with:', 'Unnamed: 11', 'Suboptimal?']\n",
    "iter_3['Problem_with'] = iter_3.apply(lambda row: '|'.join(sorted([row[col] for col in columns_to_join if not pd.isna(row[col])])), axis=1)\n",
    "iter_3.drop(columns=columns_to_join, inplace=True)\n",
    "\n",
    "## Remove duplicates (annotated both in Iteration 1 and 2)\n",
    "iter_1_2.drop_duplicates(subset=['#TRI ID', 'Sentence', 'Label', 'MoR'], keep='first', inplace=True)\n",
    "\n",
    "## Convert 'both activation & repression' into UNDEFINED\n",
    "m = iter_1_2['Explanation'] == 'BOTH activation AND repression'\n",
    "h4('ACTIVATION & REPRESSION')\n",
    "md(f'''There are {m.sum()} sentences that have been noted as <b>both ACTIVATION and REPRESSION</b><br>\n",
    "Ideally, they should have a BOTH label, but as we don't have enough data, we'll convert them into UNDEFINED.<br>\n",
    "Resulting rows:''')\n",
    "iter_1_2.loc[m, 'Valid?'] = iter_1_2.loc[m, 'MoR'].apply(lambda MoR: 'T' if MoR == 'UNDEFINED' else 'F')\n",
    "iter_1_2.loc[m, 'true_label'] = 'TRUE'\n",
    "iter_1_2.loc[m, 'true_MoR'] = 'UNDEFINED'\n",
    "display(iter_1_2[m][['#TRI ID', 'Sentence', 'Label', 'MoR', 'Valid?', 'true_label', 'true_MoR']])\n",
    "\n",
    "# Join iter 1,2,3\n",
    "validated_sents = pd.concat([iter_1_2, iter_3], axis=0)\n",
    "# Drop duplicates between iters 2 & 3 (keep Iteration 3)\n",
    "validated_sents = validated_sents.sort_values(by='Iter').drop_duplicates(subset=['#TRI ID', 'Sentence', 'Label', 'MoR'], keep='last')\n",
    "\n",
    "# Convert Negations or Suboptimal to 'Label' = 'False'\n",
    "h4('Negations & suboptimal')\n",
    "m_negations =  validated_sents['Problem_with'].str.contains('negation')\n",
    "md(f\"There are {m_negations.sum()} sentences that are negations and will be converted to 'False'\")\n",
    "m_suboptimal = validated_sents['Problem_with'].str.contains('Suboptimal')\n",
    "md(f\"There are {m_suboptimal.sum()} sentences that are suboptimal and will be converted to 'False'\")\n",
    "m = m_negations | m_suboptimal\n",
    "validated_sents.loc[m, 'Valid?'] = np.where(validated_sents.loc[m, 'Label'] == 'FALSE', 'T', 'F')\n",
    "validated_sents.loc[m, 'true_label'] = 'FALSE'\n",
    "validated_sents.loc[m, 'true_MoR'] = np.nan\n",
    "\n",
    "# Remove validated rows that have been split\n",
    "validated_sents = validated_sents[~validated_sents['#TRI ID'].isin(keep_ids.keys())]\n",
    "\n",
    "## Assertions\n",
    "assert validated_sents.duplicated(subset=['#TRI ID']).sum() == 0\n",
    "assert all(iter_3.columns == iter_1_2.columns), 'Iter 1_2 and Iter_3 have different columns'\n",
    "assert validated_sents['Valid?'].isna().sum() == 0\n",
    "assert ((validated_sents['Valid?'] == 'F') & (validated_sents['true_label'].isna())).sum() == 0\n",
    "assert ((validated_sents['true_label'] == 'TRUE') & (validated_sents['true_MoR'].isna())).sum() == 0\n",
    "display(validated_sents[(validated_sents['true_label'] == 'FALSE') & ~(validated_sents['true_MoR'].isna())])\n",
    "assert ((validated_sents['true_label'] == 'FALSE') & ~(validated_sents['true_MoR'].isna())).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6a4b89c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1991/3086 (65%) reannotated sentences have changed their labels"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Other problems encountered:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            All data        Non valid rows \n",
      "negation    60              12             \n",
      "PPI         19              2              \n",
      "dir-gene    27              10             \n",
      "Splitter    11              7              \n",
      "dir-syntax  3050            1973           \n",
      "Suboptimal  160             149            \n"
     ]
    }
   ],
   "source": [
    "# SHOW STATISTICS\n",
    "m = validated_sents['Valid?'] == 'F'\n",
    "md(f'{m.sum()}/{len(m)} ({m.sum() / len(m):.0%}) reannotated sentences have changed their labels')\n",
    "md('Other problems encountered:')\n",
    "problems = set(p for unique in validated_sents['Problem_with'].unique() for p in unique.split('|'))\n",
    "problems.discard('')\n",
    "print(f\"{'':<11} {'All data':<15} {'Non valid rows':<15}\")\n",
    "for problem in problems:\n",
    "    in_all_data  = validated_sents['Problem_with'].str.contains(problem).sum()\n",
    "    in_incorrect = validated_sents[m]['Problem_with'].str.contains(problem).sum()\n",
    "    print(f\"{problem:<11} {in_all_data:<15} {in_incorrect:<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3a8dea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE TRAINING DATASET\n",
    "updated_data = original_data.copy()\n",
    "\n",
    "# Get data to update in the same format as the training dataset\n",
    "m = validated_sents['Valid?'] == 'F'\n",
    "to_update = validated_sents[m]\n",
    "to_update.loc[:, 'true_label'] = to_update['true_label'].replace({'FALSE': 'false', 'TRUE': 'true'})\n",
    "\n",
    "# Create mappings\n",
    "label_map = to_update.set_index('#TRI ID')['true_label']\n",
    "MoR_map   = to_update.set_index('#TRI ID')['true_MoR']\n",
    "\n",
    "# Apply mappings to update 'Label' and 'Type' where '#TRI ID' matches\n",
    "mask = updated_data['#TRI ID'].isin(to_update['#TRI ID'])\n",
    "updated_data.loc[mask, 'Label'] = updated_data.loc[mask, '#TRI ID'].map(label_map)\n",
    "updated_data.loc[mask, 'Type']  = updated_data.loc[mask, '#TRI ID'].map(MoR_map)\n",
    "\n",
    "# Create column to keep track of validated rows\n",
    "updated_data['validated?'] = updated_data['#TRI ID'].isin(validated_sents['#TRI ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fe1f1b",
   "metadata": {},
   "source": [
    "## Enhance with negations from the extended NTNU dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf720de4",
   "metadata": {},
   "source": [
    "`original_tri_sentences.tsv` is a refined subset of the NTNU dataset, a set of 40K validated sentences from ExTRI. Between other things, it did not include sentences marked as 'negations'. Without those, the model doesn't learn that sentences such as:\n",
    "> TF has been observed to have no effect on TG\n",
    "Are marked as 'Valid' by the model. \n",
    "\n",
    "Thus, some of the negation sentences from the original NTNU dataset are recovered to enhance training data with negations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c473683f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#TRI</th>\n",
       "      <th>Valid</th>\n",
       "      <th>Sign</th>\n",
       "      <th>Negation</th>\n",
       "      <th>QC</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Source file</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18728219:2:ARNTL:CRY1</td>\n",
       "      <td>true</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>true</td>\n",
       "      <td>---{\"transcription factor (associated gene nam...</td>\n",
       "      <td>AGS_24h_transcriptome_QCed_MANUAL_curation_sep...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19605937:1:ARNTL:CRY1</td>\n",
       "      <td>true</td>\n",
       "      <td>+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>true</td>\n",
       "      <td>---{\"transcription factor (associated gene nam...</td>\n",
       "      <td>AGS_24h_transcriptome_QCed_MANUAL_curation_sep...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    #TRI Valid Sign Negation    QC  \\\n",
       "0  18728219:2:ARNTL:CRY1  true  NaN      NaN  true   \n",
       "1  19605937:1:ARNTL:CRY1  true    +      NaN  true   \n",
       "\n",
       "                                             Comment  \\\n",
       "0  ---{\"transcription factor (associated gene nam...   \n",
       "1  ---{\"transcription factor (associated gene nam...   \n",
       "\n",
       "                                         Source file Sentence  \n",
       "0  AGS_24h_transcriptome_QCed_MANUAL_curation_sep...      NaN  \n",
       "1  AGS_24h_transcriptome_QCed_MANUAL_curation_sep...      NaN  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 終 TODO - I will remove it from here, but there's some NTNU analysis in the one in ExTRI_classifiers\n",
    "NTNU_extended[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23338d94",
   "metadata": {},
   "source": [
    "The NTNU extended dataset did not keep the original TF/TG mentions in the text (only their normalizations). Only those sentences that are marked as negations, and where the exact mention of the normalized TF and TG are present in the text, are considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "47d26da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 496 sentences marked as negations.\n",
      "From those, both normalized TF&TG are present in 28 rows.\n"
     ]
    }
   ],
   "source": [
    "# Extract negations from NTNU\n",
    "m = (NTNU_extended['Negation'] == 'true') & ~NTNU_extended['Sentence'].isna()\n",
    "NTNU_negations = NTNU_extended[m].copy()\n",
    "NTNU_negations['TF'] = NTNU_negations['#TRI'].str.split(':').str[2]\n",
    "NTNU_negations['TG'] = NTNU_negations['#TRI'].str.split(':').str[3]\n",
    "\n",
    "m = NTNU_negations.apply(lambda row: (row['TF'] in row['Sentence']) & (row['TG'] in row['Sentence']), axis=1)\n",
    "\n",
    "print(f\"There are {len(NTNU_negations)} sentences marked as negations.\")\n",
    "print(f\"From those, both normalized TF&TG are present in {m.sum()} rows.\")\n",
    "#NTNU_negations[m].to_csv('negations.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b980cc58",
   "metadata": {},
   "source": [
    "In most cases, the sentence contains more than 1 mention of the TF or TG, making the process of swapping the TF/TG for their `[TF]` and `[TG]` tokens difficult to automate. It has been done manually, and saved in `negations_NTNU.tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ba56f724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the negations in the format of the updated data\n",
    "negations_to_add = negations_to_add[['#TRI', 'Sentence', 'TF', 'TG']]\n",
    "negations_to_add.rename(columns={\"#TRI\": \"#TRI ID\"}, inplace=True)\n",
    "negations_to_add['Label'] = 'false'\n",
    "negations_to_add['Type'] = np.nan\n",
    "negations_to_add['validated?'] = True\n",
    "\n",
    "# Combine it with the updated data\n",
    "updated_data = pd.concat([updated_data, negations_to_add], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a1a608",
   "metadata": {},
   "source": [
    "## Save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8d1b31df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32043/1024324367.py:8: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  updated_data['Label'] = updated_data['Label'].replace({'false': False, 'true': True})\n"
     ]
    }
   ],
   "source": [
    "# Ensure data has expected values\n",
    "assert updated_data['validated?'].sum()  == len(validated_sents) + len(negations_to_add)\n",
    "assert set(updated_data['Label']).issubset({'false', 'true'})\n",
    "assert set(updated_data['Type']).issubset({np.nan, 'UNDEFINED', 'ACTIVATION', 'REPRESSION'})\n",
    "assert ((updated_data['Label'] == 'False') & ~(updated_data['Type'].isna())).sum() == 0\n",
    "\n",
    "# Convert it to False and True\n",
    "updated_data['Label'] = updated_data['Label'].replace({'false': False, 'true': True})\n",
    "\n",
    "# Save the datasets\n",
    "updated_data.to_csv(updated_data_path, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".general_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
