{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing of the ExTRI2 pipeline results to create the ExTRI2 resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was used to determine the rules to use for renormalisation & discard of sentences. Sentences were extracted from the ExTRI2 resource and checked manually, to determine how to handle each category. \n",
    "\n",
    "Furthermore, the **run postprocessing.py** section is a self-contained section to create the post-processed ExTRI2 file from the file obtained from the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3>Table of contents</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "[Postprocessing of the ExTRI2 pipeline results to create the ExTRI2 resource](#Postprocessing-of-the-ExTRI2-pipeline-results-to-create-the-ExTRI2-resource)\n",
       "- [Run postprocessing.py](#Run-postprocessing.py)\n",
       "- [Setup](#Setup)\n",
       "- [Postprocessing](#Postprocessing)\n",
       "  - [HGNC orthologs](#HGNC-orthologs)\n",
       "  - [AP1 & NFKB](#AP1-&-NFKB)\n",
       "  - [Initial exploration](#Initial-exploration)\n",
       "  - [Create sets of sentences to check](#Create-sets-of-sentences-to-check)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "__import__('sys').path.append('../common/'); __import__('notebook_utils').table_of_contents('postprocessing_checkings.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "This notebook will now only be used for the normalisation of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run postprocessing.py\n",
    "Self-contained cell to run postprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### POSTPROCESSING TRI_df\n",
      "We got 6706 different TFs and 26196 different TGs from sentences labelled as TRI\n",
      "Retrieving from Entrez...\n",
      "\n",
      "4967 sentences are dropped as their TG is not normalised\n",
      "\n",
      "38287 rows (4.23%) will have its TF renormalized to NFKB\n",
      "6329 rows (0.70%) will be dropped as the TG corresponds to NFKB\n",
      "9003 rows (1.00%) will have its TF renormalized to AP1\n",
      "1858 rows (0.21%) will be dropped as the TG corresponds to AP1\n",
      "Breakdown by NCBI Symbol saved in ../../data/postprocessing/tables/AP1_NFKB_breakdown.tsv\n",
      "Number of renormalized sentences and normalization:\n",
      "4827\t0.54%\tp21 is normalized to CDKN1A\n",
      "1922\t0.21%\tp53-ps is normalized to its respective p53 symbol\n",
      "\n",
      "Number of discarded sentences and percentage from total (896330 sentences) and reasoning:\n",
      "2556\t0.29%\tTheir TF contains -AS[1-3]\n",
      "673\t0.08%\tTheir TF are circRNAs\n",
      "952\t0.11%\tTheir TF (NLRP3) is followed by inflammasome but normalised to NLRP3\n",
      "1875\t0.21%\tTheir TG (NLRP3) is followed by inflammasome but normalised to NLRP3\n",
      "1088\t0.12%\tTheir TG is followed by pathway/signalling/axis/program\n",
      "2037\t0.23%\tMDM2-TP53 pair, which is always a PPI\n",
      "1554\t0.17%\tTF is CD(4|8A|8B|74|34) positive (indicative of a cell, not a gene)\n",
      "22\t0.00%\tTG is CD(4|8A|8B|74|34) positive (indicative of a cell, not a gene)\n",
      "724\t0.08%\tPreposition of incorrectly identified as a gene\n",
      "7368\t0.82%\tThey contain translation in them (found to only be correct 40% of the time)\n",
      "41646\t4.65%\tThey are autoregulations (found to only be correct ~10% of the time)\n",
      "\n",
      "2570 out of 25879 human NCBI Gene IDs map to multiple HGNC IDs. Assigning 'None' for those ambiguous mappings.\n",
      "Fetching 1092 missing human gene symbols from Entrez...\n",
      "No orthologs in      16217 rows (1.94%). Some missing in 16985 rows (2.03%)\n",
      "No HGNCs in          17169 rows (2.05%). Some missing in 17948 rows (2.15%)\n",
      "No human symbols in  16217 rows (1.94%). Some missing in 16985 rows (2.03%)\n",
      "\n",
      "### POSTPROCESSING nonTRI_df\n",
      "We got 4284 different TFs and 10938 different TGs from sentences labelled as TRI\n",
      "Retrieving from Entrez...\n",
      "\n",
      "476 sentences are dropped as their TG is not normalised\n",
      "\n",
      "2065 rows (2.78%) will have its TF renormalized to NFKB\n",
      "1221 rows (1.65%) will be dropped as the TG corresponds to NFKB\n",
      "315 rows (0.43%) will have its TF renormalized to AP1\n",
      "265 rows (0.36%) will be dropped as the TG corresponds to AP1\n",
      "Breakdown by NCBI Symbol saved in ../../data/postprocessing/tables/AP1_NFKB_breakdown.tsv\n",
      "Number of renormalized sentences and normalization:\n",
      "83\t0.11%\tp21 is normalized to CDKN1A\n",
      "125\t0.17%\tp53-ps is normalized to its respective p53 symbol\n",
      "\n",
      "Number of discarded sentences and percentage from total (72663 sentences) and reasoning:\n",
      "60\t0.08%\tTheir TF contains -AS[1-3]\n",
      "12\t0.02%\tTheir TF are circRNAs\n",
      "99\t0.14%\tTheir TF (NLRP3) is followed by inflammasome but normalised to NLRP3\n",
      "23\t0.03%\tTheir TG (NLRP3) is followed by inflammasome but normalised to NLRP3\n",
      "1554\t2.14%\tTheir TG is followed by pathway/signalling/axis/program\n",
      "82\t0.11%\tMDM2-TP53 pair, which is always a PPI\n",
      "311\t0.43%\tTF is CD(4|8A|8B|74|34) positive (indicative of a cell, not a gene)\n",
      "245\t0.34%\tTG is CD(4|8A|8B|74|34) positive (indicative of a cell, not a gene)\n",
      "82\t0.11%\tPreposition of incorrectly identified as a gene\n",
      "665\t0.92%\tThey contain translation in them (found to only be correct 40% of the time)\n",
      "15020\t20.67%\tThey are autoregulations (found to only be correct ~10% of the time)\n",
      "\n",
      "2570 out of 25879 human NCBI Gene IDs map to multiple HGNC IDs. Assigning 'None' for those ambiguous mappings.\n",
      "Fetching 730 missing human gene symbols from Entrez...\n",
      "No orthologs in      864 rows (1.59%). Some missing in 907 rows (1.66%)\n",
      "No HGNCs in          913 rows (1.67%). Some missing in 959 rows (1.76%)\n",
      "No human symbols in  864 rows (1.59%). Some missing in 907 rows (1.66%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../common')\n",
    "sys.path.append('../../')\n",
    "from scripts.postprocessing.postprocessing import *\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "## Custom functions\n",
    "import sys\n",
    "\n",
    "sys.path.append('../common')\n",
    "sys.path.append('../../')\n",
    "\n",
    "from notebook_utils import table_of_contents, table_from_dict, h3, h4, h5, md, bold\n",
    "from renormalisations import *\n",
    "from postprocessing import *\n",
    "pd.set_option('display.max_colwidth', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#SentenceID</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>TF</th>\n",
       "      <th>TG</th>\n",
       "      <th>TF Id</th>\n",
       "      <th>TG Id</th>\n",
       "      <th>TF offset</th>\n",
       "      <th>Gene offset</th>\n",
       "      <th>TRI score</th>\n",
       "      <th>Valid</th>\n",
       "      <th>...</th>\n",
       "      <th>TG Symbol</th>\n",
       "      <th>TG TaxID</th>\n",
       "      <th>TF_type</th>\n",
       "      <th>renormalisation</th>\n",
       "      <th>TF_human_Id</th>\n",
       "      <th>TF_human_symbol</th>\n",
       "      <th>TF_HGNC_Id</th>\n",
       "      <th>TG_human_Id</th>\n",
       "      <th>TG_human_symbol</th>\n",
       "      <th>TG_HGNC_Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PMID:35388756:pu...</td>\n",
       "      <td>Chip-IP results ...</td>\n",
       "      <td>CHCHD2</td>\n",
       "      <td>GNPTG</td>\n",
       "      <td>51142</td>\n",
       "      <td>84572</td>\n",
       "      <td>1366</td>\n",
       "      <td>1458</td>\n",
       "      <td>0.992724397527042</td>\n",
       "      <td>Valid</td>\n",
       "      <td>...</td>\n",
       "      <td>GNPTG</td>\n",
       "      <td>9606</td>\n",
       "      <td>coTF</td>\n",
       "      <td></td>\n",
       "      <td>51142</td>\n",
       "      <td>CHCHD2</td>\n",
       "      <td>HGNC:21645</td>\n",
       "      <td>84572</td>\n",
       "      <td>GNPTG</td>\n",
       "      <td>HGNC:23026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PMID:26808438:pu...</td>\n",
       "      <td>Among multiple C...</td>\n",
       "      <td>ChREBP</td>\n",
       "      <td>Mid1ip1,Txnip</td>\n",
       "      <td>51085</td>\n",
       "      <td>58526;10628</td>\n",
       "      <td>919</td>\n",
       "      <td>1050</td>\n",
       "      <td>0.9927056483604216</td>\n",
       "      <td>Valid</td>\n",
       "      <td>...</td>\n",
       "      <td>MID1IP1;TXNIP</td>\n",
       "      <td>9606</td>\n",
       "      <td>dbTF</td>\n",
       "      <td></td>\n",
       "      <td>51085</td>\n",
       "      <td>MLXIPL</td>\n",
       "      <td>HGNC:12744</td>\n",
       "      <td>58526;10628</td>\n",
       "      <td>MID1IP1;TXNIP</td>\n",
       "      <td>HGNC:20715;HGNC:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           #SentenceID             Sentence      TF             TG  TF Id  \\\n",
       "0  PMID:35388756:pu...  Chip-IP results ...  CHCHD2          GNPTG  51142   \n",
       "1  PMID:26808438:pu...  Among multiple C...  ChREBP  Mid1ip1,Txnip  51085   \n",
       "\n",
       "         TG Id TF offset Gene offset           TRI score  Valid  ...  \\\n",
       "0        84572      1366        1458   0.992724397527042  Valid  ...   \n",
       "1  58526;10628       919        1050  0.9927056483604216  Valid  ...   \n",
       "\n",
       "       TG Symbol TG TaxID TF_type renormalisation TF_human_Id TF_human_symbol  \\\n",
       "0          GNPTG     9606    coTF                       51142          CHCHD2   \n",
       "1  MID1IP1;TXNIP     9606    dbTF                       51085          MLXIPL   \n",
       "\n",
       "   TF_HGNC_Id  TG_human_Id TG_human_symbol           TG_HGNC_Id  \n",
       "0  HGNC:21645        84572           GNPTG           HGNC:23026  \n",
       "1  HGNC:12744  58526;10628   MID1IP1;TXNIP  HGNC:20715;HGNC:...  \n",
       "\n",
       "[2 rows x 26 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gene_ID</th>\n",
       "      <th>human_gene_ID</th>\n",
       "      <th>TaxID</th>\n",
       "      <th>gene_symbol</th>\n",
       "      <th>human_gene_symbol</th>\n",
       "      <th>unique_human_gene_ID</th>\n",
       "      <th>unique_human_gene_symbol</th>\n",
       "      <th>HGNC_ID</th>\n",
       "      <th>unique_HGNC_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100009600</td>\n",
       "      <td>100125288</td>\n",
       "      <td>10090</td>\n",
       "      <td>Zglp1</td>\n",
       "      <td>ZGLP1</td>\n",
       "      <td>100125288</td>\n",
       "      <td>ZGLP1</td>\n",
       "      <td>HGNC:37245</td>\n",
       "      <td>HGNC:37245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100033459</td>\n",
       "      <td>None</td>\n",
       "      <td>10090</td>\n",
       "      <td>Ifi208</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Gene_ID human_gene_ID  TaxID gene_symbol human_gene_symbol  \\\n",
       "0  100009600     100125288  10090       Zglp1             ZGLP1   \n",
       "1  100033459          None  10090      Ifi208              None   \n",
       "\n",
       "  unique_human_gene_ID unique_human_gene_symbol     HGNC_ID unique_HGNC_ID  \n",
       "0            100125288                ZGLP1      HGNC:37245     HGNC:37245  \n",
       "1                 None                 None            None           None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checkings on the processed final TRI df\n",
    "config = load_config()\n",
    "final_TRI_df = load_df(config['final_ExTRI2_p'])\n",
    "orthologs_df = load_df(config['orthologs_final_p'])\n",
    "\n",
    "display(final_TRI_df[:2])\n",
    "display(orthologs_df[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthologs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gene_ID</th>\n",
       "      <th>human_gene_ID</th>\n",
       "      <th>TaxID</th>\n",
       "      <th>gene_symbol</th>\n",
       "      <th>human_gene_symbol</th>\n",
       "      <th>unique_human_gene_ID</th>\n",
       "      <th>unique_human_gene_symbol</th>\n",
       "      <th>HGNC_ID</th>\n",
       "      <th>unique_HGNC_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100009600</td>\n",
       "      <td>100125288</td>\n",
       "      <td>10090</td>\n",
       "      <td>Zglp1</td>\n",
       "      <td>ZGLP1</td>\n",
       "      <td>100125288</td>\n",
       "      <td>ZGLP1</td>\n",
       "      <td>HGNC:37245</td>\n",
       "      <td>HGNC:37245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100033459</td>\n",
       "      <td>None</td>\n",
       "      <td>10090</td>\n",
       "      <td>Ifi208</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Gene_ID human_gene_ID  TaxID gene_symbol human_gene_symbol  \\\n",
       "0  100009600     100125288  10090       Zglp1             ZGLP1   \n",
       "1  100033459          None  10090      Ifi208              None   \n",
       "\n",
       "  unique_human_gene_ID unique_human_gene_symbol     HGNC_ID unique_HGNC_ID  \n",
       "0            100125288                ZGLP1      HGNC:37245     HGNC:37245  \n",
       "1                 None                 None            None           None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All gene IDs in ExTRI2: 24574    \n",
      "Missing gene symbols: 0\n",
      "\n",
      "One-to-one human gene mapping: 23723\n",
      "One-to-many human gene mapping: 270\n",
      "- With a unique_human_gene_ID assigned: 247 (23 unassigned)\n",
      "- Assigned through exact match: 189 (58 through 1st family member rule or manual correction)\n",
      "Missing human gene mapping: 581\n",
      "Missing HGNC IDs: 619\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CHECKINGS ON ORTHOLOGS_DF\n",
    "ExTRI2_df = final_TRI_df.copy()\n",
    "display(orthologs_df[:2])\n",
    "\n",
    "# Unique human mappings\n",
    "m = orthologs_df['human_gene_ID'].str.contains(';')\n",
    "m1 = m & (orthologs_df['unique_human_gene_ID'] != 'None')\n",
    "m2 = m1 & (orthologs_df['unique_human_gene_symbol'].str.upper() == orthologs_df['gene_symbol'].str.upper())\n",
    "\n",
    "print(f\"\"\"\\\n",
    "All gene IDs in ExTRI2: {len(orthologs_df)}    \n",
    "Missing gene symbols: {orthologs_df['gene_symbol'].str.contains('None').sum()}\n",
    "\n",
    "One-to-one human gene mapping: {(~m & (orthologs_df['human_gene_ID'] != 'None')).sum()}\n",
    "One-to-many human gene mapping: {m.sum()}\n",
    "- With a unique_human_gene_ID assigned: {m1.sum()} ({(m & ~m1).sum()} unassigned)\n",
    "- Assigned through exact match: {m2.sum()} ({(m1 & ~m2).sum()} through 1st family member rule or manual correction)\n",
    "Missing human gene mapping: {(orthologs_df['human_gene_ID'] == 'None').sum()}\n",
    "Missing HGNC IDs: {(orthologs_df['HGNC_ID'] == 'None').sum()}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of orthologs to check: 81\n",
      "Manually corrected orthologs: 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene_symbol</th>\n",
       "      <th>unique_human_gene_symbol</th>\n",
       "      <th>comment</th>\n",
       "      <th>complies with the 'first-gene-family'-rule</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gene_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25642</th>\n",
       "      <td>Cyp3a23-3a1</td>\n",
       "      <td>CYP3A4</td>\n",
       "      <td>complies with th...</td>\n",
       "      <td>CYP3A4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53973</th>\n",
       "      <td>Cyp3a41a</td>\n",
       "      <td>CYP3A4</td>\n",
       "      <td>complies with th...</td>\n",
       "      <td>CYP3A4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387151</th>\n",
       "      <td>Mir133a-1</td>\n",
       "      <td>MIR133A1</td>\n",
       "      <td>complies with th...</td>\n",
       "      <td>MIR133A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68774</th>\n",
       "      <td>Ms4a6d</td>\n",
       "      <td>MS4A6A</td>\n",
       "      <td>complies with th...</td>\n",
       "      <td>MS4A6A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394432</th>\n",
       "      <td>Ugt1a7c</td>\n",
       "      <td>UGT1A7</td>\n",
       "      <td>complies with th...</td>\n",
       "      <td>UGT1A7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         gene_symbol unique_human_gene_symbol              comment  \\\n",
       "Gene_ID                                                              \n",
       "25642    Cyp3a23-3a1               CYP3A4      complies with th...   \n",
       "53973       Cyp3a41a               CYP3A4      complies with th...   \n",
       "387151     Mir133a-1             MIR133A1      complies with th...   \n",
       "68774         Ms4a6d               MS4A6A      complies with th...   \n",
       "394432       Ugt1a7c               UGT1A7      complies with th...   \n",
       "\n",
       "        complies with the 'first-gene-family'-rule  \n",
       "Gene_ID                                             \n",
       "25642                 CYP3A4                        \n",
       "53973                 CYP3A4                        \n",
       "387151              MIR133A1                        \n",
       "68774                 MS4A6A                        \n",
       "394432                UGT1A7                        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MANUALLY CHECK ORTHOLOGS WITH UNIQUE HUMAN ID EITHER UNASSIGNED OR ASSIGNED THROUGH 1ST FAMILY MEMBER RULE\n",
    "\n",
    "# --- PREPARING TABLE OF ORTHOLOGS TO CHECK ---  \n",
    "def get_unique_human_symbol_index(row):\n",
    "    '''\n",
    "    Get the index of the unique human gene symbol following these rules:\n",
    "    1) Return exact lowercase match, if any\n",
    "    2) First gene family member (e.g. ACSM2A for ACSM2; if multiple, take the one with the smallest numeric suffix)\n",
    "    3) If no match, return None\n",
    "    '''\n",
    "    # Get rodent symbol & human symbols\n",
    "    rodent_symbol = row['gene_symbol'].upper()\n",
    "    human_symbols = row['human_gene_symbol'].upper().split(\";\")\n",
    "\n",
    "    # If there's only one symbol, return index 0\n",
    "    if len(human_symbols) == 1:\n",
    "        return 0\n",
    "\n",
    "    # 1) Exact match (case-insensitive)\n",
    "    for i, c in enumerate(human_symbols):\n",
    "        if c == rodent_symbol:\n",
    "            return i\n",
    "\n",
    "    # 2) Apply first gene family member rule:\n",
    "    # Assumption: gene names are formed by \"[A-Z]+[0-9]*[A-Z]?\"\n",
    "    # Extract family stem and number (e.g. ADH1 -> stem=ADH, number=1)\n",
    "    m = re.match(r'^([A-Z]+)(\\d+)?([A-Z]?)(\\d+)?$', rodent_symbol)\n",
    "    stem = m.group(1) if m else rodent_symbol\n",
    "    num = m.group(2) if (m and m.group(2)) else None\n",
    "    letter = m.group(3) if (m and m.group(3)) else None\n",
    "    num2 = m.group(4) if (m and m.group(4)) else None\n",
    "\n",
    "    # Get human symbols that start with the same stem\n",
    "    candidates = [(i, hs) for i, hs in enumerate(human_symbols) if hs.startswith(stem)]\n",
    "    if not candidates:\n",
    "        return None\n",
    "    \n",
    "    # Prefer candidates that match the stem\n",
    "    for i, hs in candidates:\n",
    "        if hs == stem:\n",
    "            return i\n",
    "        \n",
    "    # If there is a number, prefer same numeric family (e.g. ADH1A over ADH2A)\n",
    "    if num:\n",
    "        same_family = [(i, hs) for i, hs in candidates if hs.startswith(stem + num)]\n",
    "\n",
    "        if same_family:\n",
    "            # Prefer exact stem+num (if present), else smallest suffix\n",
    "            exact = [i for i, hs in same_family if hs == stem + num]\n",
    "            if exact:\n",
    "                return exact[0]\n",
    "            same_family.sort(key=lambda x: x[1])\n",
    "            return same_family[0][0]\n",
    "        \n",
    "    # Otherwise, just pick smallest lexicographic suffix (first family member)\n",
    "    candidates.sort(key=lambda x: x[1])\n",
    "    return candidates[0][0]\n",
    "\n",
    "def assign_unique_human_fields(row):\n",
    "    '''Helper function to assign unique fields based on index'''\n",
    "    idx = get_unique_human_symbol_index(row)\n",
    "\n",
    "    if idx is None:\n",
    "        return pd.Series({\n",
    "            'unique_human_gene_ID': 'None',\n",
    "            'unique_human_gene_symbol':  'None',\n",
    "        })\n",
    "\n",
    "    return pd.Series({\n",
    "        'unique_human_gene_ID': row['human_gene_ID'].split(';')[idx],\n",
    "        'unique_human_gene_symbol': row['human_gene_symbol'].split(';')[idx],\n",
    "    })\n",
    "\n",
    "orthologs_df = orthologs_df.drop(columns=['unique_human_gene_ID', 'unique_human_gene_symbol'])\n",
    "orthologs_df = orthologs_df.join(orthologs_df.apply(assign_unique_human_fields, axis=1))\n",
    "\n",
    "# Precompute counts in ExTRI2_df\n",
    "tf_counts = ExTRI2_df['TF Symbol'].value_counts()\n",
    "tg_counts = ExTRI2_df['TG Symbol'].value_counts()\n",
    "\n",
    "# Map counts\n",
    "orthologs_df['TF_count'] = orthologs_df['gene_symbol'].map(tf_counts).fillna(0).astype(int)\n",
    "orthologs_df['TG_count'] = orthologs_df['gene_symbol'].map(tg_counts).fillna(0).astype(int)\n",
    "\n",
    "# Save orthologs with unique human ID either unassigned or assigned through 1st family member rule\n",
    "m = (orthologs_df['human_gene_ID'].str.contains(';')) & (orthologs_df['unique_human_gene_symbol'].str.upper() != orthologs_df['gene_symbol'].str.upper())\n",
    "\n",
    "cols_first = ['TaxID', 'Gene_ID', 'gene_symbol', 'unique_human_gene_symbol', 'human_gene_symbol', 'human_gene_ID', 'HGNC_ID']\n",
    "orthologs_to_check = orthologs_df[m][cols_first + [c for c in orthologs_df.columns if c not in cols_first]].sort_values(by=['gene_symbol', 'TaxID']).reset_index(drop=True)\n",
    "print(f\"Number of orthologs to check: {len(orthologs_to_check)}\")\n",
    "\n",
    "orthologs_to_check.to_csv(config['data_p'] + 'validation/orthologs_to_check.tsv', sep='\\t', index=False)\n",
    "\n",
    "# --- GETTING MANUALLY-CORRECTED ORTHOLOGS ---\n",
    "# Orthologs have been checked and corrections are in:\n",
    "orthologs_checked = pd.read_csv(config['data_p'] + 'validation/orthologs_to_check_w_comments.tsv', sep='\\t')\n",
    "# Correct ortholog is in one of these 3 columns:\n",
    "comment_cols = [\n",
    "    \"complies with the 'first-gene-family'-rule\", # We have decided to only take this column into consideration\n",
    "    # 'cited as human orthologue in NCBI Gene \"Summary\" for the mouse/rat gene',\n",
    "    # 'human gene has rodent symbol as alias'\n",
    "]\n",
    "\n",
    "# Get the subset of manually corrected orthologs\n",
    "m = (orthologs_checked[\"complies with the 'first-gene-family'-rule\"].notna())\n",
    "manually_corrected = orthologs_checked[m].copy().set_index(\"Gene_ID\")\n",
    "\n",
    "# Assign unique human gene symbol and comment based on the comments columns\n",
    "manually_corrected[\"unique_human_gene_symbol\"] = manually_corrected[comment_cols].bfill(axis=1).iloc[:, 0]\n",
    "manually_corrected[\"comment\"] = manually_corrected[comment_cols].apply(\n",
    "    lambda row: next(\n",
    "        (col for col in comment_cols if pd.notna(row[col]) and row[col] is not None),\n",
    "        np.nan), axis=1\n",
    ")\n",
    "# Display the manually corrected orthologs\n",
    "print(f\"Manually corrected orthologs: {len(manually_corrected)}\")\n",
    "display(manually_corrected[['gene_symbol', 'unique_human_gene_symbol', \"comment\"] + comment_cols].head())\n",
    "\n",
    "# Create a dictionary\n",
    "manual_orthologs_dict = manually_corrected[[\"unique_human_gene_symbol\", \"comment\"]].to_dict(orient=\"index\")\n",
    "# Save as json\n",
    "with open(config['data_p'] + 'postprocessing/manual_orthologs_corrections.json', 'w') as f:\n",
    "    json.dump(manual_orthologs_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TF_type     \n",
       "dbTF|coTF       5\n",
       "coTF|dbTF       1\n",
       "ll_coTF|coTF    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TF_human_Id</th>\n",
       "      <th>TF Symbol</th>\n",
       "      <th>TF_type</th>\n",
       "      <th>TF TaxID</th>\n",
       "      <th>TF_human_symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>10196</td>\n",
       "      <td>PRMT3|Prmt3</td>\n",
       "      <td>dbTF|coTF</td>\n",
       "      <td>9606|10090|10116</td>\n",
       "      <td>PRMT3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>10270</td>\n",
       "      <td>AKAP8|Akap8</td>\n",
       "      <td>dbTF|coTF</td>\n",
       "      <td>9606|10116</td>\n",
       "      <td>AKAP8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>1810</td>\n",
       "      <td>DR1|Dr1</td>\n",
       "      <td>dbTF|coTF</td>\n",
       "      <td>9606|10090|10116</td>\n",
       "      <td>DR1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>434</td>\n",
       "      <td>a|ASIP</td>\n",
       "      <td>ll_coTF|coTF</td>\n",
       "      <td>10090|9606</td>\n",
       "      <td>ASIP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1903</th>\n",
       "      <td>4855</td>\n",
       "      <td>NOTCH4|Notch4</td>\n",
       "      <td>dbTF|coTF</td>\n",
       "      <td>9606|10116</td>\n",
       "      <td>NOTCH4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2380</th>\n",
       "      <td>56252</td>\n",
       "      <td>YLPM1|Ylpm1</td>\n",
       "      <td>coTF|dbTF</td>\n",
       "      <td>9606|10090</td>\n",
       "      <td>YLPM1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2649</th>\n",
       "      <td>63925</td>\n",
       "      <td>Zfp335|ZNF335</td>\n",
       "      <td>dbTF|coTF</td>\n",
       "      <td>10090|9606|10116</td>\n",
       "      <td>ZNF335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     TF_human_Id      TF Symbol       TF_type          TF TaxID  \\\n",
       "53         10196    PRMT3|Prmt3     dbTF|coTF  9606|10090|10116   \n",
       "72         10270    AKAP8|Akap8     dbTF|coTF        9606|10116   \n",
       "608         1810        DR1|Dr1     dbTF|coTF  9606|10090|10116   \n",
       "1765         434         a|ASIP  ll_coTF|coTF        10090|9606   \n",
       "1903        4855  NOTCH4|Notch4     dbTF|coTF        9606|10116   \n",
       "2380       56252    YLPM1|Ylpm1     coTF|dbTF        9606|10090   \n",
       "2649       63925  Zfp335|ZNF335     dbTF|coTF  10090|9606|10116   \n",
       "\n",
       "     TF_human_symbol  \n",
       "53             PRMT3  \n",
       "72             AKAP8  \n",
       "608              DR1  \n",
       "1765            ASIP  \n",
       "1903          NOTCH4  \n",
       "2380           YLPM1  \n",
       "2649          ZNF335  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in ExTRI2 affected: 322\n"
     ]
    }
   ],
   "source": [
    "## CHECK THE DISTRUBTION OF TF TYPES PER HUMAN MAPPING\n",
    "# Check whether any TF_human_symbol has multiple TF types from different rodent orthologs \n",
    "agg_funcs = {\n",
    "    'TF Symbol': lambda x: '|'.join(x.unique()),\n",
    "    'TF_type': lambda x: '|'.join(x.unique()),\n",
    "    'TF TaxID': lambda x: '|'.join(x.unique()),\n",
    "    'TF_human_symbol': lambda x: '|'.join(x.unique()),\n",
    "}\n",
    "\n",
    "final_TRI_grouped = ExTRI2_df.groupby('TF_human_Id', as_index=False).agg(agg_funcs)\n",
    "m = final_TRI_grouped['TF_human_Id'].str.contains(';') | final_TRI_grouped['TF_type'].isin(['coTF', 'dbTF', 'll_coTF']) | (final_TRI_grouped['TF_human_Id'] == 'None')\n",
    "display(final_TRI_grouped[~m][['TF_type']].value_counts())\n",
    "display(final_TRI_grouped[~m])\n",
    "print(f\"Rows in ExTRI2 affected: {ExTRI2_df['TF_human_Id'].isin(final_TRI_grouped[~m]['TF_human_Id']).sum()}\")\n",
    "# Clashes in a few sentences. We will exclude them from CollecTRI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AP1 & NFKB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AP1 and NFKB are dimers, and as such don't have neither a NCBI EntrezID, nor a HGNC symbol. PubTator normalizes them to one of their monomers. Therefore, in `postprocessing.py`, we\n",
    "* Find all dimers incorrectly normalized to monomers using regex\n",
    "* Change the TF metadata to AP1/NFKB. Delete the TG instances (a TG can't be a dimer)\n",
    "* Save a summary of the results and affected sentences in `data/postprocessing/tables`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### POSTPROCESSING TRI_df\n",
      "We got 6706 different TFs and 26196 different TGs from sentences labelled as TRI\n",
      "Retrieving from Entrez...\n",
      "\n",
      "4967 sentences are dropped as their TG is not normalised\n",
      "\n",
      "38287 rows (4.23%) will have its TF renormalized to NFKB\n",
      "6327 rows (0.70%) will be dropped as the TG corresponds to NFKB\n",
      "9003 rows (1.00%) will have its TF renormalized to AP1\n",
      "1858 rows (0.21%) will be dropped as the TG corresponds to AP1\n",
      "Breakdown by NCBI Symbol saved in ../../data/postprocessing/tables/AP1_NFKB_breakdown.tsv\n"
     ]
    }
   ],
   "source": [
    "# POSTPROCESSING BEFORE RENORMALISATION & DISCARDING WERE IMPLEMENTED\n",
    "def half_postprocess(ExTRI2_df: pd.DataFrame, TRI_sents: bool, config: dict) -> pd.DataFrame:\n",
    "    '''same as postprocess but before the renormalisation & discarding'''\n",
    "\n",
    "    df_type = 'TRI' if TRI_sents else 'nonTRI'\n",
    "    print(f'### POSTPROCESSING {df_type}_df')\n",
    "\n",
    "    # Retrieve Symbol & TaxID from Entrez\n",
    "    save_Symbol_TaxID_dict(ExTRI2_df, config[f'EntrezID_to_Symbol_{df_type}_p'])\n",
    "\n",
    "    # Filter & add metadata\n",
    "    if TRI_sents:\n",
    "        remove_duplicates(ExTRI2_df)\n",
    "    ExTRI2_df = add_symbols_TaxID(ExTRI2_df, config[f'EntrezID_to_Symbol_{df_type}_p'])\n",
    "    add_TF_type(ExTRI2_df, config)\n",
    "    ExTRI2_df = drop_GTFs(ExTRI2_df)\n",
    "    ExTRI2_df = remove_other_species(ExTRI2_df, TaxID)\n",
    "\n",
    "    # Fix AP1 & NFKB normalisations\n",
    "    ExTRI2_df = fix_NFKB_AP1(ExTRI2_df, config)\n",
    "\n",
    "    return ExTRI2_df\n",
    "\n",
    "config = load_config()\n",
    "\n",
    "# Load raw dataframe\n",
    "TRI_df = load_preprocess_df(config['raw_TRI_p'])\n",
    "\n",
    "# Postprocess (without renormalisation/discarding)\n",
    "TRI_df = half_postprocess(TRI_df, TRI_sents=True,  config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP1 TF Unique Entities: 155\n",
      "NFKB TF Unique Entities: 115\n"
     ]
    }
   ],
   "source": [
    "# GET NUMBER OF UNIQUE ENTITY NAMES\n",
    "ExTRI2_df = TRI_df\n",
    "\n",
    "m_AP1 = ExTRI2_df['TF Symbol'].str.contains('|'.join(('FOS', 'JUN')), case=False)\n",
    "\n",
    "NFKB_symbols = {'NFKB1', 'NFKB2', 'RELA', 'RELB'}\n",
    "m_NFKB = ExTRI2_df['TF Symbol'].str.upper().isin(NFKB_symbols)\n",
    "\n",
    "print(\"AP1 TF Unique Entities:\", len(ExTRI2_df[m_AP1]['TF'].unique()))\n",
    "# print(ExTRI2_df[m_AP1]['TF'].unique())\n",
    "print(\"NFKB TF Unique Entities:\", len(ExTRI2_df[m_NFKB]['TF'].unique()))\n",
    "# print(ExTRI2_df[m_NFKB]['TF'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST-PROCESSING FUNCTIONS\n",
    "def print_symbol_counts_side_by_side(m_template, max_counts = 10):\n",
    "    'Print symbol counts of m_template for TF & TG'\n",
    "    results = []\n",
    "    for T in ('TF', 'TG'):\n",
    "        m = eval(m_template.replace('{T}', T))\n",
    "        T_lines = TRI_df[m][f'{T} Symbol'].value_counts()[:max_counts].to_string().split('\\n')\n",
    "        results.append(T_lines)\n",
    "\n",
    "    # Print the two tables side by side\n",
    "    for tf_line, tg_line in zip(*results):\n",
    "        print(f\"{tf_line:<35} {tg_line}\")\n",
    "\n",
    "def print_dubious_pairs_TFTGcounts_side_by_side(dubious_pairs):\n",
    "    '''Print counts of TF&TG of 3 different symbols side by side'''\n",
    "    all_tables = []\n",
    "    for p in dubious_pairs:\n",
    "        results = []\n",
    "        for T in ('TF', 'TG'):\n",
    "            m = TRI_df[f'{T} Symbol'].isin([';'.join((p[0], p[1])), ';'.join((p[1], p[0]))])\n",
    "            T_counts = TRI_df[m][f'{T}'].value_counts().rename(f'{T} count')[:10]\n",
    "            results.append(T_counts)\n",
    "\n",
    "        # Merge the TF and TG counts on the same index\n",
    "        merged_df = pd.concat(results, axis=1).fillna(0).astype(int)\n",
    "        all_tables.append(merged_df)\n",
    "\n",
    "    # Convert each table to a string and split by lines\n",
    "    table_strings = [table.to_string().split('\\n') for table in all_tables]\n",
    "\n",
    "    # Use itertools.zip_longest to handle tables with different lengths\n",
    "    for lines in itertools.zip_longest(*table_strings, fillvalue=''):\n",
    "        # Print each line of the three tables side by side\n",
    "        print(f\"{lines[0]:<40} {lines[1]:<40} {lines[2]}\")\n",
    "\n",
    "def print_TF_TG_counts_side_by_side(title, m_template, sep=40):\n",
    "    bold(title)\n",
    "    counts = []\n",
    "    for T in ('TF', 'TG'):\n",
    "        m = eval(m_template)\n",
    "        T_lines = TRI_df[m][[f'{T}', f'{T} Symbol']].value_counts().to_string().split('\\n')\n",
    "        counts.append(T_lines)\n",
    "    \n",
    "    for tf_line, tg_line in itertools.zip_longest(*counts, fillvalue=''):\n",
    "        print(f\"{tf_line:<{sep}} {tg_line}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>Entities normalized to +1 ID</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "25644 (2.86%) entities are normalized to more than 1 ID.<br>We revise those that appear more than 100 times further:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Symbol                           TG Symbol\n",
      "MAPK3;MAPK1            5347         MAPK3;MAPK1       1425\n",
      "Mapk3;Mapk1            2798         Mapk3;Mapk1        618\n",
      "MAP2K1;MAP2K2           717         MMP2;MMP9          395\n",
      "SMAD2;SMAD3             481         SMAD2;SMAD3        253\n",
      "MAPK8;MAPK9             311         Smad2;Smad3        171\n",
      "Map2k1;Map2k2           303         CDK4;CDK6           96\n",
      "Smad2;Smad3             222         Mmp2;Mmp9           89\n",
      "EWSR1;FLI1              216         HSD11B1;RNU1-1      66\n",
      "ABL1;BCR                169         MIR143;MIR145       66\n",
      "BCR;ABL1                152         CASP3;CASP7         63\n",
      "CREBBP;EP300            147         MAP2K1;MAP2K2       57\n",
      "OIP5-AS1;OIP5;PTGDR     144         NKX2-5;NKX3-1       54\n",
      "SMAD1;SMAD5;SMAD9       128         CASP3;CASP9         42\n",
      "MAPK1;MAPK3             111         Ifna;Ifnb1          38\n",
      "HDAC1;HDAC2             104         EWSR1;FLI1          36\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "From those, 3 seem suspicious and are investigated further: ABL1;BCR, FLI1;EWSR1, MMP2;MMP9"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           TF count  TG count                         TF count  TG count                                                   TF count  TG count\n",
      "BCR/ABL         126         9            EWS-FLI1          142        16          MMP-2/9                                         0        99\n",
      "BCR-ABL1        108        18            EWS/FLI1           43         6          MMP-2/-9                                        0        68\n",
      "BCR/ABL1         34         6            EWS/FLI-1          12        11          MMP2/9                                          0        65\n",
      "Bcr/Abl          26         4            EWS::FLI1           9         2          MMP-2 and -9                                    0        40\n",
      "BCR::ABL1        10         3            EWSR1-FLI1          9         0          matrix metalloproteinase-2 and -9               0        16\n",
      "bcr/abl           8         6            EWSR1::FLI1         7         1          matrix metalloproteinase-2/9                    0        11\n",
      "BCR::ABL          5         0                                                     matrix metalloproteinase-2/-9                   0         9\n",
      "Bcr/abl           3         0                                                     matrix metalloproteinases 2 and 9               0         9\n",
      "BCR/abl           1         0                                                     matrix metalloproteinase (MMP)-2 and -9         0         7\n",
      "                                                                                  matrix metalloproteinase 2 and 9                0         6\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Through further manual investigation of the sentences, we have determined that:\n",
       "* FL1;EWSR1 & ABL1;BCR are fusion genes. They are correct TFs but must be discarded as TGs.\n",
       "* TG = MMP9;MMP2 entities indicate that the TF regulates both genes.\n",
       "   \n",
       "One TG sentence example for each case (first two will be discarded)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABL1;BCR:\t In this report, we show evidence that [TF] transcription factor stringently controls the expression of [TG], which can strategically be targeted by our novel RUNX inhibitor, Chb-M'.\n",
      "FLI1;EWSR1:\t [TF] knockdown showed a reduced cell growth and transcriptional activity of [TG].\n",
      "MMP2;MMP9:\t Finally, nimbolide suppressed the nuclear translocation of p65/p50 and DNA binding of [TF], which is an important transcription factor for controlling [TG] and VEGF gene expression.\n"
     ]
    }
   ],
   "source": [
    "### ENTITIES NORMALIZED TO +1 ID\n",
    "bold(\"Entities normalized to +1 ID\")\n",
    "m = (TRI_df['TF Symbol'].str.upper().str.contains(';')) | TRI_df['TG Symbol'].str.upper().str.contains(';')\n",
    "md(f\"{m.sum()} ({m.sum() / len(m):.2%}) entities are normalized to more than 1 ID.<br>We revise those that appear more than 100 times further:\")\n",
    "m_template = \"TRI_df['{T} Symbol'].str.contains(';')\"\n",
    "print_symbol_counts_side_by_side(m_template, max_counts=15)\n",
    "\n",
    "dubious_pairs = [('ABL1', 'BCR'), ('FLI1','EWSR1'), ('MMP2','MMP9')]\n",
    "\n",
    "md(f'From those, 3 seem suspicious and are investigated further: {\", \".join((\";\".join(p) for p in dubious_pairs))}')\n",
    "\n",
    "print_dubious_pairs_TFTGcounts_side_by_side(dubious_pairs)\n",
    "md('''\\\n",
    "Through further manual investigation of the sentences, we have determined that:\n",
    "* FL1;EWSR1 & ABL1;BCR are fusion genes. They are correct TFs but must be discarded as TGs.\n",
    "* TG = MMP9;MMP2 entities indicate that the TF regulates both genes.\n",
    "   \n",
    "One TG sentence example for each case (first two will be discarded)\n",
    "''')\n",
    "\n",
    "for p in dubious_pairs:\n",
    "    pairs = [';'.join(p) for p in itertools.permutations(p)]\n",
    "    print(f\"{pairs[0]}:\\t\", TRI_df[TRI_df['TG Symbol'].isin(pairs)].sample(1)['Sentence'].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sets of sentences to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_to_check_path = config['data_p'] + 'validation/sents_to_check.tsv'\n",
    "sents_to_check_2_path = config['data_p'] + 'validation/sents_to_check_2.tsv'\n",
    "sents_to_check_of_path = config['data_p'] + 'validation/sents_to_check_of.tsv'\n",
    "sents_to_check_CDX_path = config['data_p'] + 'validation/sents_to_check_CDX.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>Sentences with 'p21' not normalised to 'CDKN1A':</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF   TF Symbol                           TG   TG Symbol\n",
      "p21  Tceal1       14                     p21  H3P16        4542\n",
      "     TCEAL1        1                          Kras          232\n",
      "                                              TCEAL1         29\n",
      "                                              Tceal1         25\n",
      "                                              Tpt1            1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Sentences with 'p53' not normalised to 'TP53':</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF   TF Symbol                           TG   TG Symbol\n",
      "p53  Trp53        195                    p53  Trp53-ps     1615\n",
      "                                              p53-ps        240\n",
      "                                              Trp53          90\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Sentences with MDM2-TP53 pairs must be removed: they're always a PPI.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Symbol  TG Symbol\n",
      "MDM2       TP53         1609\n",
      "Mdm2       TP53           23\n",
      "MDM2;MDM4  TP53            3\n",
      "MDM2       TP53BP2         1 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Sentences with 'MET' not normalised to 'MET':</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF   TF Symbol                           TG   TG Symbol\n",
      "MET  SLTM         494                    MET  SLTM         392\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Sentences with 'CD\\d' :</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF            TF Symbol                  TG             TG Symbol\n",
      "CD34          CD34         1704          CD34           CD34         206\n",
      "              Cd34          113          CD4            CD4          162\n",
      "CD34(         Cd34            5                         Cd4           82\n",
      "              CD34            2          Cd4            CD4           46\n",
      "CD4CD25FoxP3  FOXP3           2          CD34           Cd34          40\n",
      "cd34          CD34            2          CD74           CD74          27\n",
      "CD34+         CD34            1          CD8alpha       Cd8a          17\n",
      "CD34Exo       CD34            1          Cd4            Cd4           12\n",
      "CD34LC        CD34            1          Cd74           Cd74           9\n",
      "CD34brCD38    CD34;CD38       1          CD74           Cd74           8\n",
      "CD4.Ezh2      Cd4;Ezh2        1          Cd8a           CD8A           5\n",
      "                                         CD8a           Cd8a           5\n",
      "                                         CD8alpha       CD8A           4\n",
      "                                         CD8A           CD8A           3\n",
      "                                         CD8B           CD8B           2\n",
      "                                         CD8beta        CD8B           2\n",
      "                                         CD8a           CD8A           1\n",
      "                                         CD8b           Cd8b1          1\n",
      "                                         Cd34           CD34           1\n",
      "                                                        Cd34           1\n",
      "                                         CD4;IFN-gamma  Cd4;Ifng       1\n",
      "                                         CD4 receptor   CD4            1\n",
      "                                         Cd74           CD74           1\n",
      "                                         CD34EGFP       Cd34           1\n",
      "                                         CD34 antigen   CD34           1\n",
      "                                         Cd8alpha       CD8A           1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Entities normalised to +1 IDs: ABL1;BCR</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF        TF Symbol                      TG       TG Symbol\n",
      "BCR/ABL   ABL1;BCR     126               BCR/ABL  ABL1;BCR     9\n",
      "Bcr/Abl   ABL1;BCR      26               bcr/abl  ABL1;BCR     6\n",
      "bcr/abl   ABL1;BCR       8               Bcr/Abl  ABL1;BCR     4\n",
      "BCR::ABL  ABL1;BCR       5               \n",
      "Bcr/abl   ABL1;BCR       3               \n",
      "BCR/abl   ABL1;BCR       1               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "Autoregulation:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "4.8% of sentences show autoregulation: TF & TG are the same. Most popular:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Symbol  TG Symbol\n",
      "TP53       TP53         1978\n",
      "VEGFA      VEGFA        1108\n",
      "EGFR       EGFR          852\n",
      "MYC        MYC           719\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "Translation instead of gene expression</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7479 sentences contain 'translat' in them and should be checked\n"
     ]
    }
   ],
   "source": [
    "# PREPARE SENTECES TO CHECK\n",
    "def add_to_sents_to_check(sents_to_check: list, m_template: str, issue: str) -> list:\n",
    "    T = 'TF'\n",
    "    m = eval(m_template)\n",
    "    T = 'TG'\n",
    "    m |= eval(m_template)\n",
    "    df_m = TRI_df[m].copy()\n",
    "    df_m['issue'] = issue\n",
    "    sents_to_check.append(df_m)\n",
    "\n",
    "sents_to_check = []\n",
    "\n",
    "t = \"Sentences with 'p21' not normalised to 'CDKN1A':\"\n",
    "m_template = \"(TRI_df[f'{T}'] == 'p21') & (TRI_df[f'{T} Symbol'].str.upper() != 'CDKN1A')\"\n",
    "print_TF_TG_counts_side_by_side(t, m_template)\n",
    "add_to_sents_to_check(sents_to_check, m_template, 'p21-CDKN1A')\n",
    "\n",
    "\n",
    "t = \"Sentences with 'p53' not normalised to 'TP53':\"\n",
    "m_template = \"(TRI_df[f'{T}'] == 'p53') & (TRI_df[f'{T} Symbol'].str.upper() != 'TP53')\"\n",
    "print_TF_TG_counts_side_by_side(t, m_template)\n",
    "add_to_sents_to_check(sents_to_check, m_template, 'p53-TP53')\n",
    "\n",
    "bold(f\"Sentences with MDM2-TP53 pairs must be removed: they're always a PPI.\")\n",
    "m = TRI_df['TF Symbol'].str.upper().str.contains('MDM2')\n",
    "m &= TRI_df['TG Symbol'].str.upper().str.contains('TP53')\n",
    "print(TRI_df[m][['TF Symbol', 'TG Symbol']].value_counts().to_string(), '\\n')\n",
    "\n",
    "\n",
    "t = \"Sentences with 'MET' not normalised to 'MET':\"\n",
    "m_template = \"(TRI_df[f'{T}'] == 'MET') & (TRI_df[f'{T} Symbol'].str.upper() != 'MET')\"\n",
    "print_TF_TG_counts_side_by_side(t, m_template)\n",
    "add_to_sents_to_check(sents_to_check, m_template, 'MET')\n",
    "\n",
    "\n",
    "t = \"Sentences with 'CD\\d' :\"\n",
    "m_template = \"TRI_df[f'{T}'].str.upper().str.contains(r'^CD(?:4|8A|8B|74|34)(?!\\d)')\"\n",
    "print_TF_TG_counts_side_by_side(t, m_template)\n",
    "add_to_sents_to_check(sents_to_check, m_template, 'CD*')\n",
    "\n",
    "# Joined NCBI IDs to check\n",
    "t = \"Entities normalised to +1 IDs: ABL1;BCR\"\n",
    "m_template = \"TRI_df[f'{T} Symbol'] == 'ABL1;BCR'\"\n",
    "print_TF_TG_counts_side_by_side(t, m_template)\n",
    "add_to_sents_to_check(sents_to_check, m_template, 'ABL1;BCR')\n",
    "\n",
    "\n",
    "bold(f\"\\nAutoregulation:\")\n",
    "m = TRI_df['TF Symbol'].str.upper() == TRI_df['TG Symbol'].str.upper()\n",
    "md(f\"{m.sum() / len(TRI_df):.1%} of sentences show autoregulation: TF & TG are the same. Most popular:\")\n",
    "print(TRI_df[m][['TF Symbol', 'TG Symbol']].value_counts()[:4].to_string())\n",
    "\n",
    "# Those are potentially commonly wrong. Prepare a set of 300 sentences for validation purposes\n",
    "df_m = TRI_df[m].sample(n=300)\n",
    "df_m['issue'] = 'Autoregulation'\n",
    "sents_to_check.append(df_m)\n",
    "\n",
    "bold('\\nTranslation instead of gene expression')\n",
    "m = TRI_df['Sentence'].str.lower().str.contains('translat')\n",
    "print(f\"{m.sum()} sentences contain 'translat' in them and should be checked\")\n",
    "df_m = TRI_df[m].sample(n=100)\n",
    "df_m['issue'] = 'Translate'\n",
    "sents_to_check.append(df_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE EXCEL\n",
    "def extract_context(sentence, token='[TF]', window=4, how='both'):\n",
    "    '''Get the last and next 4 words from the token'''\n",
    "    # Split the sentence by spaces\n",
    "    words = sentence.split()\n",
    "\n",
    "    # Find the index of the word that contains '[TF]' or its variations\n",
    "    index = [i for i, word in enumerate(words) if token in word][0]\n",
    "\n",
    "    # Extract the 4 words before and after the token, handling boundaries\n",
    "    start = max(0, index) if how=='right' else max(0, index - window)\n",
    "    end = min(len(words), index + 1) if how=='left' else min(len(words), index + window + 1)\n",
    "\n",
    "\n",
    "    # Join the extracted context words back into a string\n",
    "    return ' '.join(['...'] + words[start:end] + ['...'])\n",
    "\n",
    "sents_to_check = pd.concat(sents_to_check)\n",
    "\n",
    "cols_to_keep = ['issue', 'TF', 'TF Symbol', 'TG', 'TG Symbol', 'Sentence',  '#SentenceID', \n",
    "                'TF Id', 'TG Id', 'MoR', 'TF TaxID',  'TG TaxID', 'TF_type', 'issue']\n",
    "\n",
    "sents_to_check = sents_to_check[cols_to_keep]\n",
    "\n",
    "for T in ('TF', 'TG'):\n",
    "    sents_to_check[f'{T}_context'] = sents_to_check['Sentence'].apply(lambda x: extract_context(x, token=f'{T}'))\n",
    "    sents_to_check[f'{T}_left_context'] = sents_to_check['Sentence'].apply(lambda x: extract_context(x, token=f'{T}', how='left'))\n",
    "    sents_to_check[f'{T}_right_context'] = sents_to_check['Sentence'].apply(lambda x: extract_context(x, token=f'{T}', how='right'))\n",
    "\n",
    "sents_to_check.to_csv(sents_to_check_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "dbTF Autoregulation:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2.1% of sentences show autoregulation: TF & TG are the same. Most popular:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Symbol  TG Symbol\n",
      "TP53       TP53         1978\n",
      "MYC        MYC           719\n",
      "HIF1A      HIF1A         478\n",
      "ESR1       ESR1          434\n",
      "results saved in ../../data/validation/sents_to_check_2.tsv\n"
     ]
    }
   ],
   "source": [
    "# PREPARE 2nd SET OF SENTENCES TO CHECK\n",
    "bold(f\"\\ndbTF Autoregulation:\")\n",
    "\n",
    "# Get a set of 300 sentences to check with autoregulation (dbTF)#\n",
    "# Previous set contained a lot of coTF sentences. We want to check the number of false positives in dbTF-specific autoregulation.\n",
    "m = TRI_df['TF Symbol'].str.upper() == TRI_df['TG Symbol'].str.upper()\n",
    "m &= TRI_df['TF_type'] == 'dbTF'\n",
    "md(f\"{m.sum() / len(TRI_df):.1%} of sentences show autoregulation: TF & TG are the same. Most popular:\")\n",
    "print(TRI_df[m][['TF Symbol', 'TG Symbol']].value_counts()[:4].to_string())\n",
    "df_m = TRI_df[m].sample(n=300)\n",
    "df_m['issue'] = 'dbTF_autoregulation'\n",
    "\n",
    "# We will only check dbTF autoregulation\n",
    "sents_to_check_2 = df_m\n",
    "tab_cols = (\"issue\t#SentenceID\tTF\tTF Symbol\tTG\tTG Symbol\tSentence\tTF Id\tTG Id\tTF offset\tGene offset\tTRI score\tValid\tMoR scores\tMoR\tPMID\tPMID+Sent+TRI_Id\tTF TaxID\tTG TaxID\tTF_type\")\n",
    "sents_to_check_2 = sents_to_check_2[tab_cols.split(\"\\t\")]\n",
    "\n",
    "sents_to_check_2.to_csv(sents_to_check_2_path, sep='\\t', index=False)\n",
    "print(f\"results saved in {sents_to_check_2_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TG        TG Symbol\n",
       "CD44      CD44         944\n",
       "CD133     PROM1        350\n",
       "CD40      CD40         306\n",
       "CD86      CD86         232\n",
       "          Cd86         180\n",
       "                      ... \n",
       "CD40L.    Cd40lg         1\n",
       "CD41b     ITGA2B         1\n",
       "CD42a     GP9            1\n",
       "CD44High  Cd44           1\n",
       "cd59      CD59           1\n",
       "Name: count, Length: 491, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "9319"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PREPARE 3rd SET OF SENTENCES TO CHECK\n",
    "m = final_TRI_df['TG'] == 'of'\n",
    "m |= final_TRI_df['TF'] == 'of'\n",
    "sents_to_check_of = final_TRI_df[m].sort_values(by=['TF', 'TG'])\n",
    "sents_to_check_of.to_csv(sents_to_check_of_path, sep='\\t', index=False)\n",
    "\n",
    "m = final_TRI_df['TG'].str.upper().str.contains(\"^CD[0-9]\")\n",
    "m &= final_TRI_df['Sentence'].str.contains(\"\\[TG\\] ?\")\n",
    "\n",
    "display(final_TRI_df[m][['TG', 'TG Symbol']].value_counts())\n",
    "m.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = final_TRI_df['TG'].str.upper().str.contains(\"^CD[0-9]\")\n",
    "final_TRI_df[m].to_csv(sents_to_check_CDX_path, sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".general_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
