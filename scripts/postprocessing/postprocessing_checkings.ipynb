{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing of the ExTRI2 pipeline results to create the ExTRI2 resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was used to determine the rules to use for renormalisation & discard of sentences. Sentences were extracted from the ExTRI2 resource and checked manually, to determine how to handle each category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3>Table of contents</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "[Postprocessing of the ExTRI2 pipeline results to create the ExTRI2 resource](#Postprocessing-of-the-ExTRI2-pipeline-results-to-create-the-ExTRI2-resource)\n",
       "- [Run Main](#Run-Main)\n",
       "- [Setup](#Setup)\n",
       "- [Postprocessing](#Postprocessing)\n",
       "  - [AP1 & NFKB](#AP1-&-NFKB)\n",
       "  - [Initial exploration](#Initial-exploration)\n",
       "  - [Sentences to check](#Sentences-to-check)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "__import__('sys').path.append('../common/'); __import__('notebook_utils').table_of_contents('postprocessing_checkings.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "This notebook will now only be used for the normalisation of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run postprocessing.py\n",
    "Self-contained cell to run postprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### POSTPROCESSING valid_df\n",
      "We got 6706 different TFs and 26196 different TGs from our valid sentences\n",
      "Retrieving from Entrez...\n",
      "\n",
      "4967 sentences are dropped as their TG is not normalised\n",
      "\n",
      "38287 rows (4.23%) will have its TF renormalized to NFKB\n",
      "6330 rows (0.70%) will be dropped as the TG corresponds to NFKB\n",
      "9003 rows (1.00%) will have its TF renormalized to AP1\n",
      "1858 rows (0.21%) will be dropped as the TG corresponds to AP1\n",
      "Breakdown by NCBI Symbol saved in ../../data/postprocessing/tables/AP1_NFKB_breakdown.tsv\n",
      "Number of renormalized sentences and normalization:\n",
      "4829\t0.54%\tp21 is normalized to CDKN1A\n",
      "1921\t0.21%\tp53-ps is normalized to its respective p53 symbol\n",
      "\n",
      "Number of discarded sentences and percentage from total (896863 sentences) and reasoning:\n",
      "2581\t0.29%\tTheir TF contains -AS[1-3]\n",
      "673\t0.08%\tTheir TF are circRNAs\n",
      "952\t0.11%\tTheir TF (NLRP3) is followed by inflammasome but normalised to NLRP3\n",
      "1876\t0.21%\tTheir TG (NLRP3) is followed by inflammasome but normalised to NLRP3\n",
      "82\t0.01%\tThe TG is a fusion gene\n",
      "1088\t0.12%\tTheir TG is followed by pathway/signalling/axis/program\n",
      "2037\t0.23%\tMDM2-TP53 pair, which is always a PPI\n",
      "1552\t0.17%\tTF is CD(4|8A|8B|74|34) positive (indicative of a cell, not a gene)\n",
      "22\t0.00%\tTG is CD(4|8A|8B|74|34) positive (indicative of a cell, not a gene)\n",
      "724\t0.08%\tPreposition of incorrectly identified as a gene\n",
      "7391\t0.82%\tThey contain translation in them (found to only be correct 40% of the time)\n",
      "41649\t4.64%\tThey are autoregulations (found to only be correct ~10% of the time)\n",
      "\n",
      "We get ortholog info for 24968/25602 Gene IDs\n",
      "\n",
      "\n",
      "### POSTPROCESSING nonvalid_df\n",
      "We got 4284 different TFs and 10938 different TGs from our valid sentences\n",
      "Retrieving from Entrez...\n",
      "\n",
      "476 sentences are dropped as their TG is not normalised\n",
      "\n",
      "2065 rows (2.79%) will have its TF renormalized to NFKB\n",
      "1221 rows (1.65%) will be dropped as the TG corresponds to NFKB\n",
      "315 rows (0.43%) will have its TF renormalized to AP1\n",
      "266 rows (0.36%) will be dropped as the TG corresponds to AP1\n",
      "Breakdown by NCBI Symbol saved in ../../data/postprocessing/tables/AP1_NFKB_breakdown.tsv\n",
      "Number of renormalized sentences and normalization:\n",
      "83\t0.11%\tp21 is normalized to CDKN1A\n",
      "126\t0.17%\tp53-ps is normalized to its respective p53 symbol\n",
      "\n",
      "Number of discarded sentences and percentage from total (72655 sentences) and reasoning:\n",
      "61\t0.08%\tTheir TF contains -AS[1-3]\n",
      "12\t0.02%\tTheir TF are circRNAs\n",
      "99\t0.14%\tTheir TF (NLRP3) is followed by inflammasome but normalised to NLRP3\n",
      "23\t0.03%\tTheir TG (NLRP3) is followed by inflammasome but normalised to NLRP3\n",
      "29\t0.04%\tThe TG is a fusion gene\n",
      "1555\t2.14%\tTheir TG is followed by pathway/signalling/axis/program\n",
      "82\t0.11%\tMDM2-TP53 pair, which is always a PPI\n",
      "311\t0.43%\tTF is CD(4|8A|8B|74|34) positive (indicative of a cell, not a gene)\n",
      "245\t0.34%\tTG is CD(4|8A|8B|74|34) positive (indicative of a cell, not a gene)\n",
      "82\t0.11%\tPreposition of incorrectly identified as a gene\n",
      "667\t0.92%\tThey contain translation in them (found to only be correct 40% of the time)\n",
      "15011\t20.66%\tThey are autoregulations (found to only be correct ~10% of the time)\n",
      "\n",
      "We get ortholog info for 11122/11282 Gene IDs\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../common')\n",
    "sys.path.append('../../')\n",
    "from scripts.postprocessing.postprocessing import *\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "## Custom functions\n",
    "import sys\n",
    "\n",
    "sys.path.append('../common')\n",
    "sys.path.append('../../')\n",
    "\n",
    "from notebook_utils import table_of_contents, table_from_dict, h3, h4, h5, md, bold\n",
    "from renormalisations import *\n",
    "from postprocessing import *\n",
    "pd.set_option('display.max_colwidth', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkings on the processed final valid df\n",
    "config = load_config()\n",
    "f_valid_df = load_df(config['final_ExTRI2_p'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AP1 & NFKB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AP1 and NFKB are dimers, and as such don't have neither a NCBI EntrezID, nor a HGNC symbol. PubTator normalizes them to one of their monomers. Therefore, in `postprocessing.py`, we\n",
    "* Find all dimers incorrectly normalized to monomers using regex\n",
    "* Change the TF metadata to AP1/NFKB. Delete the TG instances (a TG can't be a dimer)\n",
    "* Save a summary of the results and affected sentences in `data/postprocessing/tables`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### POSTPROCESSING valid_df\n",
      "We got 6706 different TFs and 26196 different TGs from our valid sentences\n",
      "Retrieving from Entrez...\n",
      "\n",
      "4967 sentences are dropped as their TG is not normalised\n",
      "\n",
      "38287 rows (4.23%) will have its TF renormalized to NFKB\n",
      "6330 rows (0.70%) will be dropped as the TG corresponds to NFKB\n",
      "9003 rows (1.00%) will have its TF renormalized to AP1\n",
      "1858 rows (0.21%) will be dropped as the TG corresponds to AP1\n",
      "Breakdown by NCBI Symbol saved in ../../data/postprocessing/tables/AP1_NFKB_breakdown.tsv\n"
     ]
    }
   ],
   "source": [
    "# POSTPROCESSING BEFORE RENORMALISATION & DISCARDING WERE IMPLEMENTED\n",
    "def half_postprocess(ExTRI2_df: pd.DataFrame, valid_sents: bool, config: dict) -> pd.DataFrame:\n",
    "    '''same as postprocess but before the renormalisation & discarding'''\n",
    "\n",
    "    df_type = 'valid' if valid_sents else 'nonvalid'\n",
    "    print(f'### POSTPROCESSING {df_type}_df')\n",
    "\n",
    "    # Retrieve Symbol & TaxID from Entrez\n",
    "    save_Symbol_TaxID_dict(ExTRI2_df, config[f'EntrezID_to_Symbol_{df_type}_p'])\n",
    "\n",
    "    # Filter & add metadata\n",
    "    if valid_sents:\n",
    "        remove_duplicates(ExTRI2_df)\n",
    "    ExTRI2_df = add_symbols_TaxID(ExTRI2_df, config[f'EntrezID_to_Symbol_{df_type}_p'])\n",
    "    add_TF_type(ExTRI2_df, config)\n",
    "    ExTRI2_df = drop_GTFs(ExTRI2_df)\n",
    "    ExTRI2_df = remove_other_species(ExTRI2_df, TaxID)\n",
    "\n",
    "    # Fix AP1 & NFKB normalisations\n",
    "    ExTRI2_df = fix_NFKB_AP1(ExTRI2_df, config)\n",
    "\n",
    "    return ExTRI2_df\n",
    "\n",
    "config = load_config()\n",
    "\n",
    "# Load raw dataframe\n",
    "valid_df = load_preprocess_df(config['raw_valid_p'])\n",
    "\n",
    "# Postprocess (without renormalisation/discarding)\n",
    "valid_df = half_postprocess(valid_df, valid_sents=True,  config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST-PROCESSING FUNCTIONS\n",
    "def print_symbol_counts_side_by_side(m_template, max_counts = 10):\n",
    "    'Print symbol counts of m_template for TF & TG'\n",
    "    results = []\n",
    "    for T in ('TF', 'TG'):\n",
    "        m = eval(m_template.replace('{T}', T))\n",
    "        T_lines = valid_df[m][f'{T} Symbol'].value_counts()[:max_counts].to_string().split('\\n')\n",
    "        results.append(T_lines)\n",
    "\n",
    "    # Print the two tables side by side\n",
    "    for tf_line, tg_line in zip(*results):\n",
    "        print(f\"{tf_line:<35} {tg_line}\")\n",
    "\n",
    "def print_dubious_pairs_TFTGcounts_side_by_side(dubious_pairs):\n",
    "    '''Print counts of TF&TG of 3 different symbols side by side'''\n",
    "    all_tables = []\n",
    "    for p in dubious_pairs:\n",
    "        results = []\n",
    "        for T in ('TF', 'TG'):\n",
    "            m = valid_df[f'{T} Symbol'].isin([';'.join((p[0], p[1])), ';'.join((p[1], p[0]))])\n",
    "            T_counts = valid_df[m][f'{T}'].value_counts().rename(f'{T} count')[:10]\n",
    "            results.append(T_counts)\n",
    "\n",
    "        # Merge the TF and TG counts on the same index\n",
    "        merged_df = pd.concat(results, axis=1).fillna(0).astype(int)\n",
    "        all_tables.append(merged_df)\n",
    "\n",
    "    # Convert each table to a string and split by lines\n",
    "    table_strings = [table.to_string().split('\\n') for table in all_tables]\n",
    "\n",
    "    # Use itertools.zip_longest to handle tables with different lengths\n",
    "    for lines in itertools.zip_longest(*table_strings, fillvalue=''):\n",
    "        # Print each line of the three tables side by side\n",
    "        print(f\"{lines[0]:<40} {lines[1]:<40} {lines[2]}\")\n",
    "\n",
    "def print_TF_TG_counts_side_by_side(title, m_template, sep=40):\n",
    "    bold(title)\n",
    "    counts = []\n",
    "    for T in ('TF', 'TG'):\n",
    "        m = eval(m_template)\n",
    "        T_lines = valid_df[m][[f'{T}', f'{T} Symbol']].value_counts().to_string().split('\\n')\n",
    "        counts.append(T_lines)\n",
    "    \n",
    "    for tf_line, tg_line in itertools.zip_longest(*counts, fillvalue=''):\n",
    "        print(f\"{tf_line:<{sep}} {tg_line}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>Entities normalized to +1 ID</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "25645 (3%) entities are normalized to more than 1 ID.<br>We revise those that appear more than 100 times further:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Symbol                           TG Symbol\n",
      "MAPK3;MAPK1            5347         MAPK3;MAPK1       1425\n",
      "Mapk3;Mapk1            2798         Mapk3;Mapk1        618\n",
      "MAP2K1;MAP2K2           717         MMP2;MMP9          395\n",
      "SMAD2;SMAD3             481         SMAD2;SMAD3        253\n",
      "MAPK8;MAPK9             311         Smad2;Smad3        171\n",
      "Map2k1;Map2k2           303         CDK4;CDK6           96\n",
      "Smad2;Smad3             222         Mmp2;Mmp9           89\n",
      "EWSR1;FLI1              216         HSD11B1;RNU1-1      66\n",
      "ABL1;BCR                169         MIR143;MIR145       66\n",
      "BCR;ABL1                152         CASP3;CASP7         63\n",
      "CREBBP;EP300            147         MAP2K1;MAP2K2       57\n",
      "OIP5-AS1;OIP5;PTGDR     144         NKX2-5;NKX3-1       54\n",
      "SMAD1;SMAD5;SMAD9       128         CASP3;CASP9         42\n",
      "MAPK1;MAPK3             111         Ifna;Ifnb1          38\n",
      "HDAC1;HDAC2             104         EWSR1;FLI1          36\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "From those, 3 seem suspicious and are investigated further: ABL1;BCR, FLI1;EWSR1, MMP2;MMP9"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           TF count  TG count                         TF count  TG count                                                   TF count  TG count\n",
      "BCR/ABL         126         9            EWS-FLI1          142        16          MMP-2/9                                         0        99\n",
      "BCR-ABL1        108        18            EWS/FLI1           43         6          MMP-2/-9                                        0        68\n",
      "BCR/ABL1         34         6            EWS/FLI-1          12        11          MMP2/9                                          0        65\n",
      "Bcr/Abl          26         4            EWS::FLI1           9         2          MMP-2 and -9                                    0        40\n",
      "BCR::ABL1        10         3            EWSR1-FLI1          9         0          matrix metalloproteinase-2 and -9               0        16\n",
      "bcr/abl           8         6            EWSR1::FLI1         7         1          matrix metalloproteinase-2/9                    0        11\n",
      "BCR::ABL          5         0                                                     matrix metalloproteinases 2 and 9               0         9\n",
      "Bcr/abl           3         0                                                     matrix metalloproteinase-2/-9                   0         9\n",
      "BCR/abl           1         0                                                     matrix metalloproteinase (MMP)-2 and -9         0         7\n",
      "                                                                                  matrix metalloproteinase 2 and 9                0         6\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Through further manual investigation of the sentences, we have determined that:\n",
       "* FL1;EWSR1 & ABL1;BCR are fusion genes. They are correct TFs but must be discarded as TGs.\n",
       "* TG = MMP9;MMP2 entities indicate that the TF regulates both genes.\n",
       "   \n",
       "One TG sentence example for each case (first two will be discarded)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABL1;BCR:\t These findings suggest that interfere PI3K/[TF] signal pathway via down-regulating the expression of [TG] mRNA is implicated in the effect of 2-ME2 on K562 cells.\n",
      "FLI1;EWSR1:\t Disrupted splicing of the [TF] transcript alters [TG] protein expression and EWS-FLI1-driven expression.\n",
      "MMP2;MMP9:\t Taken together, our data suggest that SHQA inhibit TNFalpha-induced [TG] expression and age-related inflammation by suppressing AP-1 and NF-kappaB pathway via [TF].\n"
     ]
    }
   ],
   "source": [
    "### ENTITIES NORMALIZED TO +1 ID\n",
    "bold(\"Entities normalized to +1 ID\")\n",
    "m = (valid_df['TF Symbol'].str.upper().str.contains(';')) | valid_df['TG Symbol'].str.upper().str.contains(';')\n",
    "md(f\"{m.sum()} ({m.sum() / len(m):.0%}) entities are normalized to more than 1 ID.<br>We revise those that appear more than 100 times further:\")\n",
    "m_template = \"valid_df['{T} Symbol'].str.contains(';')\"\n",
    "print_symbol_counts_side_by_side(m_template, max_counts=15)\n",
    "\n",
    "dubious_pairs = [('ABL1', 'BCR'), ('FLI1','EWSR1'), ('MMP2','MMP9')]\n",
    "\n",
    "md(f'From those, 3 seem suspicious and are investigated further: {\", \".join((\";\".join(p) for p in dubious_pairs))}')\n",
    "\n",
    "print_dubious_pairs_TFTGcounts_side_by_side(dubious_pairs)\n",
    "md('''\\\n",
    "Through further manual investigation of the sentences, we have determined that:\n",
    "* FL1;EWSR1 & ABL1;BCR are fusion genes. They are correct TFs but must be discarded as TGs.\n",
    "* TG = MMP9;MMP2 entities indicate that the TF regulates both genes.\n",
    "   \n",
    "One TG sentence example for each case (first two will be discarded)\n",
    "''')\n",
    "\n",
    "for p in dubious_pairs:\n",
    "    pairs = [';'.join(p) for p in itertools.permutations(p)]\n",
    "    print(f\"{pairs[0]}:\\t\", valid_df[valid_df['TG Symbol'].isin(pairs)].sample(1)['Sentence'].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_to_check_path = config['data_p'] + 'validation/sents_to_check.tsv'\n",
    "sents_to_check_2_path = config['data_p'] + 'validation/sents_to_check_2.tsv'\n",
    "sents_to_check_of_path = config['data_p'] + 'validation/sents_to_check_of.tsv'\n",
    "sents_to_check_CDX_path = config['data_p'] + 'validation/sents_to_check_CDX.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>Sentences with 'p21' not normalised to 'CDKN1A':</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF   TF Symbol                           TG   TG Symbol\n",
      "p21  Tceal1       14                     p21  H3P16        4542\n",
      "     TCEAL1        1                          Kras          232\n",
      "                                              TCEAL1         29\n",
      "                                              Tceal1         25\n",
      "                                              Tpt1            1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Sentences with 'p53' not normalised to 'TP53':</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF   TF Symbol                           TG   TG Symbol\n",
      "p53  Trp53        195                    p53  Trp53-ps     1615\n",
      "                                              p53-ps        240\n",
      "                                              Trp53          90\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Sentences with MDM2-TP53 pairs must be removed: they're always a PPI.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Symbol  TG Symbol\n",
      "MDM2       TP53         1609\n",
      "Mdm2       TP53           23\n",
      "MDM2;MDM4  TP53            3\n",
      "MDM2       TP53BP2         1 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Sentences with 'MET' not normalised to 'MET':</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF   TF Symbol                           TG   TG Symbol\n",
      "MET  SLTM         494                    MET  SLTM         392\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Sentences with 'CD\\d' :</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF            TF Symbol                  TG             TG Symbol\n",
      "CD34          CD34         1704          CD34           CD34         206\n",
      "              Cd34          113          CD4            CD4          162\n",
      "CD34(         Cd34            5                         Cd4           82\n",
      "              CD34            2          Cd4            CD4           46\n",
      "cd34          CD34            2          CD34           Cd34          40\n",
      "CD4CD25FoxP3  FOXP3           2          CD74           CD74          27\n",
      "CD34+         CD34            1          CD8alpha       Cd8a          17\n",
      "CD34LC        CD34            1          Cd4            Cd4           12\n",
      "CD34Exo       CD34            1          Cd74           Cd74           9\n",
      "CD4.Ezh2      Cd4;Ezh2        1          CD74           Cd74           8\n",
      "CD34brCD38    CD34;CD38       1          Cd8a           CD8A           5\n",
      "                                         CD8a           Cd8a           5\n",
      "                                         CD8alpha       CD8A           4\n",
      "                                         CD8A           CD8A           3\n",
      "                                         CD8B           CD8B           2\n",
      "                                         CD8beta        CD8B           2\n",
      "                                         CD34EGFP       Cd34           1\n",
      "                                         CD34 antigen   CD34           1\n",
      "                                         CD4 receptor   CD4            1\n",
      "                                         CD4;IFN-gamma  Cd4;Ifng       1\n",
      "                                         CD8b           Cd8b1          1\n",
      "                                         CD8a           CD8A           1\n",
      "                                         Cd34           CD34           1\n",
      "                                                        Cd34           1\n",
      "                                         Cd74           CD74           1\n",
      "                                         Cd8alpha       CD8A           1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Entities normalised to +1 IDs: ABL1;BCR</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF        TF Symbol                      TG       TG Symbol\n",
      "BCR/ABL   ABL1;BCR     126               BCR/ABL  ABL1;BCR     9\n",
      "Bcr/Abl   ABL1;BCR      26               bcr/abl  ABL1;BCR     6\n",
      "bcr/abl   ABL1;BCR       8               Bcr/Abl  ABL1;BCR     4\n",
      "BCR::ABL  ABL1;BCR       5               \n",
      "Bcr/abl   ABL1;BCR       3               \n",
      "BCR/abl   ABL1;BCR       1               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "Autoregulation:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "4.8% of sentences show autoregulation: TF & TG are the same. Most popular:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Symbol  TG Symbol\n",
      "TP53       TP53         1978\n",
      "VEGFA      VEGFA        1108\n",
      "EGFR       EGFR          852\n",
      "MYC        MYC           719\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "Translation instead of gene expression</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7479 sentences contain 'translat' in them and should be checked\n"
     ]
    }
   ],
   "source": [
    "# PREPARE SENTECES TO CHECK\n",
    "def add_to_sents_to_check(sents_to_check: list, m_template: str, issue: str) -> list:\n",
    "    T = 'TF'\n",
    "    m = eval(m_template)\n",
    "    T = 'TG'\n",
    "    m |= eval(m_template)\n",
    "    df_m = valid_df[m].copy()\n",
    "    df_m['issue'] = issue\n",
    "    sents_to_check.append(df_m)\n",
    "\n",
    "sents_to_check = []\n",
    "\n",
    "t = \"Sentences with 'p21' not normalised to 'CDKN1A':\"\n",
    "m_template = \"(valid_df[f'{T}'] == 'p21') & (valid_df[f'{T} Symbol'].str.upper() != 'CDKN1A')\"\n",
    "print_TF_TG_counts_side_by_side(t, m_template)\n",
    "add_to_sents_to_check(sents_to_check, m_template, 'p21-CDKN1A')\n",
    "\n",
    "\n",
    "t = \"Sentences with 'p53' not normalised to 'TP53':\"\n",
    "m_template = \"(valid_df[f'{T}'] == 'p53') & (valid_df[f'{T} Symbol'].str.upper() != 'TP53')\"\n",
    "print_TF_TG_counts_side_by_side(t, m_template)\n",
    "add_to_sents_to_check(sents_to_check, m_template, 'p53-TP53')\n",
    "\n",
    "bold(f\"Sentences with MDM2-TP53 pairs must be removed: they're always a PPI.\")\n",
    "m = valid_df['TF Symbol'].str.upper().str.contains('MDM2')\n",
    "m &= valid_df['TG Symbol'].str.upper().str.contains('TP53')\n",
    "print(valid_df[m][['TF Symbol', 'TG Symbol']].value_counts().to_string(), '\\n')\n",
    "\n",
    "\n",
    "t = \"Sentences with 'MET' not normalised to 'MET':\"\n",
    "m_template = \"(valid_df[f'{T}'] == 'MET') & (valid_df[f'{T} Symbol'].str.upper() != 'MET')\"\n",
    "print_TF_TG_counts_side_by_side(t, m_template)\n",
    "add_to_sents_to_check(sents_to_check, m_template, 'MET')\n",
    "\n",
    "\n",
    "t = \"Sentences with 'CD\\d' :\"\n",
    "m_template = \"valid_df[f'{T}'].str.upper().str.contains(r'^CD(?:4|8A|8B|74|34)(?!\\d)')\"\n",
    "print_TF_TG_counts_side_by_side(t, m_template)\n",
    "add_to_sents_to_check(sents_to_check, m_template, 'CD*')\n",
    "\n",
    "# Joined NCBI IDs to check\n",
    "t = \"Entities normalised to +1 IDs: ABL1;BCR\"\n",
    "m_template = \"valid_df[f'{T} Symbol'] == 'ABL1;BCR'\"\n",
    "print_TF_TG_counts_side_by_side(t, m_template)\n",
    "add_to_sents_to_check(sents_to_check, m_template, 'ABL1;BCR')\n",
    "\n",
    "\n",
    "bold(f\"\\nAutoregulation:\")\n",
    "m = valid_df['TF Symbol'].str.upper() == valid_df['TG Symbol'].str.upper()\n",
    "md(f\"{m.sum() / len(valid_df):.1%} of sentences show autoregulation: TF & TG are the same. Most popular:\")\n",
    "print(valid_df[m][['TF Symbol', 'TG Symbol']].value_counts()[:4].to_string())\n",
    "\n",
    "# Those are potentially commonly wrong. Prepare a set of 300 sentences for validation purposes\n",
    "df_m = valid_df[m].sample(n=300)\n",
    "df_m['issue'] = 'Autoregulation'\n",
    "sents_to_check.append(df_m)\n",
    "\n",
    "bold('\\nTranslation instead of gene expression')\n",
    "m = valid_df['Sentence'].str.lower().str.contains('translat')\n",
    "print(f\"{m.sum()} sentences contain 'translat' in them and should be checked\")\n",
    "df_m = valid_df[m].sample(n=100)\n",
    "df_m['issue'] = 'Translate'\n",
    "sents_to_check.append(df_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE EXCEL\n",
    "def extract_context(sentence, token='[TF]', window=4, how='both'):\n",
    "    '''Get the last and next 4 words from the token'''\n",
    "    # Split the sentence by spaces\n",
    "    words = sentence.split()\n",
    "\n",
    "    # Find the index of the word that contains '[TF]' or its variations\n",
    "    index = [i for i, word in enumerate(words) if token in word][0]\n",
    "\n",
    "    # Extract the 4 words before and after the token, handling boundaries\n",
    "    start = max(0, index) if how=='right' else max(0, index - window)\n",
    "    end = min(len(words), index + 1) if how=='left' else min(len(words), index + window + 1)\n",
    "\n",
    "\n",
    "    # Join the extracted context words back into a string\n",
    "    return ' '.join(['...'] + words[start:end] + ['...'])\n",
    "\n",
    "sents_to_check = pd.concat(sents_to_check)\n",
    "\n",
    "cols_to_keep = ['issue', 'TF', 'TF Symbol', 'TG', 'TG Symbol', 'Sentence',  '#SentenceID', \n",
    "                'TF Id', 'TG Id', 'MoR', 'TF TaxID',  'TG TaxID', 'TF_type', 'issue']\n",
    "\n",
    "sents_to_check = sents_to_check[cols_to_keep]\n",
    "\n",
    "for T in ('TF', 'TG'):\n",
    "    sents_to_check[f'{T}_context'] = sents_to_check['Sentence'].apply(lambda x: extract_context(x, token=f'{T}'))\n",
    "    sents_to_check[f'{T}_left_context'] = sents_to_check['Sentence'].apply(lambda x: extract_context(x, token=f'{T}', how='left'))\n",
    "    sents_to_check[f'{T}_right_context'] = sents_to_check['Sentence'].apply(lambda x: extract_context(x, token=f'{T}', how='right'))\n",
    "\n",
    "sents_to_check.to_csv(sents_to_check_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "dbTF Autoregulation:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2.2% of sentences show autoregulation: TF & TG are the same. Most popular:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Symbol  TG Symbol\n",
      "TP53       TP53         1978\n",
      "MYC        MYC           719\n",
      "HIF1A      HIF1A         478\n",
      "ESR1       ESR1          434\n",
      "results saved in ../../data/validation/sents_to_check_2.tsv\n"
     ]
    }
   ],
   "source": [
    "# PREPARE 2nd SET OF SENTENCES TO CHECK\n",
    "bold(f\"\\ndbTF Autoregulation:\")\n",
    "\n",
    "# Get a set of 300 sentences to check with autoregulation (dbTF)#\n",
    "# Previous set contained a lot of coTF sentences. We want to check the number of false positives in dbTF-specific autoregulation.\n",
    "m = valid_df['TF Symbol'].str.upper() == valid_df['TG Symbol'].str.upper()\n",
    "m &= valid_df['TF_type'] == 'dbTF'\n",
    "md(f\"{m.sum() / len(valid_df):.1%} of sentences show autoregulation: TF & TG are the same. Most popular:\")\n",
    "print(valid_df[m][['TF Symbol', 'TG Symbol']].value_counts()[:4].to_string())\n",
    "df_m = valid_df[m].sample(n=300)\n",
    "df_m['issue'] = 'dbTF_autoregulation'\n",
    "\n",
    "# We will only check dbTF autoregulation\n",
    "sents_to_check_2 = df_m\n",
    "tab_cols = (\"issue\t#SentenceID\tTF\tTF Symbol\tTG\tTG Symbol\tSentence\tTF Id\tTG Id\tTF offset\tGene offset\tMutated Genes\tMutation offsets\tValid score\tValid\tMoR scores\tMoR\tPMID\tPMID+Sent+TRI_Id\tMutated_TF\tTF TaxID\tTG TaxID\tTF_type\")\n",
    "sents_to_check_2 = sents_to_check_2[tab_cols.split(\"\\t\")]\n",
    "\n",
    "sents_to_check_2.to_csv(sents_to_check_2_path, sep='\\t', index=False)\n",
    "print(f\"results saved in {sents_to_check_2_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TG     TG Symbol\n",
       "CD44   CD44         944\n",
       "CD133  PROM1        350\n",
       "CD40   CD40         306\n",
       "CD86   CD86         232\n",
       "       Cd86         180\n",
       "                   ... \n",
       "cd22   CD22           1\n",
       "cd274  Cd274          1\n",
       "CD1    Cd1d1          1\n",
       "cd5    CD5            1\n",
       "cd59   CD59           1\n",
       "Name: count, Length: 491, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "np.int64(9321)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PREPARE 3rd SET OF SENTENCES TO CHECK\n",
    "m = f_valid_df['TG'] == 'of'\n",
    "m |= f_valid_df['TF'] == 'of'\n",
    "sents_to_check_of = f_valid_df[m].sort_values(by=['TF', 'TG'])\n",
    "sents_to_check_of.to_csv(sents_to_check_of_path, sep='\\t', index=False)\n",
    "\n",
    "m = f_valid_df['TG'].str.upper().str.contains(\"^CD[0-9]\")\n",
    "m &= f_valid_df['Sentence'].str.contains(\"\\[TG\\] ?\")\n",
    "\n",
    "display(f_valid_df[m][['TG', 'TG Symbol']].value_counts())\n",
    "m.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = f_valid_df['TG'].str.upper().str.contains(\"^CD[0-9]\")\n",
    "f_valid_df[m].to_csv(sents_to_check_CDX_path, sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".general_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
