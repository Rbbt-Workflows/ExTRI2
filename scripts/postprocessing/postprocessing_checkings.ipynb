{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing of the ExTRI2 pipeline results to create the ExTRI2 resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was used to determine the rules to use for renormalisation & discard of sentences. Sentences were extracted from the ExTRI2 resource and checked manually, to determine how to handle each category. \n",
    "\n",
    "Furthermore, the **run postprocessing.py** section is a self-contained section to create the post-processed ExTRI2 file from the file obtained from the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3>Table of contents</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "[Postprocessing of the ExTRI2 pipeline results to create the ExTRI2 resource](#Postprocessing-of-the-ExTRI2-pipeline-results-to-create-the-ExTRI2-resource)\n",
       "- [Run postprocessing.py](#Run-postprocessing.py)\n",
       "- [Setup](#Setup)\n",
       "- [Postprocessing](#Postprocessing)\n",
       "  - [AP1 & NFKB](#AP1-&-NFKB)\n",
       "  - [Initial exploration](#Initial-exploration)\n",
       "  - [Create sets of sentences to check](#Create-sets-of-sentences-to-check)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "__import__('sys').path.append('../common/'); __import__('notebook_utils').table_of_contents('postprocessing_checkings.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "This notebook will now only be used for the normalisation of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run postprocessing.py\n",
    "Self-contained cell to run postprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### POSTPROCESSING TRI_df\n",
      "We got 6706 different TFs and 26196 different TGs from sentences labelled as TRI\n",
      "Retrieving from Entrez...\n",
      "\n",
      "4967 sentences are dropped as their TG is not normalised\n",
      "\n",
      "38287 rows (4.23%) will have its TF renormalized to NFKB\n",
      "6329 rows (0.70%) will be dropped as the TG corresponds to NFKB\n",
      "9003 rows (1.00%) will have its TF renormalized to AP1\n",
      "1858 rows (0.21%) will be dropped as the TG corresponds to AP1\n",
      "Breakdown by NCBI Symbol saved in ../../data/postprocessing/tables/AP1_NFKB_breakdown.tsv\n",
      "Number of renormalized sentences and normalization:\n",
      "4827\t0.54%\tp21 is normalized to CDKN1A\n",
      "1922\t0.21%\tp53-ps is normalized to its respective p53 symbol\n",
      "\n",
      "Number of discarded sentences and percentage from total (896330 sentences) and reasoning:\n",
      "2556\t0.29%\tTheir TF contains -AS[1-3]\n",
      "673\t0.08%\tTheir TF are circRNAs\n",
      "952\t0.11%\tTheir TF (NLRP3) is followed by inflammasome but normalised to NLRP3\n",
      "1875\t0.21%\tTheir TG (NLRP3) is followed by inflammasome but normalised to NLRP3\n",
      "1088\t0.12%\tTheir TG is followed by pathway/signalling/axis/program\n",
      "2037\t0.23%\tMDM2-TP53 pair, which is always a PPI\n",
      "1554\t0.17%\tTF is CD(4|8A|8B|74|34) positive (indicative of a cell, not a gene)\n",
      "22\t0.00%\tTG is CD(4|8A|8B|74|34) positive (indicative of a cell, not a gene)\n",
      "724\t0.08%\tPreposition of incorrectly identified as a gene\n",
      "7368\t0.82%\tThey contain translation in them (found to only be correct 40% of the time)\n",
      "41646\t4.65%\tThey are autoregulations (found to only be correct ~10% of the time)\n",
      "\n",
      "Discarding 11 rows with 'LOCXXXX' or 'GmXXXX' symbols: Gm49339, Gm38393, LOC100910792, LOC103693384, LOC102551901, LOC103689968, LOC100911372\n",
      "\n",
      "We get ortholog info for 24968/25608 Gene IDs\n",
      "\n",
      "\n",
      "### POSTPROCESSING nonTRI_df\n",
      "We got 4284 different TFs and 10938 different TGs from sentences labelled as TRI\n",
      "Retrieving from Entrez...\n",
      "\n",
      "476 sentences are dropped as their TG is not normalised\n",
      "\n",
      "2065 rows (2.78%) will have its TF renormalized to NFKB\n",
      "1221 rows (1.65%) will be dropped as the TG corresponds to NFKB\n",
      "315 rows (0.43%) will have its TF renormalized to AP1\n",
      "265 rows (0.36%) will be dropped as the TG corresponds to AP1\n",
      "Breakdown by NCBI Symbol saved in ../../data/postprocessing/tables/AP1_NFKB_breakdown.tsv\n",
      "Number of renormalized sentences and normalization:\n",
      "83\t0.11%\tp21 is normalized to CDKN1A\n",
      "125\t0.17%\tp53-ps is normalized to its respective p53 symbol\n",
      "\n",
      "Number of discarded sentences and percentage from total (72663 sentences) and reasoning:\n",
      "60\t0.08%\tTheir TF contains -AS[1-3]\n",
      "12\t0.02%\tTheir TF are circRNAs\n",
      "99\t0.14%\tTheir TF (NLRP3) is followed by inflammasome but normalised to NLRP3\n",
      "23\t0.03%\tTheir TG (NLRP3) is followed by inflammasome but normalised to NLRP3\n",
      "1554\t2.14%\tTheir TG is followed by pathway/signalling/axis/program\n",
      "82\t0.11%\tMDM2-TP53 pair, which is always a PPI\n",
      "311\t0.43%\tTF is CD(4|8A|8B|74|34) positive (indicative of a cell, not a gene)\n",
      "245\t0.34%\tTG is CD(4|8A|8B|74|34) positive (indicative of a cell, not a gene)\n",
      "82\t0.11%\tPreposition of incorrectly identified as a gene\n",
      "665\t0.92%\tThey contain translation in them (found to only be correct 40% of the time)\n",
      "15020\t20.67%\tThey are autoregulations (found to only be correct ~10% of the time)\n",
      "\n",
      "Discarding 2 rows with 'LOCXXXX' or 'GmXXXX' symbols: LOC100912068\n",
      "\n",
      "We get ortholog info for 11135/11293 Gene IDs\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../common')\n",
    "sys.path.append('../../')\n",
    "from scripts.postprocessing.postprocessing import *\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "## Custom functions\n",
    "import sys\n",
    "\n",
    "sys.path.append('../common')\n",
    "sys.path.append('../../')\n",
    "\n",
    "from notebook_utils import table_of_contents, table_from_dict, h3, h4, h5, md, bold\n",
    "from renormalisations import *\n",
    "from postprocessing import *\n",
    "pd.set_option('display.max_colwidth', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkings on the processed final TRI df\n",
    "config = load_config()\n",
    "final_TRI_df = load_df(config['final_ExTRI2_p'])\n",
    "orthologs_df = load_df(config['orthologs_final_p'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HGNC orthologs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I did:\n",
    "- Date: 12/10/2025\n",
    "- Went to page: https://www.ensembl.org/biomart/martview/\n",
    "\n",
    "For the orthologs:\n",
    "- **Dataset:** Human genes (Human genes (GRCh38.p14)), from Ensembl Genes 115\n",
    "- **Attributes:**\n",
    "  - Gene stable ID\n",
    "  - Transcript stable ID\n",
    "  - Norway rat - BN/NHsdMcwi gene stable ID\n",
    "  - Mouse gene stable ID\n",
    "\n",
    "For mapping Ensembl to NCBI:\n",
    "- Ensembl Genes 115\n",
    "  - **Dataset:** Human genes (GRCh38.p14) | Mouse genes (GRCm39) | Norway rat - BN/NHsdMcwi genes (GRCr8)\n",
    "  - **Attributes** > Features > External > NCBI gene ID\n",
    "  - For human: HGNC ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_human_orthologs(TRI_df: pd.DataFrame, ensembl_folder: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Add human orthologs, using Ensembl Genes 115 release.\n",
    "    Returns: \n",
    "        TRI_df with four new columns: TF_human_Id, TG_human_Id, TF_HGNC_ID, TG_HGNC_ID\n",
    "    '''\n",
    "\n",
    "    # Load Ensembl orthologs (Ensembl Genes 115 version)\n",
    "    ensembl_orthologs = pd.read_csv(ensembl_folder + 'mart_export_human_orthologs.txt', sep='\\t', dtype='str')\n",
    "    ensembl_orthologs.rename(columns={'Norway rat - BN/NHsdMcwi gene stable ID': 'Norway rat gene stable ID'}, inplace=True)\n",
    "\n",
    "    # Only keep relevant rows and columns\n",
    "    m = ensembl_orthologs['Norway rat gene stable ID'].isna() & ensembl_orthologs['Mouse gene stable ID'].isna()\n",
    "    ensembl_orthologs = ensembl_orthologs[~m][['Gene stable ID', 'Norway rat gene stable ID', 'Mouse gene stable ID']]\n",
    "\n",
    "    # Load the mappings from Ensembl Gene IDs to NCBI Gene IDs\n",
    "    ensembl_human_df = pd.read_csv(ensembl_folder + 'mart_export_homo_sapiens_Ensembl_to_NCBI.txt', sep='\\t', dtype='str')\n",
    "    ensembl_mouse_df = pd.read_csv(ensembl_folder + 'mart_export_mus_musculus_Ensembl_to_NCBI.txt', sep='\\t', dtype='str')\n",
    "    ensembl_rat_df = pd.read_csv(ensembl_folder + 'mart_export_rattus_norvegicus_Ensembl_to_NCBI.txt', sep='\\t', dtype='str')\n",
    "\n",
    "    # Build mapping dictionaries from Ensembl ID to NCBI Gene ID\n",
    "    ensembl_human_map = dict(zip(ensembl_human_df['Gene stable ID'], ensembl_human_df['NCBI gene (formerly Entrezgene) ID']))\n",
    "    ensembl_mouse_map = dict(zip(ensembl_mouse_df['Gene stable ID'], ensembl_mouse_df['NCBI gene (formerly Entrezgene) ID']))\n",
    "    ensembl_rat_map   = dict(zip(ensembl_rat_df['Gene stable ID'],   ensembl_rat_df['NCBI gene (formerly Entrezgene) ID']))\n",
    "\n",
    "    # Add the NCBI Gene ID to the ortholog dataframe\n",
    "    ensembl_orthologs['Human NCBI Gene ID'] = ensembl_orthologs['Gene stable ID'].map(ensembl_human_map)\n",
    "    ensembl_orthologs['Mouse NCBI Gene ID'] = ensembl_orthologs['Mouse gene stable ID'].map(ensembl_mouse_map)\n",
    "    ensembl_orthologs['Rat NCBI Gene ID']   = ensembl_orthologs['Norway rat gene stable ID'].map(ensembl_rat_map)\n",
    "\n",
    "    # Build mapping from NCBI Gene ID to Human NCBI Gene ID\n",
    "    mouse_map = dict(zip(ensembl_orthologs['Mouse NCBI Gene ID'], ensembl_orthologs['Human NCBI Gene ID']))\n",
    "    rat_map   = dict(zip(ensembl_orthologs['Rat NCBI Gene ID'],   ensembl_orthologs['Human NCBI Gene ID']))\n",
    "    orthologs_map = {k: v for k, v in (mouse_map | rat_map).items() if pd.notna(k) and pd.notna(v)} | {'Complex:NFKB': 'Complex:NFKB', 'Complex:AP1': 'Complex:AP1'}\n",
    "\n",
    "    # Build mapping from NCBI Gene ID to HGNC ID\n",
    "    hgnc_map = dict(zip(ensembl_human_df['NCBI gene (formerly Entrezgene) ID'], ensembl_human_df['HGNC ID']))\n",
    "    hgnc_map = {k: v for k, v in hgnc_map.items() if pd.notna(k) and pd.notna(v)} | {'Complex:NFKB': 'Complex:NFKB', 'Complex:AP1': 'Complex:AP1'}\n",
    "\n",
    "    def map_NCBI_ID_to_human_NCBI_ID(orthologs_map, gene_ids, taxids):\n",
    "        '''Map gene IDs (human, mouse, rat) to human NCBI Gene IDs, using Ensembl 115 release orthologs.'''\n",
    "        human_ids = []\n",
    "        for id, TaxID in zip(gene_ids.split(';'), taxids.split(';')):\n",
    "            if TaxID == '9606':\n",
    "                human_ids.append(id)\n",
    "            else:\n",
    "                human_ids.append(orthologs_map.get(id, '-'))\n",
    "        return \";\".join(human_ids)\n",
    "\n",
    "    def map_NCBI_ID_to_HGNC_ID(hgnc_map, gene_ids):\n",
    "        '''Map human gene ID to HGNC ID.'''\n",
    "        human_ids = []\n",
    "        for id in gene_ids.split(';'):\n",
    "            human_ids.append(hgnc_map.get(id, '-'))\n",
    "        return \";\".join(human_ids)\n",
    "\n",
    "    # Add Human NCBI ID into the final dataframe\n",
    "    TRI_df['TF_human_Id'] = TRI_df.apply(lambda row: map_NCBI_ID_to_human_NCBI_ID(orthologs_map, row['TF Id'], row['TF TaxID']), axis=1)\n",
    "    TRI_df['TG_human_Id'] = TRI_df.apply(lambda row: map_NCBI_ID_to_human_NCBI_ID(orthologs_map, row['TG Id'], row['TG TaxID']), axis=1)\n",
    "\n",
    "    # Add HGNC ID into the final dataframe\n",
    "    TRI_df['TF_HGNC_ID'] = TRI_df.apply(lambda row: map_NCBI_ID_to_HGNC_ID(hgnc_map, row['TF_human_Id']), axis=1)\n",
    "    TRI_df['TG_HGNC_ID'] = TRI_df.apply(lambda row: map_NCBI_ID_to_HGNC_ID(hgnc_map, row['TG_human_Id']), axis=1)\n",
    "\n",
    "    return TRI_df\n",
    "\n",
    "# External folder where I have downloaded the data\n",
    "ensembl_folder = '../../data/external/ensembl_Release_115_orthologs/'\n",
    "\n",
    "final_TRI_df = add_human_orthologs(final_TRI_df, ensembl_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021792150039132527"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = final_TRI_df['TF_HGNC_ID'].str.contains('-') | final_TRI_df['TG_HGNC_ID'].str.contains('-')\n",
    "m.sum() / len(final_TRI_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TF Id                     TF Symbol                       TF TaxID                 TF_human_Id      \n",
       "19015                     Ppard                           10090                    -                    483\n",
       "25493                     Nfkbia                          10116                    -                    178\n",
       "15361                     Hmga1                           10090                    -                    116\n",
       "108058                    Camk2d                          10090                    -                    103\n",
       "13039                     Ctsl                            10090                    -                     91\n",
       "                                                                                                       ... \n",
       "311807                    Bmyc                            10116                    -                      1\n",
       "19015;19013               Ppard;Ppara                     10090;10090              -;5465                 1\n",
       "19164;19165;387518;67866  Psen1;Psen2;Tas2r145-ps3;Wfdc1  10090;10090;10090;10090  5663;5664;-;58189      1\n",
       "302855                    Rbmx                            10116                    -                      1\n",
       "24438                     H1f6                            10116                    -                      1\n",
       "Name: count, Length: 109, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TG Id        TG Symbol        TG TaxID     TG_human_Id\n",
       "19211        Pten             10090        -              370\n",
       "13088        Cyp2b10          10090        -              289\n",
       "20309        Cxcl15           10090        -              285\n",
       "26198        COX2             10116        -              268\n",
       "20249        Scd1             10090        -              258\n",
       "                                                         ... \n",
       "11668;26358  Aldh1a1;Aldh1a7  10090;10090  -;216            1\n",
       "293152       Art2b            10116        -                1\n",
       "29298        Cyp2c7           10116        -                1\n",
       "292872       Klk1c3           10116        -                1\n",
       "19202        Rhox6            10090        -                1\n",
       "Name: count, Length: 1160, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contains '-' 1.55%\n",
      "No orthologs found 1.49%\n"
     ]
    }
   ],
   "source": [
    "display(final_TRI_df[final_TRI_df['TF_human_Id'].str.contains('-')][['TF Id', 'TF Symbol', 'TF TaxID', 'TF_human_Id']].value_counts())\n",
    "display(final_TRI_df[final_TRI_df['TG_human_Id'].str.contains('-')][['TG Id', 'TG Symbol', 'TG TaxID', 'TG_human_Id']].value_counts())\n",
    "print(f\"Contains '-' {(final_TRI_df['TF_human_Id'].str.contains('-') | final_TRI_df['TG_human_Id'].str.contains('-')).sum() / len(final_TRI_df):.2%}\")\n",
    "print(f\"No orthologs found {((final_TRI_df['TF_human_Id'] == '-') | (final_TRI_df['TG_human_Id'] == '-')).sum() / len(final_TRI_df):.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09227138586585801"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(final_TRI_df['TF_human_Id'].isna() | final_TRI_df['TG_human_Id'].isna()).sum() / len(final_TRI_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TF_type\n",
       "coTF                 1426\n",
       "dbTF                 1322\n",
       "ll_coTF               171\n",
       "dbTF|coTF               5\n",
       "ll_coTF|dbTF|coTF       1\n",
       "ll_coTF|coTF            1\n",
       "coTF|dbTF               1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## CHECK THE DISTRUBTION OF TF TYPES PER HUMAN MAPPING\n",
    "# Check whether any TF_human_symbol has multiple TF types from different rodent orthologs \n",
    "agg_funcs = {\n",
    "    'TF_type': lambda x: '|'.join(x.unique()),\n",
    "    'TF TaxID': lambda x: '|'.join(x.unique())\n",
    "}\n",
    "\n",
    "final_TRI_grouped = final_TRI_df.groupby('TF_human_Id', as_index=False).agg(agg_funcs)\n",
    "m = final_TRI_grouped['TF_human_Id'].str.contains(';')\n",
    "final_TRI_grouped[~m]['TF_type'].value_counts()\n",
    "# No TF_human_symbol has multiple TF types, nothing to correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - If we use Ensembl, then the way we obtained the TF types is no longer valid (as it used the HGNC ones). Find where that is and update it too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discarding 11 rows with 'LOCXXXX' or 'GmXXXX' symbols: Gm49339, Gm38393, LOC100910792, LOC103693384, LOC102551901, LOC103689968, LOC100911372\n",
      "\n",
      "We get ortholog info for 24968/25608 Gene IDs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO - If we use Ensembl, the cells below are no longer needed. Delete them\n",
    "# TODO a\n",
    "# TODO - Update this function to the NCBI mapping (it simplifies it a tonne  :) )\n",
    "import re\n",
    "\n",
    "def add_HGNC_symbols(ExTRI2_df: pd.DataFrame, orthologs_path: str) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    '''\n",
    "    use ortholog dicts in orthologs_path (downloaded from HGNC) to get HGNC orthologs for mouse, rat, & human HGNC IDs\n",
    "    '''\n",
    "\n",
    "    ## HELPER FUNCTIONS\n",
    "    def get_unique_HGNC_symbol_index(row):\n",
    "        '''\n",
    "        Helper function to get a unique HGNC symbol for each Entrez ID, when there are multiple options.\n",
    "        1) Return exact lowercase match, if any\n",
    "        2) First gene family member (e.g. ACSM2A for ACSM2; if multiple, take the one with the smallest numeric suffix)\n",
    "        3) If no match, return NaN\n",
    "        '''\n",
    "        # Get rodent symbol & human symbols\n",
    "        rodent_symbol = row['symbol'].lower()\n",
    "        human_symbols = row['human_symbol'].lower().split(\";\")\n",
    "\n",
    "        # If there's only one symbol, return index 0\n",
    "        if len(human_symbols) == 1:\n",
    "            return 0\n",
    "\n",
    "        # 1) Return exact lowercase match, if any\n",
    "        for i, c in enumerate(human_symbols):\n",
    "            if c == rodent_symbol:\n",
    "                return i\n",
    "\n",
    "        # 2) Apply first gene family member rule\n",
    "\n",
    "        # Strip the numbers from the end of rodent_symbol\n",
    "        family_stem_match = re.match(r'^(.*?)(\\d+)?$', rodent_symbol)\n",
    "        family_stem = family_stem_match.group(1) if family_stem_match else None\n",
    "\n",
    "        # Get human symbols that start with the same stem\n",
    "        matches = [(i, c) for i, c in enumerate(human_symbols) if c.startswith(family_stem)] if family_stem else []\n",
    "\n",
    "        if matches:\n",
    "            # Sort by numeric suffix (so ACSM2 < ACSM11)\n",
    "            def numeric_suffix(symbol):\n",
    "                m = re.search(r'(\\d+)$', symbol)\n",
    "                return int(m.group(1)) if m else float('inf')\n",
    "\n",
    "            matches.sort(key=lambda x: numeric_suffix(x[1]))\n",
    "            return matches[0][0]  # return index of first (smallest numeric suffix)\n",
    "\n",
    "        # No matches found — leave blank\n",
    "        return None\n",
    "    \n",
    "    def assign_unique_fields(row):\n",
    "        '''Helper function to assign unique fields based on index'''\n",
    "        idx = get_unique_HGNC_symbol_index(row)\n",
    "\n",
    "        if idx is None:\n",
    "            return pd.Series({\n",
    "                \"unique_human_symbol\":      \"-\",\n",
    "                \"unique_hgnc_id\":           \"-\",\n",
    "                \"unique_human_entrez_gene\": \"-\",\n",
    "            })\n",
    "\n",
    "        return pd.Series({\n",
    "            \"unique_human_symbol\":      row[\"human_symbol\"].split(\";\")[idx],\n",
    "            \"unique_hgnc_id\":           row[\"hgnc_id\"].split(\";\")[idx],\n",
    "            \"unique_human_entrez_gene\": row[\"human_entrez_gene\"].split(\";\")[idx],\n",
    "        })\n",
    "\n",
    "    def fill_ortholog_column(id, column):\n",
    "        '''Helper function to fill ortholog columns'''\n",
    "        result = []\n",
    "        for entrez_gene in id.split(\";\"):\n",
    "            result.append(orthologs_map[entrez_gene][column]) if entrez_gene in orthologs_map else \"-\"\n",
    "        return \";\".join(result)\n",
    "\n",
    "    # Create empty df to store orthologs\n",
    "    orthologs = pd.DataFrame()\n",
    "\n",
    "    # Get mouse & rat orthologs\n",
    "    for rodent in ['mouse', 'rat']:\n",
    "        # Load as string\n",
    "        hgnc_df = load_df(orthologs_path + f\"human_{rodent}_hcop_fifteen_column.txt\")\n",
    "        hgnc_df = hgnc_df.rename(columns={f\"{rodent}_entrez_gene\": \"entrez_gene\", f\"{rodent}_symbol\": \"symbol\"})\n",
    "        hgnc_df['TaxID'] = '10090' if rodent == 'mouse' else '10116'\n",
    "        orthologs = pd.concat([orthologs, hgnc_df])\n",
    "\n",
    "    # Get human HNGC symbols\n",
    "    h_hgnc_df = load_df(orthologs_path + \"hgnc_human.tsv\")\n",
    "    h_hgnc_df = h_hgnc_df.rename(columns={\"HGNC ID\": \"hgnc_id\", \"NCBI Gene ID\": \"entrez_gene\", \"Approved symbol\": \"symbol\"})\n",
    "    h_hgnc_df['human_entrez_gene'] = h_hgnc_df['entrez_gene']\n",
    "    h_hgnc_df['human_symbol'] = h_hgnc_df['symbol']\n",
    "    h_hgnc_df['TaxID'] = '9606'\n",
    "    orthologs = pd.concat([orthologs, h_hgnc_df])\n",
    "\n",
    "    # Keep only IDs present in the ExTRI2_df\n",
    "    TF_ids = {j for id in ExTRI2_df['TF Id'].unique() for j in id.split(';')}\n",
    "    TG_ids = {j for id in ExTRI2_df['TG Id'].unique() for j in id.split(';')}\n",
    "    TF_TG_ids = TF_ids | TG_ids\n",
    "    orthologs = orthologs[orthologs['entrez_gene'].isin(TF_TG_ids)]\n",
    "\n",
    "    # Remove all rows that don't have a symbol, human entrez ID, or hgnc ID\n",
    "    m = (orthologs['symbol'] != '-') & (orthologs['human_entrez_gene'] != '-') & (orthologs['hgnc_id'] != '-')\n",
    "    orthologs = orthologs[m]\n",
    "\n",
    "    # === Resolve cases where entrez_gene has a 1-to-many mapping with symbol. Can be fixed by eliminating all \"LOCXXXX\" & \"GmXXXX\" ones. ===\n",
    "    # Get entrez IDs with multiple symbols\n",
    "    entrezIDs_with_multiple_symbols = (\n",
    "        orthologs[['entrez_gene', 'symbol']]\n",
    "        .drop_duplicates()\n",
    "        .groupby('entrez_gene')\n",
    "        .filter(lambda g: len(g) > 1)['entrez_gene']\n",
    "        .unique()\n",
    "    )\n",
    "    # Discard all rows with 'LOCXXXX' or 'GmXXXX' symbols for these entrez IDs (& assert this solves all cases)\n",
    "    m_to_discard = orthologs['entrez_gene'].isin(entrezIDs_with_multiple_symbols) & (orthologs['symbol'].str.contains(r'^(?:LOC|Gm\\d+)', regex=True))\n",
    "    print(f\"Discarding {m_to_discard.sum()} rows with 'LOCXXXX' or 'GmXXXX' symbols: {', '.join(orthologs[m_to_discard]['symbol'].unique())}\\n\")\n",
    "    orthologs = orthologs[~m_to_discard]\n",
    "    assert (orthologs[['entrez_gene', 'symbol']].drop_duplicates()['entrez_gene'].duplicated().sum() == 0), \"There are still entrez IDs with multiple symbols after discarding 'LOCXXXX' and 'GmXXXX' ones\"\n",
    "\n",
    "\n",
    "    # Join with ';' when an EntrezID has more than 1 human ortholog\n",
    "    agg_funcs = {\n",
    "        \"symbol\": lambda x: ';'.join(x.unique()),\n",
    "        \"TaxID\": lambda x: ';'.join(x.unique()),\n",
    "        \"human_entrez_gene\": lambda x: ';'.join(x),\n",
    "        \"hgnc_id\": lambda x: ';'.join(x),\n",
    "        \"human_symbol\": lambda x: ';'.join(x),\n",
    "    }\n",
    "    orthologs = orthologs.groupby(['entrez_gene']).agg(agg_funcs).reset_index()\n",
    "\n",
    "    # Add NFKB & AP1 orthologs\n",
    "    for dimer in ['NFKB', 'AP1']:\n",
    "        orthologs = pd.concat([orthologs, pd.DataFrame([{\n",
    "            'entrez_gene': f'Complex:{dimer}',\n",
    "            'symbol': dimer,\n",
    "            'TaxID': '9606',\n",
    "            'human_entrez_gene': f'Complex:{dimer}',\n",
    "            'hgnc_id': f'Complex:{dimer}',\n",
    "            'human_symbol': dimer,\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "    # Add columns with 1-to-1 mapping (unique_human_symbol, unique_hgnc_id, unique_human_entrez_gene), by using either exact match or closest match of symbol wrt human_symbol\n",
    "    orthologs = orthologs.join(orthologs.apply(assign_unique_fields, axis=1))\n",
    "\n",
    "    # Show how many orthologs we get\n",
    "    print(f\"We get ortholog info for {len(orthologs)}/{len(TF_TG_ids)} Gene IDs\\n\")\n",
    "\n",
    "    # Fill in ortholog columns for TFs and TGs using the \"unique\" fields for a 1-to-1 mapping\n",
    "    orthologs_map = orthologs.set_index('entrez_gene').to_dict(orient='index')\n",
    "    for T in ('TF', 'TG'):\n",
    "        ExTRI2_df[f\"{T}_human_entrez_gene\"] = ExTRI2_df[f'{T} Id'].apply(lambda id: fill_ortholog_column(id, \"unique_human_entrez_gene\"))\n",
    "        ExTRI2_df[f\"{T}_hgnc_id\"]           = ExTRI2_df[f'{T} Id'].apply(lambda id: fill_ortholog_column(id, \"unique_hgnc_id\"))\n",
    "        ExTRI2_df[f\"{T}_human_symbol\"]      = ExTRI2_df[f'{T} Id'].apply(lambda id: fill_ortholog_column(id, \"unique_human_symbol\"))\n",
    "\n",
    "    return ExTRI2_df, orthologs\n",
    "\n",
    "\n",
    "trial_TRI_df, orthologs_df = add_HGNC_symbols(final_TRI_df, config['orthologs_p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of orthologs to check: 356\n"
     ]
    }
   ],
   "source": [
    "## SAVE ORTHOLOGS WITH AUTOMATIC UNIQUE MAPPING TO CHECK MANUALLY\n",
    "\n",
    "# Find orthologs that have multiple human symbols, and whose chosen unique human symbol is not an exact string match of the rodent symbol\n",
    "# These were automatically mapped using approximate string matching, and must be checked manually\n",
    "m = (orthologs_df['human_symbol'].str.contains(';')) & (orthologs_df['symbol'].str.lower() != orthologs_df['unique_human_symbol'].str.lower())\n",
    "orthologs_df[m][['symbol', 'unique_human_symbol', 'TaxID']]\n",
    "\n",
    "# Precompute counts in final_TRI_df\n",
    "tf_counts = final_TRI_df['TF Symbol'].value_counts()\n",
    "tg_counts = final_TRI_df['TG Symbol'].value_counts()\n",
    "\n",
    "# Map counts\n",
    "orthologs_df['TF_count'] = orthologs_df['symbol'].map(tf_counts).fillna(0).astype(int)\n",
    "orthologs_df['TG_count'] = orthologs_df['symbol'].map(tg_counts).fillna(0).astype(int)\n",
    "\n",
    "# Save a dataframe of orthologs to check the correctness of the automatic unique mapping\n",
    "cols_first = ['TaxID', 'entrez_gene', 'symbol', 'unique_human_symbol', 'human_symbol', 'human_entrez_gene', 'hgnc_id']\n",
    "orthologs_to_check = orthologs_df[m][cols_first + [c for c in orthologs_df[m].columns if c not in cols_first]].sort_values(by=['symbol', 'TaxID']).reset_index(drop=True)\n",
    "print(f\"Number of orthologs to check: {len(orthologs_to_check)}\")\n",
    "\n",
    "orthologs_to_check.to_csv(config['data_p'] + 'validation/orthologs_to_check.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TF_type\n",
       "coTF       1415\n",
       "dbTF       1318\n",
       "ll_coTF     169\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## CHECK THE DISTRUBTION OF TF TYPES PER HUMAN MAPPING\n",
    "# Check whether any TF_human_symbol has multiple TF types from different rodent orthologs \n",
    "agg_funcs = {\n",
    "    'TF_type': lambda x: ';'.join(x.unique()),\n",
    "    'TF TaxID': lambda x: ';'.join(x.unique())\n",
    "}\n",
    "\n",
    "final_TRI_grouped = final_TRI_df.groupby('TF_human_symbol', as_index=False).agg(agg_funcs)\n",
    "m = final_TRI_grouped['TF_type'].str.contains(';')\n",
    "final_TRI_grouped[~m]['TF_type'].value_counts()\n",
    "# No TF_human_symbol has multiple TF types, nothing to correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AP1 & NFKB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AP1 and NFKB are dimers, and as such don't have neither a NCBI EntrezID, nor a HGNC symbol. PubTator normalizes them to one of their monomers. Therefore, in `postprocessing.py`, we\n",
    "* Find all dimers incorrectly normalized to monomers using regex\n",
    "* Change the TF metadata to AP1/NFKB. Delete the TG instances (a TG can't be a dimer)\n",
    "* Save a summary of the results and affected sentences in `data/postprocessing/tables`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### POSTPROCESSING TRI_df\n",
      "We got 6706 different TFs and 26196 different TGs from sentences labelled as TRI\n",
      "Retrieving from Entrez...\n",
      "\n",
      "4967 sentences are dropped as their TG is not normalised\n",
      "\n",
      "38287 rows (4.23%) will have its TF renormalized to NFKB\n",
      "6327 rows (0.70%) will be dropped as the TG corresponds to NFKB\n",
      "9003 rows (1.00%) will have its TF renormalized to AP1\n",
      "1858 rows (0.21%) will be dropped as the TG corresponds to AP1\n",
      "Breakdown by NCBI Symbol saved in ../../data/postprocessing/tables/AP1_NFKB_breakdown.tsv\n"
     ]
    }
   ],
   "source": [
    "# POSTPROCESSING BEFORE RENORMALISATION & DISCARDING WERE IMPLEMENTED\n",
    "def half_postprocess(ExTRI2_df: pd.DataFrame, TRI_sents: bool, config: dict) -> pd.DataFrame:\n",
    "    '''same as postprocess but before the renormalisation & discarding'''\n",
    "\n",
    "    df_type = 'TRI' if TRI_sents else 'nonTRI'\n",
    "    print(f'### POSTPROCESSING {df_type}_df')\n",
    "\n",
    "    # Retrieve Symbol & TaxID from Entrez\n",
    "    save_Symbol_TaxID_dict(ExTRI2_df, config[f'EntrezID_to_Symbol_{df_type}_p'])\n",
    "\n",
    "    # Filter & add metadata\n",
    "    if TRI_sents:\n",
    "        remove_duplicates(ExTRI2_df)\n",
    "    ExTRI2_df = add_symbols_TaxID(ExTRI2_df, config[f'EntrezID_to_Symbol_{df_type}_p'])\n",
    "    add_TF_type(ExTRI2_df, config)\n",
    "    ExTRI2_df = drop_GTFs(ExTRI2_df)\n",
    "    ExTRI2_df = remove_other_species(ExTRI2_df, TaxID)\n",
    "\n",
    "    # Fix AP1 & NFKB normalisations\n",
    "    ExTRI2_df = fix_NFKB_AP1(ExTRI2_df, config)\n",
    "\n",
    "    return ExTRI2_df\n",
    "\n",
    "config = load_config()\n",
    "\n",
    "# Load raw dataframe\n",
    "TRI_df = load_preprocess_df(config['raw_TRI_p'])\n",
    "\n",
    "# Postprocess (without renormalisation/discarding)\n",
    "TRI_df = half_postprocess(TRI_df, TRI_sents=True,  config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP1 TF Unique Entities: 155\n",
      "NFKB TF Unique Entities: 115\n"
     ]
    }
   ],
   "source": [
    "# GET NUMBER OF UNIQUE ENTITY NAMES\n",
    "ExTRI2_df = TRI_df\n",
    "\n",
    "m_AP1 = ExTRI2_df['TF Symbol'].str.contains('|'.join(('FOS', 'JUN')), case=False)\n",
    "\n",
    "NFKB_symbols = {'NFKB1', 'NFKB2', 'RELA', 'RELB'}\n",
    "m_NFKB = ExTRI2_df['TF Symbol'].str.upper().isin(NFKB_symbols)\n",
    "\n",
    "print(\"AP1 TF Unique Entities:\", len(ExTRI2_df[m_AP1]['TF'].unique()))\n",
    "# print(ExTRI2_df[m_AP1]['TF'].unique())\n",
    "print(\"NFKB TF Unique Entities:\", len(ExTRI2_df[m_NFKB]['TF'].unique()))\n",
    "# print(ExTRI2_df[m_NFKB]['TF'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST-PROCESSING FUNCTIONS\n",
    "def print_symbol_counts_side_by_side(m_template, max_counts = 10):\n",
    "    'Print symbol counts of m_template for TF & TG'\n",
    "    results = []\n",
    "    for T in ('TF', 'TG'):\n",
    "        m = eval(m_template.replace('{T}', T))\n",
    "        T_lines = TRI_df[m][f'{T} Symbol'].value_counts()[:max_counts].to_string().split('\\n')\n",
    "        results.append(T_lines)\n",
    "\n",
    "    # Print the two tables side by side\n",
    "    for tf_line, tg_line in zip(*results):\n",
    "        print(f\"{tf_line:<35} {tg_line}\")\n",
    "\n",
    "def print_dubious_pairs_TFTGcounts_side_by_side(dubious_pairs):\n",
    "    '''Print counts of TF&TG of 3 different symbols side by side'''\n",
    "    all_tables = []\n",
    "    for p in dubious_pairs:\n",
    "        results = []\n",
    "        for T in ('TF', 'TG'):\n",
    "            m = TRI_df[f'{T} Symbol'].isin([';'.join((p[0], p[1])), ';'.join((p[1], p[0]))])\n",
    "            T_counts = TRI_df[m][f'{T}'].value_counts().rename(f'{T} count')[:10]\n",
    "            results.append(T_counts)\n",
    "\n",
    "        # Merge the TF and TG counts on the same index\n",
    "        merged_df = pd.concat(results, axis=1).fillna(0).astype(int)\n",
    "        all_tables.append(merged_df)\n",
    "\n",
    "    # Convert each table to a string and split by lines\n",
    "    table_strings = [table.to_string().split('\\n') for table in all_tables]\n",
    "\n",
    "    # Use itertools.zip_longest to handle tables with different lengths\n",
    "    for lines in itertools.zip_longest(*table_strings, fillvalue=''):\n",
    "        # Print each line of the three tables side by side\n",
    "        print(f\"{lines[0]:<40} {lines[1]:<40} {lines[2]}\")\n",
    "\n",
    "def print_TF_TG_counts_side_by_side(title, m_template, sep=40):\n",
    "    bold(title)\n",
    "    counts = []\n",
    "    for T in ('TF', 'TG'):\n",
    "        m = eval(m_template)\n",
    "        T_lines = TRI_df[m][[f'{T}', f'{T} Symbol']].value_counts().to_string().split('\\n')\n",
    "        counts.append(T_lines)\n",
    "    \n",
    "    for tf_line, tg_line in itertools.zip_longest(*counts, fillvalue=''):\n",
    "        print(f\"{tf_line:<{sep}} {tg_line}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>Entities normalized to +1 ID</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "25644 (2.86%) entities are normalized to more than 1 ID.<br>We revise those that appear more than 100 times further:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Symbol                           TG Symbol\n",
      "MAPK3;MAPK1            5347         MAPK3;MAPK1       1425\n",
      "Mapk3;Mapk1            2798         Mapk3;Mapk1        618\n",
      "MAP2K1;MAP2K2           717         MMP2;MMP9          395\n",
      "SMAD2;SMAD3             481         SMAD2;SMAD3        253\n",
      "MAPK8;MAPK9             311         Smad2;Smad3        171\n",
      "Map2k1;Map2k2           303         CDK4;CDK6           96\n",
      "Smad2;Smad3             222         Mmp2;Mmp9           89\n",
      "EWSR1;FLI1              216         HSD11B1;RNU1-1      66\n",
      "ABL1;BCR                169         MIR143;MIR145       66\n",
      "BCR;ABL1                152         CASP3;CASP7         63\n",
      "CREBBP;EP300            147         MAP2K1;MAP2K2       57\n",
      "OIP5-AS1;OIP5;PTGDR     144         NKX2-5;NKX3-1       54\n",
      "SMAD1;SMAD5;SMAD9       128         CASP3;CASP9         42\n",
      "MAPK1;MAPK3             111         Ifna;Ifnb1          38\n",
      "HDAC1;HDAC2             104         EWSR1;FLI1          36\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "From those, 3 seem suspicious and are investigated further: ABL1;BCR, FLI1;EWSR1, MMP2;MMP9"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           TF count  TG count                         TF count  TG count                                                   TF count  TG count\n",
      "BCR/ABL         126         9            EWS-FLI1          142        16          MMP-2/9                                         0        99\n",
      "BCR-ABL1        108        18            EWS/FLI1           43         6          MMP-2/-9                                        0        68\n",
      "BCR/ABL1         34         6            EWS/FLI-1          12        11          MMP2/9                                          0        65\n",
      "Bcr/Abl          26         4            EWS::FLI1           9         2          MMP-2 and -9                                    0        40\n",
      "BCR::ABL1        10         3            EWSR1-FLI1          9         0          matrix metalloproteinase-2 and -9               0        16\n",
      "bcr/abl           8         6            EWSR1::FLI1         7         1          matrix metalloproteinase-2/9                    0        11\n",
      "BCR::ABL          5         0                                                     matrix metalloproteinase-2/-9                   0         9\n",
      "Bcr/abl           3         0                                                     matrix metalloproteinases 2 and 9               0         9\n",
      "BCR/abl           1         0                                                     matrix metalloproteinase (MMP)-2 and -9         0         7\n",
      "                                                                                  matrix metalloproteinase 2 and 9                0         6\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Through further manual investigation of the sentences, we have determined that:\n",
       "* FL1;EWSR1 & ABL1;BCR are fusion genes. They are correct TFs but must be discarded as TGs.\n",
       "* TG = MMP9;MMP2 entities indicate that the TF regulates both genes.\n",
       "   \n",
       "One TG sentence example for each case (first two will be discarded)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABL1;BCR:\t In this report, we show evidence that [TF] transcription factor stringently controls the expression of [TG], which can strategically be targeted by our novel RUNX inhibitor, Chb-M'.\n",
      "FLI1;EWSR1:\t [TF] knockdown showed a reduced cell growth and transcriptional activity of [TG].\n",
      "MMP2;MMP9:\t Finally, nimbolide suppressed the nuclear translocation of p65/p50 and DNA binding of [TF], which is an important transcription factor for controlling [TG] and VEGF gene expression.\n"
     ]
    }
   ],
   "source": [
    "### ENTITIES NORMALIZED TO +1 ID\n",
    "bold(\"Entities normalized to +1 ID\")\n",
    "m = (TRI_df['TF Symbol'].str.upper().str.contains(';')) | TRI_df['TG Symbol'].str.upper().str.contains(';')\n",
    "md(f\"{m.sum()} ({m.sum() / len(m):.2%}) entities are normalized to more than 1 ID.<br>We revise those that appear more than 100 times further:\")\n",
    "m_template = \"TRI_df['{T} Symbol'].str.contains(';')\"\n",
    "print_symbol_counts_side_by_side(m_template, max_counts=15)\n",
    "\n",
    "dubious_pairs = [('ABL1', 'BCR'), ('FLI1','EWSR1'), ('MMP2','MMP9')]\n",
    "\n",
    "md(f'From those, 3 seem suspicious and are investigated further: {\", \".join((\";\".join(p) for p in dubious_pairs))}')\n",
    "\n",
    "print_dubious_pairs_TFTGcounts_side_by_side(dubious_pairs)\n",
    "md('''\\\n",
    "Through further manual investigation of the sentences, we have determined that:\n",
    "* FL1;EWSR1 & ABL1;BCR are fusion genes. They are correct TFs but must be discarded as TGs.\n",
    "* TG = MMP9;MMP2 entities indicate that the TF regulates both genes.\n",
    "   \n",
    "One TG sentence example for each case (first two will be discarded)\n",
    "''')\n",
    "\n",
    "for p in dubious_pairs:\n",
    "    pairs = [';'.join(p) for p in itertools.permutations(p)]\n",
    "    print(f\"{pairs[0]}:\\t\", TRI_df[TRI_df['TG Symbol'].isin(pairs)].sample(1)['Sentence'].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sets of sentences to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_to_check_path = config['data_p'] + 'validation/sents_to_check.tsv'\n",
    "sents_to_check_2_path = config['data_p'] + 'validation/sents_to_check_2.tsv'\n",
    "sents_to_check_of_path = config['data_p'] + 'validation/sents_to_check_of.tsv'\n",
    "sents_to_check_CDX_path = config['data_p'] + 'validation/sents_to_check_CDX.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>Sentences with 'p21' not normalised to 'CDKN1A':</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF   TF Symbol                           TG   TG Symbol\n",
      "p21  Tceal1       14                     p21  H3P16        4542\n",
      "     TCEAL1        1                          Kras          232\n",
      "                                              TCEAL1         29\n",
      "                                              Tceal1         25\n",
      "                                              Tpt1            1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Sentences with 'p53' not normalised to 'TP53':</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF   TF Symbol                           TG   TG Symbol\n",
      "p53  Trp53        195                    p53  Trp53-ps     1615\n",
      "                                              p53-ps        240\n",
      "                                              Trp53          90\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Sentences with MDM2-TP53 pairs must be removed: they're always a PPI.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Symbol  TG Symbol\n",
      "MDM2       TP53         1609\n",
      "Mdm2       TP53           23\n",
      "MDM2;MDM4  TP53            3\n",
      "MDM2       TP53BP2         1 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Sentences with 'MET' not normalised to 'MET':</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF   TF Symbol                           TG   TG Symbol\n",
      "MET  SLTM         494                    MET  SLTM         392\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Sentences with 'CD\\d' :</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF            TF Symbol                  TG             TG Symbol\n",
      "CD34          CD34         1704          CD34           CD34         206\n",
      "              Cd34          113          CD4            CD4          162\n",
      "CD34(         Cd34            5                         Cd4           82\n",
      "              CD34            2          Cd4            CD4           46\n",
      "CD4CD25FoxP3  FOXP3           2          CD34           Cd34          40\n",
      "cd34          CD34            2          CD74           CD74          27\n",
      "CD34+         CD34            1          CD8alpha       Cd8a          17\n",
      "CD34Exo       CD34            1          Cd4            Cd4           12\n",
      "CD34LC        CD34            1          Cd74           Cd74           9\n",
      "CD34brCD38    CD34;CD38       1          CD74           Cd74           8\n",
      "CD4.Ezh2      Cd4;Ezh2        1          Cd8a           CD8A           5\n",
      "                                         CD8a           Cd8a           5\n",
      "                                         CD8alpha       CD8A           4\n",
      "                                         CD8A           CD8A           3\n",
      "                                         CD8B           CD8B           2\n",
      "                                         CD8beta        CD8B           2\n",
      "                                         CD8a           CD8A           1\n",
      "                                         CD8b           Cd8b1          1\n",
      "                                         Cd34           CD34           1\n",
      "                                                        Cd34           1\n",
      "                                         CD4;IFN-gamma  Cd4;Ifng       1\n",
      "                                         CD4 receptor   CD4            1\n",
      "                                         Cd74           CD74           1\n",
      "                                         CD34EGFP       Cd34           1\n",
      "                                         CD34 antigen   CD34           1\n",
      "                                         Cd8alpha       CD8A           1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Entities normalised to +1 IDs: ABL1;BCR</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF        TF Symbol                      TG       TG Symbol\n",
      "BCR/ABL   ABL1;BCR     126               BCR/ABL  ABL1;BCR     9\n",
      "Bcr/Abl   ABL1;BCR      26               bcr/abl  ABL1;BCR     6\n",
      "bcr/abl   ABL1;BCR       8               Bcr/Abl  ABL1;BCR     4\n",
      "BCR::ABL  ABL1;BCR       5               \n",
      "Bcr/abl   ABL1;BCR       3               \n",
      "BCR/abl   ABL1;BCR       1               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "Autoregulation:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "4.8% of sentences show autoregulation: TF & TG are the same. Most popular:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Symbol  TG Symbol\n",
      "TP53       TP53         1978\n",
      "VEGFA      VEGFA        1108\n",
      "EGFR       EGFR          852\n",
      "MYC        MYC           719\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "Translation instead of gene expression</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7479 sentences contain 'translat' in them and should be checked\n"
     ]
    }
   ],
   "source": [
    "# PREPARE SENTECES TO CHECK\n",
    "def add_to_sents_to_check(sents_to_check: list, m_template: str, issue: str) -> list:\n",
    "    T = 'TF'\n",
    "    m = eval(m_template)\n",
    "    T = 'TG'\n",
    "    m |= eval(m_template)\n",
    "    df_m = TRI_df[m].copy()\n",
    "    df_m['issue'] = issue\n",
    "    sents_to_check.append(df_m)\n",
    "\n",
    "sents_to_check = []\n",
    "\n",
    "t = \"Sentences with 'p21' not normalised to 'CDKN1A':\"\n",
    "m_template = \"(TRI_df[f'{T}'] == 'p21') & (TRI_df[f'{T} Symbol'].str.upper() != 'CDKN1A')\"\n",
    "print_TF_TG_counts_side_by_side(t, m_template)\n",
    "add_to_sents_to_check(sents_to_check, m_template, 'p21-CDKN1A')\n",
    "\n",
    "\n",
    "t = \"Sentences with 'p53' not normalised to 'TP53':\"\n",
    "m_template = \"(TRI_df[f'{T}'] == 'p53') & (TRI_df[f'{T} Symbol'].str.upper() != 'TP53')\"\n",
    "print_TF_TG_counts_side_by_side(t, m_template)\n",
    "add_to_sents_to_check(sents_to_check, m_template, 'p53-TP53')\n",
    "\n",
    "bold(f\"Sentences with MDM2-TP53 pairs must be removed: they're always a PPI.\")\n",
    "m = TRI_df['TF Symbol'].str.upper().str.contains('MDM2')\n",
    "m &= TRI_df['TG Symbol'].str.upper().str.contains('TP53')\n",
    "print(TRI_df[m][['TF Symbol', 'TG Symbol']].value_counts().to_string(), '\\n')\n",
    "\n",
    "\n",
    "t = \"Sentences with 'MET' not normalised to 'MET':\"\n",
    "m_template = \"(TRI_df[f'{T}'] == 'MET') & (TRI_df[f'{T} Symbol'].str.upper() != 'MET')\"\n",
    "print_TF_TG_counts_side_by_side(t, m_template)\n",
    "add_to_sents_to_check(sents_to_check, m_template, 'MET')\n",
    "\n",
    "\n",
    "t = \"Sentences with 'CD\\d' :\"\n",
    "m_template = \"TRI_df[f'{T}'].str.upper().str.contains(r'^CD(?:4|8A|8B|74|34)(?!\\d)')\"\n",
    "print_TF_TG_counts_side_by_side(t, m_template)\n",
    "add_to_sents_to_check(sents_to_check, m_template, 'CD*')\n",
    "\n",
    "# Joined NCBI IDs to check\n",
    "t = \"Entities normalised to +1 IDs: ABL1;BCR\"\n",
    "m_template = \"TRI_df[f'{T} Symbol'] == 'ABL1;BCR'\"\n",
    "print_TF_TG_counts_side_by_side(t, m_template)\n",
    "add_to_sents_to_check(sents_to_check, m_template, 'ABL1;BCR')\n",
    "\n",
    "\n",
    "bold(f\"\\nAutoregulation:\")\n",
    "m = TRI_df['TF Symbol'].str.upper() == TRI_df['TG Symbol'].str.upper()\n",
    "md(f\"{m.sum() / len(TRI_df):.1%} of sentences show autoregulation: TF & TG are the same. Most popular:\")\n",
    "print(TRI_df[m][['TF Symbol', 'TG Symbol']].value_counts()[:4].to_string())\n",
    "\n",
    "# Those are potentially commonly wrong. Prepare a set of 300 sentences for validation purposes\n",
    "df_m = TRI_df[m].sample(n=300)\n",
    "df_m['issue'] = 'Autoregulation'\n",
    "sents_to_check.append(df_m)\n",
    "\n",
    "bold('\\nTranslation instead of gene expression')\n",
    "m = TRI_df['Sentence'].str.lower().str.contains('translat')\n",
    "print(f\"{m.sum()} sentences contain 'translat' in them and should be checked\")\n",
    "df_m = TRI_df[m].sample(n=100)\n",
    "df_m['issue'] = 'Translate'\n",
    "sents_to_check.append(df_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE EXCEL\n",
    "def extract_context(sentence, token='[TF]', window=4, how='both'):\n",
    "    '''Get the last and next 4 words from the token'''\n",
    "    # Split the sentence by spaces\n",
    "    words = sentence.split()\n",
    "\n",
    "    # Find the index of the word that contains '[TF]' or its variations\n",
    "    index = [i for i, word in enumerate(words) if token in word][0]\n",
    "\n",
    "    # Extract the 4 words before and after the token, handling boundaries\n",
    "    start = max(0, index) if how=='right' else max(0, index - window)\n",
    "    end = min(len(words), index + 1) if how=='left' else min(len(words), index + window + 1)\n",
    "\n",
    "\n",
    "    # Join the extracted context words back into a string\n",
    "    return ' '.join(['...'] + words[start:end] + ['...'])\n",
    "\n",
    "sents_to_check = pd.concat(sents_to_check)\n",
    "\n",
    "cols_to_keep = ['issue', 'TF', 'TF Symbol', 'TG', 'TG Symbol', 'Sentence',  '#SentenceID', \n",
    "                'TF Id', 'TG Id', 'MoR', 'TF TaxID',  'TG TaxID', 'TF_type', 'issue']\n",
    "\n",
    "sents_to_check = sents_to_check[cols_to_keep]\n",
    "\n",
    "for T in ('TF', 'TG'):\n",
    "    sents_to_check[f'{T}_context'] = sents_to_check['Sentence'].apply(lambda x: extract_context(x, token=f'{T}'))\n",
    "    sents_to_check[f'{T}_left_context'] = sents_to_check['Sentence'].apply(lambda x: extract_context(x, token=f'{T}', how='left'))\n",
    "    sents_to_check[f'{T}_right_context'] = sents_to_check['Sentence'].apply(lambda x: extract_context(x, token=f'{T}', how='right'))\n",
    "\n",
    "sents_to_check.to_csv(sents_to_check_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "dbTF Autoregulation:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2.1% of sentences show autoregulation: TF & TG are the same. Most popular:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Symbol  TG Symbol\n",
      "TP53       TP53         1978\n",
      "MYC        MYC           719\n",
      "HIF1A      HIF1A         478\n",
      "ESR1       ESR1          434\n",
      "results saved in ../../data/validation/sents_to_check_2.tsv\n"
     ]
    }
   ],
   "source": [
    "# PREPARE 2nd SET OF SENTENCES TO CHECK\n",
    "bold(f\"\\ndbTF Autoregulation:\")\n",
    "\n",
    "# Get a set of 300 sentences to check with autoregulation (dbTF)#\n",
    "# Previous set contained a lot of coTF sentences. We want to check the number of false positives in dbTF-specific autoregulation.\n",
    "m = TRI_df['TF Symbol'].str.upper() == TRI_df['TG Symbol'].str.upper()\n",
    "m &= TRI_df['TF_type'] == 'dbTF'\n",
    "md(f\"{m.sum() / len(TRI_df):.1%} of sentences show autoregulation: TF & TG are the same. Most popular:\")\n",
    "print(TRI_df[m][['TF Symbol', 'TG Symbol']].value_counts()[:4].to_string())\n",
    "df_m = TRI_df[m].sample(n=300)\n",
    "df_m['issue'] = 'dbTF_autoregulation'\n",
    "\n",
    "# We will only check dbTF autoregulation\n",
    "sents_to_check_2 = df_m\n",
    "tab_cols = (\"issue\t#SentenceID\tTF\tTF Symbol\tTG\tTG Symbol\tSentence\tTF Id\tTG Id\tTF offset\tGene offset\tTRI score\tValid\tMoR scores\tMoR\tPMID\tPMID+Sent+TRI_Id\tTF TaxID\tTG TaxID\tTF_type\")\n",
    "sents_to_check_2 = sents_to_check_2[tab_cols.split(\"\\t\")]\n",
    "\n",
    "sents_to_check_2.to_csv(sents_to_check_2_path, sep='\\t', index=False)\n",
    "print(f\"results saved in {sents_to_check_2_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TG        TG Symbol\n",
       "CD44      CD44         944\n",
       "CD133     PROM1        350\n",
       "CD40      CD40         306\n",
       "CD86      CD86         232\n",
       "          Cd86         180\n",
       "                      ... \n",
       "CD40L.    Cd40lg         1\n",
       "CD41b     ITGA2B         1\n",
       "CD42a     GP9            1\n",
       "CD44High  Cd44           1\n",
       "cd59      CD59           1\n",
       "Name: count, Length: 491, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "9319"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PREPARE 3rd SET OF SENTENCES TO CHECK\n",
    "m = final_TRI_df['TG'] == 'of'\n",
    "m |= final_TRI_df['TF'] == 'of'\n",
    "sents_to_check_of = final_TRI_df[m].sort_values(by=['TF', 'TG'])\n",
    "sents_to_check_of.to_csv(sents_to_check_of_path, sep='\\t', index=False)\n",
    "\n",
    "m = final_TRI_df['TG'].str.upper().str.contains(\"^CD[0-9]\")\n",
    "m &= final_TRI_df['Sentence'].str.contains(\"\\[TG\\] ?\")\n",
    "\n",
    "display(final_TRI_df[m][['TG', 'TG Symbol']].value_counts())\n",
    "m.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = final_TRI_df['TG'].str.upper().str.contains(\"^CD[0-9]\")\n",
    "final_TRI_df[m].to_csv(sents_to_check_CDX_path, sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".general_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
